<!doctype html>
<html
  lang="en-us"
  
>
  <head>
    <meta charset="utf-8" />
<meta
  name="viewport"
  content="width=device-width, initial-scale=1, shrink-to-fit=no"
/>







  

<title>
  Real-Time High Quality Rendering | NothingToSay0031
</title>
<meta
  name="description"
  content="A comprehensive exploration of real-time rendering, covering shadows, global illumination, shading models, ray tracing, and industry techniques."
/>










<script>
  window.siteConfig = JSON.parse("{\"anchor_icon\":null,\"clipboard\":{\"copyright\":{\"content\":\"本文版权：本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！\",\"count\":50,\"enable\":false},\"fail\":\"复制失败 (ﾟ⊿ﾟ)ﾂ\",\"success\":\"复制成功(*^▽^*)\"},\"code_block\":{\"expand\":true},\"icon_font\":\"4552607_tq6stt6tcg\",\"outdate\":{\"daysago\":180,\"enable\":false,\"message\":\"本文最后更新于 {time}，请注意文中内容可能已经发生变化。\"}}");
</script>











  
  
  
    
  

  
  
  
    
  

  
    

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  rel="preload"
  as="style"
  href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7cNoto%20Serif%20SC:400,400italic,700,700italic%7c&amp;display=swap"
/>
<link
  rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7cNoto%20Serif%20SC:400,400italic,700,700italic%7c&amp;display=swap"
  media="print"
  onload="this.media='all'"
/>






  <link
    rel="preload"
    href="//at.alicdn.com/t/c/font_4552607_tq6stt6tcg.woff2"
    as="font"
    type="font/woff2"
    crossorigin="anonymous"
  />



  







  
 <link rel="stylesheet" href="https://nothingtosay0031.github.io/css/loader.min.2ad0e9bbffb534e893c0ecefc44787a277cf851387e8ad9dccfbc3a5f0886dbe.css" />




  <meta property="og:type" content="website" />
  <meta property="og:title" content="Real-Time High Quality Rendering | NothingToSay0031" />
  <meta
    property="og:description"
    content="A comprehensive exploration of real-time rendering, covering shadows, global illumination, shading models, ray tracing, and industry techniques."
  />
  <meta property="og:url" content="https://nothingtosay0031.github.io/post/games202/" />
  <meta
    property="og:site_name"
    content="NothingToSay0031"
  />
  <meta
    property="og:image"
    content="/"
  />
  <meta property="article:author" content="NothingToSay0031" />
  <meta property="article:published_time" content="2024-11-22T12:33:58&#43;00:00" />
  <meta property="article:modified_time" content="2024-11-22T12:33:58&#43;00:00" />
  
  
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:image" content="/" />
  
  
  
  
  




<link rel="shortcut icon" href="https://nothingtosay0031.github.io/favicon.ico">








  
 <link rel="stylesheet" href="https://nothingtosay0031.github.io/css/main.min.4e3ed4ec96a449612baa01e942ad2e62fab14c5e1e8f6b3eeb13d1cbc2e0dc67.css" />





  <link
    rel="preload"
    as="style"
    href="https://npm.webcache.cn/photoswipe@5.4.4/dist/photoswipe.css"
    onload="this.onload=null;this.rel='stylesheet'"
  />






  <link
    rel="preload"
    as="style"
    href="https://npm.webcache.cn/katex@0.16.9/dist/katex.min.css"
    onload="this.onload=null;this.rel='stylesheet'"
  />








  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/pace-js@1.2.4/pace.min.js"
    
    
    
    
    integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous"
  ></script>





  


  <link rel="stylesheet" href="https://npm.webcache.cn/@reimujs/aos@0.1.0/dist/aos.css" />




  </head>
  <body>
    
  <div id='loader'>
    <div class="loading-left-bg loading-bg"></div>
    <div class="loading-right-bg loading-bg"></div>
    <div class="spinner-box">
      <div class="loading-taichi">
        
          <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="https://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
            <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff5252" />
            <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z 
          M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95z" fill="#ff5252" />
          </svg>
        
      </div>
      <div class="loading-word">少女祈祷中...</div>
    </div>
  </div>
  </div>
  <script>
    var time = null;
    var startLoading = () => {
      time = Date.now();
      document.getElementById('loader').classList.remove("loading");
    }
    var endLoading = () => {
      if (!time) {
        document.body.style.overflow = 'auto';
        document.getElementById('loader').classList.add("loading");
      } else {
        if (Date.now() - time > 500) {
          time = null;
          document.body.style.overflow = 'auto';
          document.getElementById('loader').classList.add("loading");
        } else {
          setTimeout(endLoading, 500 - (Date.now() - time));
          time = null;
        }
      }
    }
    window.addEventListener('DOMContentLoaded', endLoading);
    document.getElementById('loader').addEventListener('click', endLoading);
  </script>


<div id="copy-tooltip" style="pointer-events: none; opacity: 0; transition: all 0.2s ease; position: fixed;top: 50%;left: 50%;z-index: 999;transform: translate(-50%, -50%);color: white;background: rgba(0, 0, 0, 0.5);padding: 10px 15px;border-radius: 10px;">
</div>


    <div id="container">
      <div id="wrap">
        
<div id="header-nav">
  <nav id="main-nav">
    
      <span class="main-nav-link-wrap">
        <div class='main-nav-icon icon rotate'>
          
            
              &#xe62b;
            
          
        </div>
        <a class="main-nav-link" href="https://nothingtosay0031.github.io/">Home</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class='main-nav-icon icon rotate'>
          
            
              &#xe62b;
            
          
        </div>
        <a class="main-nav-link" href="https://nothingtosay0031.github.io/archives">Archives</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class='main-nav-icon icon rotate'>
          
            
              &#xe62b;
            
          
        </div>
        <a class="main-nav-link" href="https://nothingtosay0031.github.io/about">About</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class='main-nav-icon icon rotate'>
          
            
              &#xe62b;
            
          
        </div>
        <a class="main-nav-link" href="https://nothingtosay0031.github.io/friend">Friend</a>
      </span>
    
    <a id="main-nav-toggle" class="nav-icon"></a>
  </nav>
  <nav id="sub-nav">
    
    
  </nav>
</div>
<header id="header">
  
    <img fetchpriority="high" src="https://nothingtosay0031.github.io/images/banner.webp" alt="Real-Time High Quality Rendering">
  

  <div id="header-outer">
    <div id="header-title">
      
        
        
          
        
  
        
          <a href="https://nothingtosay0031.github.io/" id="logo">
            <h1 data-aos="slide-up">Real-Time High Quality Rendering</h1>
          </a>
        
      
  
      
        
        
        <h2 id="subtitle-wrap" data-aos="slide-down">
          
        </h2>
      
    </div>
  </div>
</header>
        <main id="content">
          
          <section id="main">
  <article
  class="h-entry article"
  itemprop="blogPost"
  itemscope
  itemtype="https://schema.org/BlogPosting"
>
  <div
    class="article-inner"
    data-aos="fade-up"
  >
    <div class="article-meta">
      <div class="article-date">
  <a
    href="https://nothingtosay0031.github.io/post/games202/"
    class="article-date-link"
    data-aos="zoom-in"
  >
    <time datetime="2024-11-22 12:33:58 &#43;0000 UTC" itemprop="datePublished"
      >2024-11-22</time
    >
    <time style="display: none;" id="post-update-time"
      >2024-11-22</time
    >
  </a>
</div>

      <div class="article-category">
  
</div>

    </div>
    <div class="hr-line"></div>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
      
        <h1 id="real-time-shadows">
<a class="header-anchor" href="#real-time-shadows"></a>
Real-Time Shadows
</h1><h2 id="shadow-mapping">
<a class="header-anchor" href="#shadow-mapping"></a>
Shadow Mapping
</h2><p>Shadow mapping is an <strong>image-space algorithm</strong> to determine if a point is in shadow.</p>
<ul>
<li><strong>Pros</strong>: Does not require scene geometry</li>
<li><strong>Cons</strong>: Prone to <strong>self-occlusion</strong> and <strong>aliasing</strong> issues</li>
</ul>
<h3 id="algorithm-steps">
<a class="header-anchor" href="#algorithm-steps"></a>
Algorithm Steps
</h3><ol>
<li><strong>Generate Depth Map</strong>: Render the scene from the light’s perspective to create a <strong>depth map (shadow map)</strong>.</li>
<li><strong>Shadow Check</strong>: Compare the shading point’s distance to the light with the <strong>shadow map</strong> depth to decide if the point is in shadow.</li>
</ol>
<h3 id="self-occlusion-shadow-acne">
<a class="header-anchor" href="#self-occlusion-shadow-acne"></a>
Self-Occlusion (Shadow Acne)
</h3><p>Self-occlusion, also known as <a href="https://digitalrune.github.io/DigitalRune-Documentation/html/3f4d959e-9c98-4a97-8d85-7a73c26145d7.htm">shadow acne</a>, happens when <strong>points that should be lit</strong> are incorrectly shadowed. This often results from limited <strong>depth map resolution</strong>, which cannot capture detailed depth variations within small areas.</p>
<h4 id="mitigating-self-occlusion">
<a class="header-anchor" href="#mitigating-self-occlusion"></a>
Mitigating Self-Occlusion
</h4><ul>
<li>
<p><strong>Bias Adjustment</strong>: Adding a <strong>bias</strong> during shadow checking helps reduce shadow acne by making points close to surfaces visible.</p>
<ul>
<li><strong>Drawback</strong>: Excessive bias can cause a “shadow detachment” effect, where shadows appear separated from the object.</li>
</ul>
</li>
<li>
<p><strong>Second-Depth Shadow Mapping</strong>:<br>
Another approach is to use a <strong>second-depth shadow map</strong>. Instead of storing the first depth, it records the midpoint between the first and second depths.</p>
<ul>
<li><strong>Requirements</strong>: Needs <strong>watertight geometry</strong></li>
<li><strong>Trade-off</strong>: Higher computational cost</li>
</ul>
</li>
</ul>
<h3 id="aliasing">
<a class="header-anchor" href="#aliasing"></a>
Aliasing
</h3><p>Shadow maps can have <strong>aliasing</strong> (jagged edges) at shadow boundaries, usually due to <strong>limited depth map resolution</strong> and insufficient <strong>sampling</strong>.</p>
<h2 id="percentage-closer-filtering">
<a class="header-anchor" href="#percentage-closer-filtering"></a>
Percentage Closer Filtering
</h2><p>PCF (Percentage Closer Filtering) is an anti-aliasing technique applied to shadow edges. By <strong>filtering the visibility results</strong> around shadow edges, it helps reduce the jagged, staircase-like artifacts commonly seen in shadow maps.</p>
<h3 id="process">
<a class="header-anchor" href="#process"></a>
Process
</h3><ol>
<li>
<p><strong>Calculate Scene Distance</strong>: For a shading point $p$, first calculate its <strong>distance to the light source</strong> in the scene, denoted as $D_{\text{scene}}(p)$.</p>
</li>
<li>
<p><strong>Define the Filter Kernel</strong>: Choose a <strong>convolution kernel</strong> $w$ for filtering. This kernel determines the size of the region sampled around the point $p$ in the <strong>depth map</strong> (shadow map).</p>
<ul>
<li>For example, with a $3 \times 3$ kernel, the region around $p$ in the depth map will include <strong>9 pixels</strong>.</li>
</ul>
</li>
<li>
<p><strong>Gather Depths for Comparison</strong>: Use the kernel to read <strong>depth values</strong> $D_{\text{SM}}(q)$ from the shadow map at neighboring points $q$ around $p$, where $q \in \mathcal{N}(p)$.</p>
</li>
<li>
<p><strong>Compute Visibility for Each Neighbor</strong>: For each sampled depth $D_{\text{SM}}(q)$, calculate a <strong>visibility result</strong>:</p>
$$\chi^{+} \left[ D_{\text{SM}}(q) - D_{\text{scene}}(p) \right] = \begin{cases}  
    1 & \text{if } D_{\text{SM}}(q) > D_{\text{scene}}(p) \\  
    0 & \text{otherwise}  
    \end{cases}$$</li>
<li>
<p><strong>Weighted Average of Visibility Results</strong>: Using the <strong>weights</strong> $w(p, q)$ from the convolution kernel, compute the <strong>weighted average</strong> of the visibility results to obtain the final visibility $V(p)$ for point $p$:</p>
$$V(p) = \sum_{q \in \mathcal{N}(p)} w(p, q) \cdot \chi^{+}\left[ D_{\text{SM}}(q) - D_{\text{scene}}(p) \right]$$</li>
</ol>
<h3 id="example">
<a class="header-anchor" href="#example"></a>
Example
</h3><p>For a <strong>mean filter</strong> with a <strong>$3 \times 3$ kernel</strong>:</p>
<ul>
<li>For a shading point $p$, gather the <strong>9 neighboring pixels</strong> in the shadow map, and compare each depth to $D_{\text{scene}}(p)$.</li>
<li>Suppose the visibility results are: $$\begin{bmatrix} 1 & 0 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix}$$</li>
<li>The <strong>final visibility</strong> for $p$ is the <strong>average</strong> of these values, yielding $V(p) = 0.667$.</li>
</ul>
<h3 id="effect-of-kernel-size">
<a class="header-anchor" href="#effect-of-kernel-size"></a>
Effect of Kernel Size
</h3><ul>
<li><strong>Smaller Kernel</strong>: Results in sharper shadow edges.</li>
<li><strong>Larger Kernel</strong>: Produces softer, more blurred shadow edges.</li>
</ul>
<h2 id="soft-shadows">
<a class="header-anchor" href="#soft-shadows"></a>
Soft Shadows
</h2><p>When an <strong>area light source</strong> illuminates an object, it creates <strong>soft shadows</strong> with varying edge softness, depending on the <strong>degree of occlusion</strong>.</p>
<p>While <strong>PCF</strong> can blur shadow edges, it cannot dynamically create soft shadows, as its <strong>filter kernel size is fixed</strong>. To achieve true soft shadows, the filter kernel size should adjust based on occlusion.</p>
<h2 id="pcss-percentage-closer-soft-shadows">
<a class="header-anchor" href="#pcss-percentage-closer-soft-shadows"></a>
PCSS (Percentage Closer Soft Shadows)
</h2><p><strong>PCSS (Percentage Closer Soft Shadows)</strong> builds on PCF to produce soft shadows. It calculates the <strong>relative depth of occluders</strong> between the shading point and light source and adjusts the <strong>PCF filter kernel size</strong> based on this depth to achieve edge softness that varies with occlusion.</p>
<h3 id="pcss-algorithm-steps">
<a class="header-anchor" href="#pcss-algorithm-steps"></a>
PCSS Algorithm Steps
</h3><ol>
<li>
<p><strong>Blocker Search</strong>:</p>
<ul>
<li>In the depth map, find the <strong>average depth of occluders</strong> around the shading point within a specified area.</li>
<li>The search area can be a <strong>fixed size</strong> (e.g., $5 \times 5$) or adjusted based on the <strong>light source size</strong> $w_{\text{light}}$ and <strong>distance</strong> between the shading point and light $d_{\text{receiver}}$.</li>
</ul>
</li>
<li>
<p><strong>Penumbra Estimation</strong>:</p>
<ul>
<li>Assuming the light, occluder, and shading surface are <strong>parallel</strong>, use the <strong>light source size</strong> $w_{\text{light}}$, the <strong>distance to the shading point</strong> $d_{\text{receiver}}$, and <strong>average occluder depth</strong> $d_{\text{blocker}}$ to estimate the <strong>penumbra width</strong>: $$w_{\text{penumbra}} = \frac{ d_{\text{receiver}} - d_{\text{blocker}} }{ d_{\text{blocker}} } \cdot w_{\text{light}}$$</li>
</ul>
</li>
<li>
<p><strong>PCF Application</strong>:</p>
<ul>
<li>Run <strong>PCF</strong> using a filter kernel size proportional to the estimated <strong>penumbra width</strong> $w_{\text{penumbra}}$, resulting in soft shadow edges that adapt to the occlusion level.</li>
</ul>
</li>
</ol>
<h2 id="vssm-variance-soft-shadow-mapping">
<a class="header-anchor" href="#vssm-variance-soft-shadow-mapping"></a>
VSSM (Variance Soft Shadow Mapping)
</h2><p>To estimate the <strong>average depth of occluders</strong> $z_{\text{occ}}$ in the first step of <strong>PCSS</strong>, the algorithm must sample the depths of all nearby texels around the shading point and compare them to the shading point&rsquo;s depth. This process is repeated in the third step to determine <strong>average visibility</strong>.</p>
<p><strong>VSSM (Variance Soft Shadow Mapping)</strong> approximates these steps to <strong>significantly accelerate</strong> both the first and third steps of PCSS. The accelerated process involves:</p>
<ol>
<li><strong>Calculating the mean and variance</strong> of the depth distribution around the shading point.</li>
<li><strong>Using inequalities</strong> to estimate either the <strong>average unoccluded depth</strong> $z_{\text{occ}}$ or the <strong>visibility result</strong> $V(p)$ at the shading point.</li>
</ol>
<h3 id="calculating-mean-and-variance-of-depth-distribution">
<a class="header-anchor" href="#calculating-mean-and-variance-of-depth-distribution"></a>
Calculating Mean and Variance of Depth Distribution
</h3><p>The <strong>mean</strong> $E(X)$ and <strong>variance</strong> $\text{Var}(X)$ of a random variable $X$ are related by:</p>
$$ \text{Var}(X) = E(X^2) - E^2(X) $$<p>To compute these values, we store both the <strong>depth</strong> $z$ and <strong>depth squared</strong> $z^2$ for each texel when generating the depth map. This allows us to calculate the <strong>summed area table (SAT)</strong> for both.</p>
<p>When a region around the shading point is queried, we can retrieve the <strong>mean depth</strong> $z_{\text{avg}}$ and <strong>mean squared depth</strong> $(z^2)_{\text{avg}}$ from the SATs, using $(1)$ to find the <strong>variance</strong> of the depth distribution:</p>
$$\text{Variance} = (z^2)_{\text{avg}} - (z_{\text{avg}})^2$$<h3 id="estimating-unoccluded-depth-or-visibility-with-inequalities">
<a class="header-anchor" href="#estimating-unoccluded-depth-or-visibility-with-inequalities"></a>
Estimating Unoccluded Depth or Visibility with Inequalities
</h3><p><strong>Chebyshev&rsquo;s inequality</strong> describes the relationship between a probability distribution&rsquo;s density function, mean, and variance:</p>
$$P(x > t) \le \frac{\sigma^2}{\sigma^2 + (t - \mu)^2}$$<p>Let $z_t$ represent the depth of the shading point. Then $P(z > z_t)$ gives the <strong>proportion of unoccluded texels</strong> around the shading point.</p>
<p>VSSM assumes equality in <strong>Chebyshev’s inequality</strong> for simplicity:</p>
$$P(z > z_t) = \frac{(z^2)_{\text{avg}} - (z_{\text{avg}})^2}{(z^2)_{\text{avg}} - (z_{\text{avg}})^2 + (z_t - z_{\text{avg}})^2}$$<ul>
<li>If this is used in <strong>PCSS Step 3</strong>, it directly provides the <strong>visibility</strong> result for the shading point: $V(p) = P(z > z_t)$.</li>
<li>For <strong>PCSS Step 1</strong>, we assume that the <strong>average depth of unoccluded texels</strong> equals the shading point’s depth $z_{\text{unocc}} = z_t$. Given the occluder average depth $z_{\text{occ}}$ and the unoccluded average depth $z_{\text{unocc}}$, we have:</li>
</ul>
$$[1 - P(z > z_t)] \cdot z_{\text{occ}} + P(z > z_t) \cdot z_{\text{unocc}} = z_{\text{avg}}$$<p>By substituting values, we can estimate the <strong>average occluder depth</strong> $z_{\text{occ}}$, used in <strong>PCSS Step 2</strong> to estimate the penumbra.</p>
<h2 id="moment-shadow-mapping">
<a class="header-anchor" href="#moment-shadow-mapping"></a>
Moment Shadow Mapping
</h2><p><strong>Moment Shadow Mapping</strong> improves on <strong>VSSM</strong> by reconstructing the cumulative distribution function (CDF) for depths around the shading point using higher-order moments, rather than just the mean and second-order moment.</p>
<ul>
<li><strong>Higher-order moments</strong> yield a more accurate fit for the CDF, reducing issues like <strong>light leaks</strong> in shadow generation.</li>
<li>The trade-off: <strong>increased storage</strong> and <strong>computational resources</strong> are required due to higher-order depth terms, but the shadow quality improves.</li>
</ul>
<h2 id="distance-field-soft-shadows">
<a class="header-anchor" href="#distance-field-soft-shadows"></a>
Distance Field Soft Shadows
</h2><p>Unlike the PCSS series, <strong>Distance Field Soft Shadows</strong> utilize a <strong>signed distance field (SDF)</strong> instead of a shadow map to create soft shadows.</p>
<ul>
<li>The <strong>SDF</strong> $\text{sdf}: \mathbb{R}^3 \rightarrow \mathbb{R}$ stores the distance from any point in the 3D scene to the nearest surface. Points inside an object have negative values, while points outside have positive values.</li>
</ul>
<p>To compute the shadow at a shading point $o$:</p>
<ol>
<li><strong>Trace a ray</strong> from $o$ towards the light source.</li>
<li>During this tracing, record each point $p$&rsquo;s <strong>SDF value</strong> $\text{sdf}(p)$ and its distance from $o$.</li>
<li>Calculate the <strong>safe viewing angle</strong> $\theta$, representing the maximum angle from $o$ towards the light source without occlusion.</li>
</ol>
<p>The smaller $\theta$ is, the more occluded the shading point is, resulting in softer shadows.</p>
<h3 id="safe-angle-calculation">
<a class="header-anchor" href="#safe-angle-calculation"></a>
Safe Angle Calculation
</h3><p>Instead of a direct inverse trigonometric function, the following formula is often used for efficiency:</p>
$$ \theta = \min \Big\{ \frac{k \cdot \text{sdf}(p)}{\| p - o \|}, 1 \Big\} $$<ul>
<li><strong>Parameter $k$</strong> controls the shadow’s softness; larger $k$ values create softer shadow edges.</li>
</ul>
<h3 id="considerations">
<a class="header-anchor" href="#considerations"></a>
Considerations
</h3><ul>
<li><strong>High-quality, fast soft shadows</strong> are achievable with distance fields.</li>
<li><strong>Drawbacks</strong>: SDFs require <strong>significant pre-computation</strong> and storage, as they must capture depth information for every point in the 3D space. In contrast, PCSS techniques only store depth at the light source.</li>
</ul>
<p>To reduce resource usage, <strong>hierarchical data structures</strong> like <strong>octrees</strong> can subdivide the scene efficiently. In regions distant from any surface, a coarser subdivision reduces storage demands and computational costs.</p>
<h1 id="real-time-environment-mapping">
<a class="header-anchor" href="#real-time-environment-mapping"></a>
Real-Time Environment Mapping
</h1><h2 id="ambient-lighting-and-shading">
<a class="header-anchor" href="#ambient-lighting-and-shading"></a>
Ambient Lighting and Shading
</h2><p><strong>Environment Mapping</strong> simplifies the representation of surrounding light and environment by projecting it directly as a texture. This texture, which represents the ambient light, assumes light sources are located at infinity. Common representations include <strong>spherical maps</strong> or <strong>cube maps</strong>.</p>
<h3 id="image-based-lighting-ibl">
<a class="header-anchor" href="#image-based-lighting-ibl"></a>
Image-Based Lighting (IBL)
</h3><p><strong>Image-Based Lighting (IBL)</strong> is a technique for shading a point $p$ using an environment map, <strong>ignoring visibility</strong> $V(p, \omega_i)$. The rendering equation for IBL is:</p>
$$L_o(p, \omega_o) = \int_{\Omega^+} L_i(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos \theta_i \cancel{V(p, \omega_i)} \, \mathrm{d}\omega_i$$<p>In IBL, shading is simplified by <strong>excluding visibility calculations</strong> from the rendering equation.</p>
<h4 id="approximation-with-monte-carlo-sampling">
<a class="header-anchor" href="#approximation-with-monte-carlo-sampling"></a>
Approximation with Monte Carlo Sampling
</h4><p>To approximate this rendering integral, <strong>Monte Carlo sampling</strong> can be used to sample directions of incoming ambient light.</p>
<ul>
<li><strong>Trade-off</strong>: Achieving convergence with Monte Carlo methods requires a <strong>high number of samples</strong> for accurate results, which can demand significant computational resources.</li>
</ul>
<h2 id="split-sum-approximation">
<a class="header-anchor" href="#split-sum-approximation"></a>
Split Sum Approximation
</h2><h3 id="approximation-in-rtr">
<a class="header-anchor" href="#approximation-in-rtr"></a>
Approximation in RTR
</h3><p>In real-time rendering, a useful approximation for evaluating integrals of function products is to <strong>separate the integral of two multiplied functions</strong> into the product of their integrals:</p>
$$\int_{\Omega} f(x) g(x) \, \mathrm{d}x \approx \frac{\int_{\Omega_G} f(x) \, \mathrm{d}x}{\int_{\Omega_G} \, \mathrm{d}x} \cdot \int_{\Omega} g(x) \, \mathrm{d}x = \overline{f(x)} \cdot \int_{\Omega} g(x) \, \mathrm{d}x$$<p>($\overline{f(x)}$: the average $f(x)$ in the support of $G$)</p>
<h4 id="key-points">
<a class="header-anchor" href="#key-points"></a>
Key Points
</h4><ul>
<li>This approximation holds well when:
<ul>
<li>The integrand $g(x)$ is relatively smooth (e.g., BRDF for diffuse materials).</li>
<li>The support domain of $g(x)$ is small (e.g., few light sources are directly sampled for visibility).</li>
</ul>
</li>
</ul>
<h3 id="applying-split-sum-to-rendering-equation">
<a class="header-anchor" href="#applying-split-sum-to-rendering-equation"></a>
Applying Split Sum to Rendering Equation
</h3><p>To avoid sampling every direction for the integral, <strong>Split Sum</strong> approximates the rendering equation by factoring out the BRDF during integration. This transforms the rendering equation into the following form:</p>
$$\begin{align}  
L_o(p, \omega_o) & = \int_{\Omega^+} L_i(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos\theta_i \, \mathrm{d}\omega_i  \notag\\  
& \approx \frac{\int_{\Omega^+} L_i(p, \omega_i) \, \mathrm{d}\omega_i}{\int_{\Omega^+} \mathrm{d}\omega_i} \cdot \int_{\Omega^+} f_r(p, \omega_i, \omega_o) \cos\theta_i \, \mathrm{d}\omega_i    \notag
\end{align}$$<ul>
<li>For <strong>diffuse materials</strong> with smooth BRDFs, this provides a reasonable approximation.</li>
<li>For <strong>glossy materials</strong>, even if the BRDF isn’t smooth, the support domain is generally small, so the approximation remains effective.</li>
</ul>
<h3 id="sum-based-approximation">
<a class="header-anchor" href="#sum-based-approximation"></a>
Sum-Based Approximation
</h3><p>The method is called &ldquo;Split Sum&rdquo; rather than &ldquo;Split Integral&rdquo; because its originators rewrote the integral as a sum:</p>
$$\frac{1}{N} \sum_{k=1}^{N} \frac{L_i(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos\theta_i}{\mathrm{pdf}(p, \omega_i)} \approx \left( \frac{1}{N} \sum_{k=1}^{N} L_i(p, \omega_i) \right) \left( \frac{1}{N} \sum_{k=1}^{N} \frac{f_r(p, \omega_i, \omega_o) \cos\theta_i}{\mathrm{pdf}(p, \omega_i)} \right)$$<p>This sum-based approach allows for efficient computation by reducing the number of samples needed, making it suitable for real-time rendering.</p>
<h3 id="pre-filtering-of-the-environment-map">
<a class="header-anchor" href="#pre-filtering-of-the-environment-map"></a>
Pre-filtering of the Environment Map
</h3><p>The first part of the integral:</p>
$$\frac{\int_{\Omega^{+}} L_i(p, \omega_i) \, \mathrm{d}\omega_i}{\int_{\Omega^{+}} \mathrm{d}\omega_i}$$<p>can be seen as applying a filter to the environment map, where the size of the filter kernel depends on the support set of the BRDF (Bidirectional Reflectance Distribution Function).</p>
<ul>
<li>When shading a given point, querying the pre-filtered environment map using the ideal specular reflection direction essentially queries the environment lighting in the area around the specular reflection direction.</li>
<li>This approach eliminates the need for sampling, as it directly accesses the precomputed data.</li>
</ul>
<p>To implement this, you can precompute a mipmap of the environment map, which is an image pyramid of prefiltered environment maps at different levels of detail. During shading, based on the BRDF, you can select the appropriate filter kernel size and fetch the corresponding lighting information from the appropriate mipmap level or use trilinear interpolation.</p>
<h3 id="evaluating-the-brdf-integral">
<a class="header-anchor" href="#evaluating-the-brdf-integral"></a>
Evaluating the BRDF Integral
</h3><p>The second part of the integral:</p>
$$\int_{\Omega^{+}} f_r(p, \omega_i, \omega_o) \cos\theta_i \, \mathrm{d}\omega_i$$<p>can be computed either by evaluating all possible parameters directly or by using <strong>LTC</strong> (Linearly Transformed Cosines).</p>
<p>For <strong>microfacet models</strong> of BRDFs, the <strong>Schlick’s approximation</strong> for the Fresnel term is commonly used:</p>
$$F(\theta) = R_0 + (1 - R_0) \left( 1 - \cos\theta \right)^5$$<p>Where:</p>
<ul>
<li>$R_0 = \left( \frac{\eta_1 - \eta_2}{\eta_1 + \eta_2} \right)^2$ is the base reflectance value, with $\eta_1$ and $\eta_2$ being the refractive indices of the two materials surrounding the shading point.</li>
<li>In real-time rendering, the incident angle $\theta_i$, the exit angle $\theta_o$, and their half-angle are considered very close. Thus, we use $\theta$ to describe the light direction.</li>
</ul>
<p>The integral can then be split based on the Fresnel term:</p>
$$\begin{align}  
\int_{\Omega^{+}} f_r(p, \omega_i, \omega_o) \cos\theta_i \, \mathrm{d}\omega_i &\approx \int_{\Omega^{+}} \frac{f_r(p, \omega_i, \omega_o)}{F(\theta_i)} \left[ R_0 + (1 - R_0) (1 - \cos\theta_i)^5 \right] \cos\theta_i \, \mathrm{d}\omega_i  \notag\\  
&= R_0 \int_{\Omega^{+}} \frac{f_r(p, \omega_i, \omega_o)}{F(\theta_i)} \left[ 1 - (1 - \cos\theta_i)^5 \right] \cos\theta_i \, \mathrm{d}\omega_i  \notag\\ & + \int_{\Omega^{+}} \frac{f_r(p, \omega_i, \omega_o)}{F(\theta_i)} (1 - \cos\theta_i)^5 \cos\theta_i \, \mathrm{d}\omega_i  \notag 
\end{align}$$<p>Thus, the integral no longer depends on the base color $R_0$. During precomputation, only the <strong>roughness</strong> and <strong>incident angle cosine</strong> need to be considered when evaluating the term $\frac{f_r(p, \omega_i, \omega_o)}{F(\theta)}$.</p>
<h2 id="environment-lighting-shadows">
<a class="header-anchor" href="#environment-lighting-shadows"></a>
Environment Lighting Shadows
</h2><p>When shading is required, real-time rendering becomes a challenging problem.</p>
<p>The equation for shading at a given point $p$ is:</p>
$$L_o(p, \omega_o) = \int_{\Omega^+} L_i(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos\theta_i \cdot V(p, \omega_i) \, \mathrm{d}\omega_i$$<p>Where:</p>
<ul>
<li>$L_o(p, \omega_o)$: Shading result at point $p$</li>
<li>$L_i(p, \omega_i)$: Incoming environmental lighting</li>
<li>$f_r(p, \omega_i, \omega_o)$: BRDF function</li>
<li>$V(p, \omega_i)$: Visibility factor for incoming light at point $p$</li>
</ul>
<h3 id="key-challenges">
<a class="header-anchor" href="#key-challenges"></a>
Key Challenges
</h3><ul>
<li>
<p><strong>Numerous Light Sources</strong>:<br>
Since the environment lighting comes from all directions, if shading is considered as a &ldquo;many-light rendering&rdquo; problem, each light source would require its own shadow map. The number of shadow maps required would become unmanageable.</p>
</li>
<li>
<p><strong>Visibility Complexity</strong>:<br>
The visibility term $V(p, \omega_i)$ can be highly complex. If treated as a sampling problem, it might not be approximated by methods like the <strong>split sum</strong> approach.</p>
</li>
<li>
<p><strong>BRDF and Support Set</strong>:<br>
The support set of $L_i(p, \omega_i)$ spans the entire hemisphere, while the BRDF function $f_r(p, \omega_i, \omega_o)$ might not be smooth. After separating $V$, the remaining integral of $L_i(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos\theta_i$ might still have a large support set, which could also be non-smooth.</p>
</li>
</ul>
<h3 id="industry-solutions">
<a class="header-anchor" href="#industry-solutions"></a>
Industry Solutions
</h3><ul>
<li><strong>Shadow Maps</strong>:<br>
In practice, the industry tends to choose the brightest light sources (e.g., the sun) or a few prominent light sources to generate shadow maps. Shadows are then created from these selected sources.</li>
</ul>
<h3 id="research-directions">
<a class="header-anchor" href="#research-directions"></a>
Research Directions
</h3><ul>
<li>
<p><strong>Imperfect Shadow Maps</strong>:<br>
Techniques to improve shadow map quality by handling issues like low resolution and artifacts.</p>
</li>
<li>
<p><strong>Lightcuts</strong>:<br>
A method to approximate shadow computation by reducing the number of light sources that contribute to a given point&rsquo;s shading.</p>
</li>
<li>
<p><strong>Real-Time Ray Tracing (RTRT)</strong>:<br>
Potentially the ultimate solution for real-time shadow generation, allowing for accurate and dynamic lighting and shadow effects.</p>
</li>
<li>
<p><strong>Precomputed Radiance Transfer (PRT)</strong>:<br>
A technique that precomputes the transfer of radiance for different lighting conditions, which can be used for efficient shading in complex environments.</p>
</li>
</ul>
<h2 id="precomputed-radiance-transfer">
<a class="header-anchor" href="#precomputed-radiance-transfer"></a>
Precomputed Radiance Transfer
</h2><p>Precomputed Radiance Transfer (PRT) optimizes real-time rendering by separating the rendering equation&rsquo;s integrand into two components: <strong>lighting</strong> and <strong>light transport</strong>. These components are precomputed and represented in the frequency domain, transforming complex integral calculations into efficient vector or matrix operations during shading.</p>
<h3 id="precomputing-lighting-and-light-transport">
<a class="header-anchor" href="#precomputing-lighting-and-light-transport"></a>
Precomputing Lighting and Light Transport
</h3><p>PRT separates the rendering equation&rsquo;s integrand into two components: <strong>lighting</strong> and <strong>light transport</strong>, which are precomputed to create two texture images.</p>
<ul>
<li>
$$
   \begin{align} 
   \underset{\text{Shading result}}{\underbrace{L_o(\omega_o)}} &= \int_{\Omega^+} \underset{\text{Lighting}}{\underbrace{L_i(\omega_i)}} \cdot \underset{\text{BRDF}}{\underbrace{f_r(\omega_i, \omega_o) \cos\theta_i}} \cdot \underset{\text{Visibility}}{\underbrace{V(\omega_i)}} \, \mathrm{d}\omega_i  \notag  \\ 
   &= \int_{\Omega^+} \underset{\text{Lighting}}{\underbrace{L_i(\omega_i)}} \cdot \underset{\text{Light Transport, } T(\omega_i, \omega_o)}{\underbrace{f_r(\omega_i, \omega_o) \cos\theta_i V(\omega_i)}} \, \mathrm{d}\omega_i   \notag 
   \end{align}
   $$<ul>
<li><strong>Lighting</strong>: $L_i(\omega_i)$, the incoming light distribution.</li>
<li><strong>Light Transport</strong>: $T(\omega_i, \omega_o)$ combines BRDF, cosine attenuation, and visibility.</li>
</ul>
</li>
<li>
$$
   \begin{align} 
   L_i(\omega_i) &= \sum_p \underset{\text{Lighting coefficients}}{\underbrace{l_p}} \cdot \underset{\text{Basis function}}{\underbrace{B_p(\omega_i)}}  \notag \\ 
   T(\omega_i, \omega_o) &= \sum_q \underset{\text{Transport coefficients}}{\underbrace{t_q(\omega_o)}} \cdot \underset{\text{Basis function}}{\underbrace{B_q(\omega_i)}}   \notag 
   \end{align}
   $$</li>
<li>
<p><strong>Orthogonal Basis System</strong></p>
<ul>
<li>
<p>The basis functions $B_i(\omega_i)$ form an orthogonal set, allowing any signal in the spatial domain to be expanded into its orthogonal series by computing the coefficients through a dot product operation:</p>
$$
     \begin{align}
     l_p &= \int_{\Omega^+} L_i(\omega_i) \overline{B_p(\omega_i)} \, \mathrm{d}\omega_i \notag  \\
     t_q(\omega_o) &= \int_{\Omega^+} T(\omega_i, \omega_o) \overline{B_q(\omega_i)} \, \mathrm{d}\omega_i \notag 
     \end{align}
     $$</li>
</ul>
</li>
<li>
<p><strong>Dot Product in Extended Space</strong></p>
<ul>
<li>The dot product for complex-valued functions is defined as the integral of the product of one function and the conjugate of the other over the given domain.</li>
<li>This simplifies computational operations during shading.</li>
</ul>
</li>
</ul>
<h4 id="key-points-1">
<a class="header-anchor" href="#key-points-1"></a>
Key Points
</h4><ul>
<li><strong>Lighting</strong> and <strong>light transport</strong> are precomputed in the frequency domain, enabling efficient real-time rendering.</li>
<li>Orthogonal basis systems allow clean and stable decomposition of spatial signals into coefficients.</li>
<li>The PRT framework transforms expensive integral operations into simple linear algebra.</li>
</ul>
<h3 id="spherical-harmonics-sh-in-prt">
<a class="header-anchor" href="#spherical-harmonics-sh-in-prt"></a>
Spherical Harmonics (SH) in PRT
</h3><p>PRT commonly uses <strong>spherical harmonics (SH)</strong> as basis functions for transformations.</p>
<ul>
<li>
<p>Many functions in real-time rendering, such as $L_i(\omega_i)$ and $T(\omega_i)$, are defined on a sphere.</p>
<ul>
<li>Using 2D Fourier transform for such functions may introduce seams on the spherical domain when reconstructing signals, while SH functions avoid this issue.</li>
</ul>
</li>
<li>
<p><strong>Environmental lighting maps</strong> can be stored as cube maps, and each face of the cube can be processed individually with SH transformations.</p>
</li>
<li>
<p><strong>Additional benefits of SH</strong>:</p>
<ul>
<li>SH allows for efficient computation of coefficients when rotating a light source.</li>
<li>SH basis functions form an orthonormal basis:
<ul>
<li>$\int_{\Omega} B_n(\omega) B_m(\omega) \, \mathrm{d}\omega = 0$ for $m \ne n$, and</li>
<li>$\int_{\Omega} \left[B_n(\omega)\right]^2 \, \mathrm{d}\omega = 1$.</li>
<li>This property ensures stability and simplifies mathematical operations.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Low-Order SH for Efficiency</strong>:</p>
<ul>
<li>Retaining only low-order SH terms in the frequency domain reduces computational complexity while maintaining acceptable accuracy for low-frequency effects like <strong>diffuse lighting</strong>.</li>
</ul>
</li>
<li>
<p><strong>Limitations of SH and Alternatives</strong></p>
<ul>
<li>
<p>SH is well-suited for low-frequency information, such as diffuse reflections. However, describing high-frequency details (e.g., specular highlights) requires higher-order SH terms, which increase computational cost.</p>
</li>
<li>
<p>To address this, some research explores alternative basis functions like the <strong>wavelet transform</strong>, which can capture high-frequency details more efficiently.</p>
</li>
</ul>
</li>
</ul>
<h3 id="prt-diffuse-case">
<a class="header-anchor" href="#prt-diffuse-case"></a>
PRT Diffuse Case
</h3>$$ 
f_r(\omega_i, \omega_o) = \rho \quad \text{and} \quad T(\omega_i, \omega_o) = T(\omega_i).
$$$$
\begin{align} 
L(\omega_o) &= \int_{\Omega^+} L_i(\omega_i) \cdot f_r(\omega_i, \omega_o) \cos\theta_i V(\omega_i) \,\mathrm{d}\omega_i  \notag \\ 
&= \int_{\Omega^+} \sum_p \left[l_p \cdot B_p(\omega_i)\right] \cdot \rho \cdot \cos\theta_i V(\omega_i) \,\mathrm{d}\omega_i  \notag \\ 
&= \rho \sum_p l_p \int_{\Omega^+} B_p(\omega_i) \cdot \cos\theta_i V(\omega_i) \,\mathrm{d}\omega_i  \notag \\ 
&= \rho \sum_p l_p \cdot T_p(\omega_i)  \notag \\ 
&= \rho \cdot  \notag 
\begin{bmatrix} 
l_0 & l_1 & \cdots & l_p 
\end{bmatrix} 
\begin{bmatrix} 
T_0(\omega_i) & T_1(\omega_i) & \cdots & T_p(\omega_i) 
\end{bmatrix}^T. 
\end{align}
$$$$
\begin{align} 
L(\omega_o) &= \int_{\Omega^+} L_i(\omega_i) \cdot f_r(\omega_i, \omega_o) \cos\theta_i V(\omega_i) \,\mathrm{d}\omega_i  \notag \\ 
&= \int_{\Omega^+} \sum_p \left[l_p \cdot B_p(\omega_i)\right] \cdot \sum_q \left[t_q \cdot B_q(\omega_i)\right] \,\mathrm{d}\omega_i  \notag \\ 
&= \rho \sum_p \sum_q l_p \cdot t_q \cdot \int_{\Omega^+} B_p(\omega_i) \cdot B_q(\omega_i) \,\mathrm{d}\omega_i  \notag \\ 
&\text{Note: } \int_{\Omega^+} B_p(\omega_i) \cdot B_q(\omega_i) \,\mathrm{d}\omega_i = 
\begin{cases} 
1 & \text{if } p = q \\ 
0 & \text{if } p \ne q 
\end{cases}  \notag \\
&\text{Thus, } L(\omega_o) = \rho \cdot \sum_k l_k t_k \quad \text{where } k = \min\{p, q\} \notag  \\ 
&= \rho \cdot 
\begin{bmatrix} 
l_0 & l_1 & \cdots & l_k 
\end{bmatrix} 
\begin{bmatrix} 
t_0 & t_1 & \cdots & t_k 
\end{bmatrix}^T.  \notag 
\end{align}
$$<p>For diffuse surfaces, the PRT shading computation at any point on the surface simplifies to a <strong>dot product between two vectors</strong>.</p>
<ul>
<li>For diffuse surfaces, using <strong>up to third-order spherical harmonics</strong> typically achieves good approximations.</li>
</ul>
<h3 id="prt-glossy-case">
<a class="header-anchor" href="#prt-glossy-case"></a>
PRT Glossy Case
</h3><p>For rendering equations involving glossy materials, the formula is given by:</p>
$$
\begin{align} 
L(\omega_o) &= \int_{\Omega^+} L_i(\omega_i) \cdot f_r(\omega_i, \omega_o) \cos\theta_i V(\omega_i) \,\mathrm{d}\omega_i  \notag \\ 
&= \int_{\Omega^+} \sum_p \left[l_p \cdot B_p(\omega_i)\right] \cdot f_r(\omega_i, \omega_o) \cos\theta_i V(\omega_i) \,\mathrm{d}\omega_i \notag \\ 
&= \sum_p l_p \underbrace{\int_{\Omega^+} B_p(\omega_i) \cdot f_r(\omega_i, \omega_o) \cos\theta_i V(\omega_i) \,\mathrm{d}\omega_i}_{T_p(\omega_i, \omega_o)}\notag  \\ 
&= \sum_p l_p \cdot T_p(\omega_i, \omega_o)\notag  \\ 
&= 
\begin{bmatrix} 
l_0 & l_1 & \cdots & l_p 
\end{bmatrix} 
\begin{bmatrix} 
T_0(\omega_i, \omega_o) \\ 
T_1(\omega_i, \omega_o) \\ 
\vdots \\ 
T_p(\omega_i, \omega_o) 
\end{bmatrix}.\notag 
\end{align}
$$<p>Alternatively, consider expanding the incoming radiance $L_i(\omega_i)$ and the reflectance $f_r(\omega_i, \omega_o)$ in terms of spherical harmonics:</p>
$$
\begin{align} 
L(\omega_o) &= \int_{\Omega^+} L_i(\omega_i) \cdot f_r(\omega_i, \omega_o) \cos\theta_i V(\omega_i) \,\mathrm{d}\omega_i \notag \\ 
&= \int_{\Omega^+} \sum_p \left[l_p \cdot B_p(\omega_i)\right] \cdot \sum_q \left[t_q(\omega_o) \cdot B_q(\omega_i)\right] \,\mathrm{d}\omega_i \notag \\ 
&= \sum_p \sum_q l_p \cdot t_q(\omega_o) \cdot \int_{\Omega^+} B_p(\omega_i) \cdot B_q(\omega_i) \,\mathrm{d}\omega_i \notag \\ 
&\text{Note: } \int_{\Omega^+} B_p(\omega_i) \cdot B_q(\omega_i) \,\mathrm{d}\omega_i = 
\begin{cases} 
1 & \text{if } p = q \\ 
0 & \text{if } p \ne q 
\end{cases} \notag \\ 
&\text{Thus, } L(\omega_o) = \sum_k l_k t_k(\omega_o), \quad \text{where } k = \min\{p, q\}.\notag 
\end{align}
$$<p>Next, expand the coefficient $t_k(\omega_o)$ in terms of an orthogonal basis $\{B_r(\omega_o)\}$:</p>
$$
t_k(\omega_o) = \sum_r \underbrace{t_{k,r}(\omega_o)}_{\text{Transfer matrix element}} \cdot \underbrace{B_r(\omega_o)}_{\text{Basis function}}.
$$<p>Substitute this back into the equation:</p>
$$
\begin{align} 
L(\omega_o) &= \sum_k l_k t_k(\omega_o) \notag \\ 
&= \sum_k l_k \sum_r t_{k,r} B_r(\omega_o) \notag \\ 
&= 
\begin{bmatrix} 
l_0 & l_1 & \cdots & l_k 
\end{bmatrix} 
\begin{bmatrix} 
t_{0,0} & t_{0,1} & \cdots & t_{0,r} \\ 
t_{1,0} & t_{1,1} & \cdots & t_{1,r} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
t_{k,0} & t_{k,1} & \cdots & t_{k,r} 
\end{bmatrix} 
\begin{bmatrix} 
B_0(\omega_o) \\ 
B_1(\omega_o) \\ 
\vdots \\ 
B_r(\omega_o) 
\end{bmatrix}.\notag 
\end{align}
$$<p>For glossy materials, the process for shading a point on a surface using PRT requires the following:</p>
<ul>
<li>Glossy materials demand higher-order spherical harmonics compared to diffuse surfaces.</li>
<li>In academic research, even 10th-order spherical harmonics may sometimes be insufficient for representing highly glossy surfaces effectively.</li>
</ul>
<h1 id="real-time-global-illumination">
<a class="header-anchor" href="#real-time-global-illumination"></a>
Real-Time Global Illumination
</h1><p><strong>Global Illumination (GI)</strong> refers to the comprehensive lighting effects in a scene, encompassing both direct and indirect lighting. It plays a crucial role in creating realistic graphics but is computationally complex.</p>
<p>In <strong>real-time rendering (RTR)</strong>, global illumination typically considers:</p>
<ul>
<li><strong>Direct lighting</strong> effects.</li>
<li><strong>Indirect lighting</strong> effects that involve light scattering once between surfaces in the scene.</li>
</ul>
<p>When a surface illuminated by a primary light source acts as a <strong>secondary light source</strong>, it scatters light and contributes to indirect lighting. Achieving this effect involves two core tasks:</p>
<ol>
<li>
<p><strong>Identifying secondary light sources</strong>:</p>
<ul>
<li>These are surfaces directly lit by the primary light source.</li>
</ul>
</li>
<li>
<p><strong>Calculating the contribution of secondary sources</strong>:</p>
<ul>
<li>The indirect lighting contribution of secondary light sources to the shading point is computed.</li>
</ul>
</li>
</ol>
<h2 id="image-space-algorithms">
<a class="header-anchor" href="#image-space-algorithms"></a>
Image-Space Algorithms
</h2><p>To compute direct lighting effects, algorithms often use a <strong>depth map</strong> generated from the light source&rsquo;s perspective.</p>
<p>For global illumination algorithms, if indirect lighting calculations rely solely on:</p>
<ul>
<li><strong>Camera-view information</strong> (e.g., screen space data).</li>
<li><strong>Depth maps</strong> (generated from the light&rsquo;s perspective).</li>
<li>And do <strong>not</strong> utilize additional 3D scene information from other viewpoints.</li>
</ul>
<p>Then such algorithms are categorized as <strong>image-space algorithms</strong>.</p>
<h2 id="reflective-shadow-maps-rsm">
<a class="header-anchor" href="#reflective-shadow-maps-rsm"></a>
Reflective Shadow Maps (RSM)
</h2><p><strong>Reflective Shadow Maps (RSM)</strong> is a real-time global illumination algorithm based on shadow maps. Shadow maps generated from the light source&rsquo;s perspective store information about surfaces directly illuminated by the light source. These directly lit surfaces act as <strong>secondary light sources</strong>, reflecting light to indirectly illuminate the scene. RSM are frequently used in video games for effects such as <strong>flashlight illumination</strong>.</p>
<h3 id="key-concepts-in-rsm">
<a class="header-anchor" href="#key-concepts-in-rsm"></a>
Key Concepts in RSM
</h3><ul>
<li>Each <strong>pixel in the shadow map</strong> corresponds to a surface patch in the scene.</li>
<li><strong>Diffuse material assumption</strong>:
<ul>
<li>RSM assumes these surface patches are diffuse, enabling the estimation of <strong>radiant flux</strong>.</li>
<li>The algorithm ignores visibility between secondary sources and shaded points, allowing efficient sampling to estimate indirect lighting.</li>
</ul>
</li>
</ul>
<h3 id="steps-in-the-rsm-algorithm">
<a class="header-anchor" href="#steps-in-the-rsm-algorithm"></a>
Steps in the RSM Algorithm
</h3><ol>
<li>
<p><strong>Generate the Reflective Shadow Map</strong>:</p>
<ul>
<li>In addition to storing scene <strong>depth</strong>, RSM records the following data for each pixel:
<ul>
<li><strong>World-space coordinates</strong> of the surface.</li>
<li><strong>Surface normal vector</strong>.</li>
<li><strong>Reflected direct lighting flux</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Calculate contributions during shading</strong>:</p>
<ul>
<li>Using the recorded data, RSM computes the contributions of direct and indirect lighting for a given point in the scene.</li>
</ul>
</li>
</ol>
<p>The derivation of the RSM algorithm for indirect lighting at shading point $p$ is as follows:</p>
<ul>
<li>
<p><strong>The rendering equation for indirect illumination at point $p$ from a surface patch $q$:</strong></p>
$$L_o^{\text{indir}}\left(p ,\, \omega_o\right) = \int_{A_\text{patch}} L_i\left(q \to p\right) f_{r}\left( p ,\, q \to p ,\, \omega_o \right) \cos\theta_p \cdot V\left( p \leftrightarrow q \right) \cdot \frac{\cos\theta_q \, \mathrm{d}A}{\left\| q - p \right\|^2}$$<ul>
<li>$\cos\theta_p$: Cosine of the angle between the normal at $p$ and the incident direction $\overrightarrow{pq}$.</li>
<li>$\cos\theta_q$: Cosine of the angle between the normal at $q$ and the outgoing direction $\overrightarrow{qp}$.</li>
<li>$\mathrm{d}A$: Differential area element at $q$.</li>
<li>$d\omega=\frac{dA\cos\theta^{\prime}}{\|x^{\prime}-x\|^2}$: Definition of the solid angle is the projected area on the unit sphere.</li>
</ul>
</li>
<li>
$$L_i\left(q \to p\right) = f_r\left(q ,\, \omega_i \to \omega_o\right) \cdot \frac{\Phi_q^{\prime}}{\mathrm{d}A}$$<ul>
<li>
<p>Assuming the patch at $q$ is diffuse:</p>
$$f_r\left(q ,\, \omega_i \to \omega_o\right) = \frac{\rho_q}{\pi}$$<p>where $\rho_q$ is the reflectance at $q$.</p>
</li>
<li>
<p>$\Phi_q^{\prime}$: Radiant flux received by $q$ from direct illumination.</p>
</li>
</ul>
</li>
<li>
<p><strong>Visibility $V(p \leftrightarrow q)$:</strong></p>
<ul>
<li>RSM assumes visibility is ignored $$ V\left( p \leftrightarrow q \right) = 1$$</li>
</ul>
</li>
<li>
<p><strong>Final expression for indirect illumination at $p$:</strong></p>
<ul>
<li>Substituting all components:</li>
</ul>
$$L_o^{\text{indir}}\left(p ,\, \omega_o\right) = \int_{A_\text{patch}} \left( \frac{\rho_q}{\pi} \cdot \frac{\Phi_q^{\prime}}{\mathrm{d}A} \right) f_{r}\left( p ,\, q \to p ,\, \omega_o \right) \cos\theta_p \cdot \frac{\cos\theta_q \, \mathrm{d}A}{\left\| q - p \right\|^2}$$<ul>
<li>Replacing the integral with a sum over sampled patches $q$:</li>
</ul>
$$L_o^{\text{indir}}\left(p ,\, \omega_o\right) = \sum_{q}\left[ f_{r}\left( p ,\, q \to p ,\, \omega_o \right) \cdot \frac{\rho_q}{\pi} \Phi_q^{\prime} \frac{\cos\theta_p \cos\theta_q}{\left\| q - p \right\|^2} \right]$$<ul>
<li>Simplifying further:</li>
</ul>
$$L_o^{\text{indir}}\left(p ,\, \omega_o\right) = \sum_{q}\left[ f_{r}\left( p ,\, q \to p ,\, \omega_o \right) \cdot \Phi_q \frac{\cos\theta_p \cos\theta_q}{\left\| q - p \right\|^2} \right]$$<p>where $\Phi_q = \frac{\rho_q}{\pi} \Phi_q^{\prime}$ represents the radiant flux reflected by $q$ due to direct illumination.</p>
</li>
</ul>
<h3 id="acceleration">
<a class="header-anchor" href="#acceleration"></a>
Acceleration
</h3><ul>
<li>A depth map with a resolution of $512 \times 512$ contains over <strong>260,000 surface patches</strong> $q$, each acting as a secondary light source.
<ul>
<li>Calculating the contribution of indirect illumination from all these patches is computationally expensive.</li>
</ul>
</li>
<li><strong>Sampling surface patches</strong> can significantly reduce computation costs:
<ul>
<li>The shading point is projected onto the depth map.</li>
<li>Patches farther from the shading point are assigned <strong>lower sampling probability densities</strong> but <strong>higher sampling weights</strong>.</li>
<li>This strategy selects a subset of patches and estimates the indirect illumination as the <strong>expected value</strong> of the samples.</li>
</ul>
</li>
</ul>
<h3 id="pros-and-cons">
<a class="header-anchor" href="#pros-and-cons"></a>
Pros and Cons
</h3><p><strong>Pros:</strong></p>
<ul>
<li><strong>Easy to implement</strong>: Integrates smoothly with existing shadow map pipelines.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><strong>Linear performance scaling</strong>: Performance becomes increasingly costly with the number of light sources.</li>
<li><strong>Lack of visibility checks</strong> for indirect illumination: Ignores occlusions between secondary light sources and shading points, which can affect realism.</li>
<li><strong>Assumptions made</strong>:
<ul>
<li>Assumes surfaces are diffuse reflectors.</li>
<li>Assumes depth equals distance, which may not always be the case.</li>
</ul>
</li>
<li><strong>Sampling rate vs. quality tradeoff.</strong></li>
</ul>
<h2 id="world-space-algorithms">
<a class="header-anchor" href="#world-space-algorithms"></a>
World Space Algorithms
</h2><p>World space algorithms are a category of global illumination techniques that, in addition to using <strong>image space information</strong> from both the camera’s and light source’s viewpoints, also utilize <strong>3D information about the scene</strong> directly within the world space.</p>
<h2 id="light-propagation-volumes-lpv">
<a class="header-anchor" href="#light-propagation-volumes-lpv"></a>
Light Propagation Volumes (LPV)
</h2><p>The <strong>Light Propagation Volumes (LPV)</strong> algorithm approximates indirect lighting by dividing the scene into a 3D grid and propagating radiance from secondary light sources through this grid. This method captures an approximate <strong>radiance field</strong> without performing detailed visibility checks between shading points and secondary light sources.</p>
<h3 id="key-principles-of-lpv">
<a class="header-anchor" href="#key-principles-of-lpv"></a>
Key Principles of LPV
</h3><ol>
<li>
<p><strong>3D Grid and Radiance Field Propagation</strong>:<br>
The scene is divided into <strong>3D grid cells (voxels)</strong>, and radiance from secondary light sources is propagated across these cells to approximate the indirect lighting radiance field.</p>
</li>
<li>
<p><strong>Handling Diffuse Surfaces</strong>:<br>
LPV does not make assumptions about secondary light source materials. However, <strong>high-frequency details are lost</strong> during radiance propagation, making it behave similarly to diffuse materials.</p>
</li>
<li>
<p><strong>No Visibility Checks Between Grid Cells</strong>:<br>
For efficient propagation, LPV ignores visibility checks between grid cells, leading to potential artifacts such as light leaking.</p>
</li>
</ol>
<h3 id="indirect-lighting-computation-with-lpv">
<a class="header-anchor" href="#indirect-lighting-computation-with-lpv"></a>
Indirect Lighting Computation with LPV
</h3><ul>
<li>
<p>For any shading point $p$, <strong>indirect radiance</strong> $L_i^{\text{indir}}(p ,\, \omega_i)$ is obtained directly from the 3D grid. This allows indirect illumination to be calculated through the rendering equation:</p>
$$L_o^{\text{indir}}(p ,\, \omega_o) = \int_{\Omega^{+}} \underbrace{L_i^{\text{indir}}(p ,\, \omega_i)}_{\text{Obtained from radiance field}} \, f_r(p, \omega_i, \omega_o) \cos\theta_p \, \mathrm{d}\omega_i $$</li>
</ul>
<h3 id="lpv-process-steps">
<a class="header-anchor" href="#lpv-process-steps"></a>
LPV Process Steps
</h3><ol>
<li>
<p><strong>Generate Secondary Light Sources</strong>:</p>
<ul>
<li>Using a <strong>Reflective Shadow Map (RSM)</strong>, the algorithm identifies scene surfaces illuminated by direct lighting. Each RSM pixel represents a <strong>secondary light source</strong>. For optimization, these surfaces are sampled to create virtual point lights.</li>
</ul>
</li>
<li>
<p><strong>Inject Radiance into the 3D Grid</strong>:</p>
<ul>
<li>The scene is pre-divided into a <strong>3D grid</strong>.</li>
<li>For each grid cell, the virtual point lights within it are gathered, and their radiance distribution is combined.</li>
<li>This radiance is projected onto <strong>spherical harmonics (SH)</strong>. Only the first two orders (four basis functions) are typically used in industry to reduce computational costs.</li>
</ul>
</li>
<li>
<p><strong>Propagate Radiance Through the Grid</strong>:</p>
<ul>
<li>Each cell collects radiance from its neighboring cells, updating its <strong>directional radiance distribution</strong> after each iteration.</li>
<li>This propagation is repeated <strong>4–5 times</strong> until the radiance distribution stabilizes.</li>
</ul>
</li>
<li>
<p><strong>Render Using Grid Radiance Data</strong>:</p>
<ul>
<li>During rendering, each shading point retrieves indirect lighting from its corresponding grid cell&rsquo;s stored <strong>directional radiance distribution</strong>.</li>
</ul>
</li>
</ol>
<h3 id="limitations-and-artifacts">
<a class="header-anchor" href="#limitations-and-artifacts"></a>
Limitations and Artifacts
</h3><ul>
<li><strong>Light Leaking</strong>:<br>
LPV can suffer from <strong>light leaking</strong> if grid cells are too large compared to the scene geometry. This is because LPV assumes <strong>uniform radiance distribution within each cell</strong>, which can cause unintended illumination on surfaces behind the actual illuminated geometry.</li>
</ul>
<h2 id="voxel-global-illumination-vxgi">
<a class="header-anchor" href="#voxel-global-illumination-vxgi"></a>
Voxel Global Illumination (VXGI)
</h2><p><strong>Voxel Global Illumination (VXGI)</strong> is an algorithm that achieves a global illumination effect closer to ray tracing compared to other techniques like RSM and LPV, though it generally has a higher computational cost.</p>
<h3 id="core-concept-of-vxgi">
<a class="header-anchor" href="#core-concept-of-vxgi"></a>
Core Concept of VXGI
</h3><ul>
<li>
<p><strong>Voxelization and Sparse Octree Structure</strong>:<br>
Before execution, the entire scene is discretized into <strong>voxels</strong> and organized into a <strong>sparse octree</strong> structure, creating a hierarchical data structure.</p>
</li>
<li>
<p><strong>Photon Mapping Approach</strong>:<br>
Similar to photon mapping in offline rendering, VXGI &ldquo;casts photons&rdquo; from the light source into the scene to collect information on secondary light sources. This information is later used during shading to calculate indirect lighting contributions.</p>
</li>
</ul>
<h3 id="vxgi-process-steps">
<a class="header-anchor" href="#vxgi-process-steps"></a>
VXGI Process Steps
</h3><ol>
<li>
<p><strong>Photon Injection from Light Source</strong>:</p>
<ul>
<li>Photons are cast from the light source into the scene.</li>
<li>Each voxel that receives photons records <strong>incident radiance and surface normal distributions</strong>. This information is stored in the leaf nodes of the sparse octree.</li>
</ul>
</li>
<li>
<p><strong>Radiance Propagation Up the Octree</strong>:</p>
<ul>
<li>For each non-leaf node, <strong>filtering is applied</strong> to the data from its child nodes. This creates a hierarchical representation of radiance and normal distributions, allowing coarser approximations of secondary light sources at each level of the octree.</li>
</ul>
</li>
<li>
<p><strong>Cone Tracing for Shading</strong>:</p>
<ul>
<li>During rendering, cone tracing is used to approximate indirect lighting. For each shading point, a <strong>cone is traced</strong> in the direction of reflection.</li>
<li>As the cone progresses, it accumulates both <strong>indirect lighting contributions</strong> and <strong>occlusion effects</strong> from the voxels it intersects, simulating light bouncing and shading.</li>
</ul>
</li>
</ol>
<h3 id="handling-different-surface-materials">
<a class="header-anchor" href="#handling-different-surface-materials"></a>
Handling Different Surface Materials
</h3><ul>
<li>
<p><strong>Glossy Surfaces</strong>:<br>
For glossy surfaces, cone tracing follows the ideal reflection direction, using a single cone for indirect lighting.</p>
</li>
<li>
<p><strong>Diffuse Surfaces</strong>:<br>
For diffuse surfaces, multiple cones (e.g., 8 cones) are used to approximate the scattered light. Gaps between cones are usually negligible in their impact.</p>
</li>
</ul>
<h3 id="cone-tracing-and-sparse-octree-acceleration">
<a class="header-anchor" href="#cone-tracing-and-sparse-octree-acceleration"></a>
Cone Tracing and Sparse Octree Acceleration
</h3><ul>
<li>
<p><strong>Cone Tracing Optimization</strong>:<br>
Cone tracing in VXGI leverages the <strong>sparse octree structure</strong> for efficiency. As the cone progresses along its axis, the current cone radius determines the appropriate <strong>octree level</strong>. At each step, interpolation is used to acquire <strong>geometry and lighting information</strong> from the corresponding voxel, which contributes to indirect lighting and occlusion.</p>
</li>
<li>
<p><strong>Indirect Lighting and Occlusion Calculation</strong>:<br>
Each voxel&rsquo;s <strong>indirect lighting contribution</strong> and <strong>occlusion factor</strong> are accumulated to compute the final indirect illumination and occlusion at the shading point.</p>
<ul>
<li>Indirect lighting contribution $c$ is computed with the formula: $c = \alpha c + \left(1-\alpha\right)\alpha_2 c_2$</li>
<li>Occlusion factor $\alpha$ is updated as: $\alpha = \alpha + \left(1-\alpha\right) \alpha_2$</li>
</ul>
</li>
</ul>
<h3 id="vxgi-vs-lpv-and-rsm">
<a class="header-anchor" href="#vxgi-vs-lpv-and-rsm"></a>
VXGI vs. LPV and RSM
</h3><ul>
<li>
<p><strong>Voxelization vs. 3D Grids</strong>:</p>
<ul>
<li>VXGI requires <strong>voxelization of the entire scene</strong>, which is resource-intensive. Unlike LPV, where grid cells have no direct correlation with scene geometry, VXGI’s voxel representation is tied closely to the actual objects, adding complexity and resource demands.</li>
</ul>
</li>
<li>
<p><strong>Material Flexibility</strong>:</p>
<ul>
<li><strong>RSM</strong> assumes surfaces are <strong>diffuse</strong> when acting as secondary light sources.</li>
<li><strong>LPV</strong> similarly captures mostly low-frequency, diffuse-like lighting by projecting directional radiance onto spherical harmonics.</li>
<li><strong>VXGI</strong> allows both <strong>diffuse and glossy materials</strong> as secondary light sources, making it a more flexible choice.</li>
</ul>
</li>
<li>
<p><strong>Direct vs. Indirect Calculation</strong>:</p>
<ul>
<li>LPV uses radiance propagation within a 3D grid to approximate the influence of secondary lighting on a shading point.</li>
<li><strong>RSM</strong> directly calculates secondary lighting influence but does not account for visibility between points.</li>
<li><strong>VXGI</strong> directly calculates secondary light contributions while accounting for <strong>visibility</strong> using cone tracing, offering more accurate occlusion and lighting.</li>
</ul>
</li>
</ul>
<h3 id="advantages-and-trade-offs">
<a class="header-anchor" href="#advantages-and-trade-offs"></a>
Advantages and Trade-Offs
</h3><ul>
<li><strong>Quality and Realism</strong>:
<ul>
<li><strong>VXGI</strong> offers a global illumination quality closer to <strong>ray tracing</strong>, with fewer assumptions than LPV and RSM, achieving higher realism at the cost of computational intensity.</li>
</ul>
</li>
<li><strong>Resource Demand</strong>:
<ul>
<li><strong>Voxelization complexity</strong> and high <strong>resource consumption</strong> limit VXGI’s applicability, especially in real-time graphics under constrained hardware.</li>
</ul>
</li>
</ul>
<h3 id="application-example">
<a class="header-anchor" href="#application-example"></a>
Application Example
</h3><ul>
<li><strong>Marvel&rsquo;s Spider-Man</strong>:
<ul>
<li>This game demonstrates VXGI&rsquo;s high-quality global illumination, resulting in realistic and immersive lighting effects.</li>
</ul>
</li>
</ul>
<h2 id="screen-space-algorithms">
<a class="header-anchor" href="#screen-space-algorithms"></a>
Screen Space Algorithms
</h2><p>Screen space algorithms for <strong>global illumination</strong> generate indirect lighting effects using only information from the <strong>camera&rsquo;s view</strong>. This approach applies <strong>post-processing</strong> to a scene rendered with only direct lighting, adding indirect lighting effects afterward.</p>
<h3 id="ssao-screen-space-ambient-occlusion">
<a class="header-anchor" href="#ssao-screen-space-ambient-occlusion"></a>
SSAO (Screen Space Ambient Occlusion)
</h3><p>SSAO (Screen Space Ambient Occlusion) is a technique used to approximate global illumination effects in screen space. The method assumes <strong>constant indirect lighting</strong> and that all surfaces are <strong>diffuse materials</strong>. By estimating the <strong>visibility of shading points</strong>, SSAO directly calculates the effects of indirect illumination.</p>
<p>For <strong>uniform incident lighting</strong> ($L_i = \text{constant}$) and <strong>diffuse BSDF</strong> ($f_r = \frac{\rho}{\pi}$), both terms can be factored out of the integral:</p>
$$ L_o(p, \omega_o) = \int_{\Omega^+} L_i(p, \omega_i) f_r(p, \omega_i, \omega_o) V(p, \omega_i) \cos \theta_i \, \mathrm{d}\omega_i $$$$= \frac{\rho}{\pi} \cdot L_i(p) \cdot \int_{\Omega^+} V(p, \omega_i) \cos \theta_i \, \mathrm{d}\omega_i $$<h3 id="derivation-of-ssao">
<a class="header-anchor" href="#derivation-of-ssao"></a>
Derivation of SSAO
</h3><p>The derivation of Screen Space Ambient Occlusion (SSAO) starts from the <strong>rendering equation</strong>:</p>
$$L_o(p,\omega_o) = \int_{\Omega^+} L_i(p,\omega_i) f_r(p,\omega_i,\omega_o) V(p,\omega_i) \cos\theta_i \, \mathrm{d}\omega_i$$<p>Where:</p>
<ul>
<li>$L_o(p, \omega_o)$: Outgoing radiance at point $p$ in direction $\omega_o$.</li>
<li>$L_i(p, \omega_i)$: Incoming radiance at $p$ from direction $\omega_i$.</li>
<li>$f_r$: Bidirectional scattering distribution function (BSDF).</li>
<li>$V(p, \omega_i)$: <strong>Visibility function</strong>, indicating whether light reaches $p$ from $\omega_i$.</li>
<li>$\cos \theta_i$: Angle between surface normal and incoming light.</li>
<li>$\Omega^+$: Hemisphere above the surface at $p$.</li>
</ul>
<h4 id="separating-the-visibility-term">
<a class="header-anchor" href="#separating-the-visibility-term"></a>
Separating the Visibility Term
</h4><p>The equation is approximated by isolating the <strong>visibility function $V(p, \omega_i)$</strong>:</p>
$$L_o^{\text{indir}}(p, \omega_o) \approx  
\frac{\int_{\Omega^+} V(p, \omega_i) \cos \theta_i \, \mathrm{d}\omega_i}{\int_{\Omega^+} \cos \theta_i \, \mathrm{d}\omega_i}  
\cdot \int_{\Omega^+} L_i^{\text{indir}}(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos \theta_i \, \mathrm{d}\omega_i $$<ul>
<li>
<p>The first term represents <strong>weight-averaged visibility</strong> ($\overline{V}$) over all directions:</p>
$$\frac{\int_{\Omega^+} V(p, \omega_i) \cos \theta_i \, \mathrm{d}\omega_i}{\int_{\Omega^+} \cos \theta_i \, \mathrm{d}\omega_i}  
    \triangleq k_A $$</li>
<li>
<p>For Ambient Occlusion (AO), $k_A$ simplifies to:</p>
$$k_A = \frac{\int_{\Omega^+} V(p, \omega_i) \cos \theta_i \, \mathrm{d}\omega_i}{\pi} $$</li>
<li>
<p>The second term simplifies under assumptions of <strong>diffuse BSDF</strong> and <strong>uniform indirect lighting</strong>:</p>
<ul>
<li>$L_i^{\text{indir}}(p, \omega_i)$: Assumed constant ($L_i^{\text{indir}}(p)$).</li>
<li>Diffuse BRDF ($f_r = \frac{\rho}{\pi}$) is also constant.</li>
</ul>
</li>
<li>
<p>Thus:</p>
</li>
</ul>
$$\int_{\Omega^+} L_i^{\text{indir}}(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos \theta_i \, \mathrm{d}\omega_i  
= L_i^{\text{indir}}(p) \cdot \frac{\rho}{\pi} \cdot \pi = L_i^{\text{indir}}(p) \cdot \rho $$<h4 id="why-include-cos-theta_i-in-visibility-term">
<a class="header-anchor" href="#why-include-cos-theta_i-in-visibility-term"></a>
Why Include $\cos \theta_i$ in Visibility Term?
</h4>$$\int_{\Omega} f(x) g(x) \, \mathrm{d}x \approx \frac{\int_{\Omega_G} f(x) \, \mathrm{d}x}{\int_{\Omega_G} \, \mathrm{d}x} \cdot \int_{\Omega} g(x) \, \mathrm{d}x$$<p>For the approximation, the $ \mathrm{d}x $ term is replaced by the <strong>projected solid angle</strong> $\mathrm{d}x_\perp = \cos \theta_i \, \mathrm{d}\omega_i$.</p>
<ul>
<li>
<p>A projected solid angle is defined as $\mathrm{d}x_\perp = \cos \theta_i \, \mathrm{d}\omega_i$.</p>
</li>
<li>
<p>Mapping the hemisphere ($\Omega^+$) to a <strong>unit disk</strong> results in an integration equal to the area of the disk ($\pi$):</p>
$$\int_{\Omega^+} \cos\theta_i \, \mathrm{d}\omega_i = \pi$$</li>
</ul>
<h3 id="real-time-computation-of-k_ap-in-ssao">
<a class="header-anchor" href="#real-time-computation-of-k_ap-in-ssao"></a>
Real-Time Computation of $k_A(p)$ in SSAO
</h3><p>To compute $k_A(p)$ in real time, SSAO uses <strong>random sampling</strong> within a sphere centered at the shading point. These samples are tested for <strong>visibility</strong> using the <strong>depth buffer (z-buffer)</strong> to determine whether they are visible from the camera. The ratio of visible samples is used to estimate the visibility of the shading point.</p>
<h4 id="key-points-2">
<a class="header-anchor" href="#key-points-2"></a>
Key Points
</h4><ul>
<li>
<p><strong>Surface Normal Direction Ignored</strong>:<br>
SSAO does not account for the <strong>surface normal direction</strong> of the shading point during visibility estimation.</p>
<ul>
<li>To mitigate this, SSAO only applies if <strong>more than half</strong> of the sample points are occluded.</li>
</ul>
</li>
<li>
<p><strong>Cosine Weighting Not Applied</strong>:<br>
The average visibility $k_A(p)$ is calculated without <strong>cosine weighting</strong>, which makes the result <strong>physically inaccurate</strong>. However, it still produces visually plausible results.</p>
</li>
<li>
<p><strong>Camera Visibility vs. Shading Point Visibility</strong>:</p>
<ul>
<li><strong>Camera visibility</strong> derived from the z-buffer may differ from the true <strong>shading point visibility</strong>.</li>
<li>The z-buffer only provides an <strong>approximation</strong> of the scene&rsquo;s geometry, leading to potential inaccuracies like <strong>false occlusions</strong> (shadows where there should be none).</li>
</ul>
</li>
</ul>
<h4 id="sampling-in-ssao">
<a class="header-anchor" href="#sampling-in-ssao"></a>
Sampling in SSAO
</h4><ul>
<li>
<p><strong>Number of Samples</strong>:</p>
<ul>
<li>Increasing the number of samples improves accuracy.</li>
<li>For performance reasons, <strong>only ~16 samples</strong> are typically used.</li>
</ul>
</li>
<li>
<p><strong>Randomized Sampling</strong>:</p>
<ul>
<li>Sample positions are derived from a <strong>randomized texture</strong> to prevent banding artifacts.</li>
</ul>
</li>
<li>
<p><strong>Noise and Smoothing</strong>:</p>
<ul>
<li>The result often contains noise due to the limited sample count.</li>
<li>A <strong>blur filter with edge preservation</strong> is applied to smooth the output without sacrificing sharpness at object boundaries.</li>
</ul>
</li>
<li>
<p><strong>Limited Radius</strong>:</p>
<ul>
<li>SSAO is limited to a <strong>local occlusion radius</strong> to avoid capturing occlusion from distant objects.</li>
<li>This limitation is beneficial for <strong>enclosed areas</strong> like interiors, where distant occlusion would be unrealistic.</li>
</ul>
</li>
</ul>
<h3 id="horizon-based-ambient-occlusion-hbao">
<a class="header-anchor" href="#horizon-based-ambient-occlusion-hbao"></a>
Horizon-Based Ambient Occlusion (HBAO)
</h3><p>HBAO is an improvement over SSAO, also implemented in <strong>screen space</strong>, and provides more accurate ambient occlusion by addressing the limitations of SSAO.</p>
<h4 id="key-characteristics">
<a class="header-anchor" href="#key-characteristics"></a>
Key Characteristics
</h4><ul>
<li><strong>Normal Direction Considered</strong>:<br>
HBAO requires surface normals to be known and <strong>only samples within a hemisphere</strong> above the shading point, aligned with the normal.</li>
</ul>
<h4 id="why-hbao-is-more-accurate">
<a class="header-anchor" href="#why-hbao-is-more-accurate"></a>
Why HBAO is More Accurate
</h4><ul>
<li>
<p><strong>Limited Range of Occlusion</strong>:</p>
<ul>
<li>HBAO <strong>only considers occlusion within a specific range</strong>, avoiding SSAO&rsquo;s simplistic assumption where any nearby geometry is treated as fully occluding, regardless of distance.</li>
<li>This approach reduces <strong>false occlusions</strong> and produces more accurate shading.</li>
</ul>
</li>
<li>
<p><strong>Directional Sampling</strong>:</p>
<ul>
<li>By sampling within a <strong>bounded hemisphere</strong>, HBAO incorporates the surface normal&rsquo;s influence, making the visibility calculation more physically accurate.</li>
</ul>
</li>
</ul>
<h3 id="ssdo-screen-space-directional-occlusion">
<a class="header-anchor" href="#ssdo-screen-space-directional-occlusion"></a>
SSDO (Screen Space Directional Occlusion)
</h3><p>SSDO (Screen Space Directional Occlusion) improves upon SSAO by introducing the concept of <strong>directional and non-uniform indirect lighting</strong>, leading to more realistic global illumination effects.</p>
<h4 id="core-idea">
<a class="header-anchor" href="#core-idea"></a>
Core Idea
</h4><ul>
<li>Unlike SSAO, which assumes indirect lighting comes from a constant and distant source, <strong>SSDO assumes indirect lighting originates from nearby surfaces</strong>.</li>
<li>If an object occludes the shading point from the camera, the occluder acts as a <strong>secondary light source</strong>, contributing indirect illumination to the shading point.</li>
<li>Direct lighting, meanwhile, enters the shading point from directions that are not occluded.</li>
</ul>
<h4 id="advantages-of-ssdo">
<a class="header-anchor" href="#advantages-of-ssdo"></a>
Advantages of SSDO
</h4><ul>
<li>
<p><strong>Non-Uniform Indirect Illumination</strong>:</p>
<ul>
<li>Indirect light is influenced by nearby surfaces, resulting in <strong>color bleeding effects</strong>.</li>
<li>The final global illumination effect is more realistic compared to SSAO&rsquo;s uniform approach.</li>
</ul>
</li>
</ul>
<h4 id="steps-of-the-ssdo-algorithm">
<a class="header-anchor" href="#steps-of-the-ssdo-algorithm"></a>
Steps of the SSDO Algorithm
</h4><ol>
<li>
<p><strong>Sampling in a Hemisphere</strong>:</p>
<ul>
<li>Around the shading point $P$, sample points within a hemisphere defined by the <strong>normal $n$</strong> and a fixed radius $r_\text{max}$.</li>
<li>Project these sample points into the <strong>camera&rsquo;s shadow map</strong> to find corresponding surface points in the scene.</li>
</ul>
</li>
<li>
<p><strong>Occlusion and Secondary Source Detection</strong>:</p>
<ul>
<li>Compare the depth of the sampled surface points with the depth of the shading point $P$:
<ul>
<li>If a surface point&rsquo;s depth is <strong>smaller</strong> than $P$&rsquo;s depth, it acts as a <strong>potential secondary light source</strong> that could occlude $P$.</li>
<li>If a surface point&rsquo;s depth is <strong>greater</strong>, it corresponds to a direction from which <strong>direct light</strong> reaches $P$.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Indirect Lighting Accumulation</strong>:</p>
<ul>
<li>For potential secondary light sources, use their <strong>surface normals</strong> and <strong>direct lighting information</strong> to compute their contribution to $P$&rsquo;s indirect illumination.</li>
<li>Accumulate the contributions to get the final indirect lighting at $P$.</li>
</ul>
</li>
</ol>
<h4 id="example-workflow">
<a class="header-anchor" href="#example-workflow"></a>
<strong>Example Workflow</strong>
</h4><ul>
<li>For a given shading point $P$, with normal $n$:
<ul>
<li>Sample points $A, B, C, D$ within a hemisphere of radius $r_\text{max}$.</li>
<li>Project these points into the shadow map to locate their corresponding surface points.</li>
<li>Compare depths:
<ul>
<li>$C$&rsquo;s depth is greater than $P$&rsquo;s depth → $C$ represents a <strong>direct lighting direction</strong>.</li>
<li>$A, B, D$&rsquo;s depths are smaller → They are <strong>potential secondary light sources</strong>.</li>
</ul>
</li>
<li>Use surface normals and lighting data to determine indirect light contributions:
<ul>
<li>Only $B, D$ contribute indirect illumination based on their orientations.</li>
</ul>
</li>
<li>Accumulate the indirect light contributions to compute the final shading for $P$.</li>
</ul>
</li>
</ul>
<h3 id="issues-with-ssdo">
<a class="header-anchor" href="#issues-with-ssdo"></a>
Issues with SSDO
</h3><p>While SSDO can achieve near offline rendering quality, it has certain limitations that impact its accuracy and applicability.</p>
<ul>
<li>
<p><strong>Visibility Misjudgments</strong>:</p>
<ul>
<li>SSDO uses <strong>depth comparisons</strong> to determine visibility between the shading point and sample points.</li>
<li>This approach can lead to <strong>errors when other objects obscure the sampled points</strong>:
<ul>
<li>For example, a sample point $A$ corresponds to a surface point with depth $z_1$, smaller than the shading point $P$&rsquo;s depth.</li>
<li>SSDO might incorrectly treat $A$ as a <strong>secondary light source</strong>, when it actually represents a direction of <strong>direct lighting</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Limited Sampling Range</strong>:</p>
<ul>
<li>SSDO only considers global illumination within a <strong>small area</strong> around the shading point.</li>
<li>If the source of indirect lighting is <strong>too far</strong> from the shading point, SSDO fails to capture its contribution, resulting in <strong>inaccurate colors</strong>.</li>
</ul>
</li>
<li>
<p><strong>Loss of Hidden Information</strong>:</p>
<ul>
<li>As a <strong>screen-space algorithm</strong>, SSDO only processes information visible from the <strong>camera&rsquo;s perspective</strong>.</li>
<li>Any occluded or off-screen surfaces are ignored, leading to a loss of potential indirect light contributions.</li>
</ul>
</li>
</ul>
<h2 id="screen-space-reflection-ssr">
<a class="header-anchor" href="#screen-space-reflection-ssr"></a>
Screen Space Reflection (SSR)
</h2><p>SSAO assumes indirect lighting comes from a distant source, while SSDO assumes it comes from near the shading point. In reality, indirect lighting can originate from both distant sources and nearby surfaces. <strong>SSR (Screen Space Reflection)</strong> simulates ray tracing in screen space, accounting for both distant and nearby indirect lighting.</p>
<p>SSR&rsquo;s execution, given a shading point $p$, involves three main steps:</p>
<ol>
<li>
<p><strong>Determine possible incoming light directions</strong> $\omega_i$ at the shading point:</p>
<ul>
<li>Use an importance sampling method to generate possible reflection directions based on the surface&rsquo;s BRDF $f_r(p, \omega_i, \omega_o)$. This process samples more probable reflection directions and assigns them probability densities $\mathrm{pdf}(\omega_i)$.</li>
</ul>
</li>
<li>
<p><strong>Trace rays in screen space along sampled directions</strong>:</p>
<ul>
<li>Rather than finding intersections in 3D world space (e.g., using BVH for 3D primitives), SSR finds intersections in 2D screen space. Here, it uses depth image pyramids (e.g., depth mipmaps) as a 2D acceleration structure.</li>
<li>If two shading points are close, their ray intersections with the scene may contribute to each other&rsquo;s indirect lighting, allowing intersection reuse to speed up calculations.</li>
<li>SSR can leverage prefiltering from the split sum method: apply a filter to screen-space information so that tracing a single ideal reflection ray yields results for multiple reflection directions. Since each point has a unique depth, depth variation must be considered during filtering.</li>
</ul>
</li>
<li>
<p><strong>Calculate indirect lighting assuming diffuse surface at intersection</strong>:</p>
<ul>
<li>If the intersected surface at point $q$ is diffuse, use its color as the reflected light color $L_o(q, q \to p)$ and solve the rendering equation for indirect lighting:</li>
</ul>
$$
   \begin{align} 
   L_o^{\text{indir}}(p, \omega_o) &= \int_{\Omega^+} L_i(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos\theta_i \,\mathrm{d}\omega_i \notag\\ 
   &= \sum_{\omega_i}\frac{L_i(p, \omega_i) f_r(p, \omega_i, \omega_o) \cos\theta_i}{ \mathrm{pdf}(\omega_i) } \notag\\ 
   &= \sum_q \frac{L_o(q, q \to p) f_r(p, q \to p, \omega_o) \cos\theta_i}{ \mathrm{pdf}(q \to p) } \notag
   \end{align}
   $$</li>
</ol>
<p>SSR is essentially Monte Carlo ray tracing, capturing only direct lighting and indirect lighting from a single scatter. It assumes that surfaces scatter light as diffuse material and relies on <strong>depth mipmaps</strong> (2D) rather than 3D acceleration structures like BVH or k-d trees.</p>
<p><strong>Requires only scene information visible from the camera&rsquo;s perspective</strong>, which is available from the geometry buffer in deferred rendering.</p>
<p><strong>Limitations</strong>: Since depth mipmaps only store visible surfaces from the camera&rsquo;s perspective, SSR lacks information about surfaces occluded or outside the screen view.</p>
<p><strong>To prevent harsh reflection cutoffs for objects extending beyond the screen bounds</strong>, SSR applies an additional decay factor when tracing reflection rays. Although not physically accurate, this produces more natural, progressively blurred reflections compared to abrupt cutoffs.</p>
<h1 id="physically-based-rendering">
<a class="header-anchor" href="#physically-based-rendering"></a>
Physically-Based Rendering
</h1><ul>
<li>
<p><strong>Definition</strong>:</p>
<ul>
<li>
<p><strong>Physically-Based Rendering (PBR)</strong> refers to rendering techniques where everything—materials, lighting, cameras, and light transport—is grounded in physical principles.</p>
</li>
<li>
<p>While PBR extends to various aspects of rendering, it is most commonly associated with <strong>physically-based materials</strong>.</p>
</li>
</ul>
</li>
<li>
<p><strong>PBR in Real-Time Rendering (RTR)</strong>:</p>
<ul>
<li>Real-time rendering (RTR) often uses <strong>approximations</strong> for efficiency, making it less physically accurate than offline rendering.</li>
<li>Examples include:
<ul>
<li><strong>Microfacet Models</strong>:
<ul>
<li>Widely used for general surface modeling.</li>
<li>These models are based on physics but are sometimes applied <strong>in physically incorrect ways</strong>.</li>
</ul>
</li>
<li><strong>Disney Principled BRDFs</strong>:
<ul>
<li>Designed for ease of use by artists rather than strict physical correctness.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Handling Complex Scattering</strong>:</p>
<ul>
<li>For materials like <strong>participating media</strong> (e.g., fog or smoke) or <strong>hair</strong>, where light scattering is complex, computations are often broken down into simpler stages:
<ul>
<li><strong>Single Scattering</strong>: Calculates light interacting with the medium once.</li>
<li><strong>Multiple Scattering</strong>: Estimates light interacting with the medium multiple times.</li>
<li>The results from these computations are combined to produce the final effect.</li>
</ul>
</li>
<li>Example:
<ul>
<li><strong>Dual Scattering Algorithm</strong>: A fast approximation for modeling <strong>multiple light scattering in hair</strong>, widely used in real-time rendering for hair effects.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="microfacet-brdf">
<a class="header-anchor" href="#microfacet-brdf"></a>
Microfacet BRDF
</h2><p><strong>Microfacet Models</strong> are physically-based local lighting models. They assume that a surface is composed of many tiny, flat microfacets, each capable of reflecting or refracting light perfectly. This roughness at the microscopic level leads to the macroscopic appearance of the surface.</p>
<p>The BRDF (Bidirectional Reflectance Distribution Function) for a microfacet model is given as:</p>
$$f_\text{r}\left(\omega_i ,\, \omega_o \right) = \frac{F(\omega_i,\,h)\, G(\omega_i,\,\omega_o,\,h) \, D(h)}{4 \,\left| \omega_i\cdot n\right| \,\left| \omega_o\cdot n\right|}$$<ul>
<li>
<p><strong>Vectors</strong>:</p>
<ul>
<li>$n$: The macro surface <strong>normal vector</strong>.</li>
<li>$h = \frac{\omega_i + \omega_o}{\|\omega_i + \omega_o\|}$: The <strong>halfway vector</strong>, representing the microfacet&rsquo;s normal direction.</li>
<li>$\omega_i$: The incident light direction.</li>
<li>$\omega_o$: The outgoing light direction.</li>
</ul>
</li>
<li>
<p><strong>Terms in the BRDF</strong>:</p>
<ul>
<li><strong>Fresnel Term</strong> $F(\omega_i, h)$:
<ul>
<li>Describes the <strong>proportion of light</strong> reflected at the boundary of two media, based on the angle of incidence and material properties.</li>
</ul>
</li>
<li><strong>Normal Distribution Function (NDF)</strong> $D(h)$:
<ul>
<li>Determines the fraction of microfacets aligned with the halfway vector $h$.</li>
<li>Governs how rough or smooth the surface appears.</li>
</ul>
</li>
<li><strong>Shadowing-Masking Term</strong> $G(\omega_i, \omega_o, h)$:
<ul>
<li>Accounts for <strong>self-shadowing</strong> and <strong>masking</strong> between microfacets.</li>
<li>Reduces the effective light contribution due to occlusion between microfacets.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="fresnel-term">
<a class="header-anchor" href="#fresnel-term"></a>
Fresnel Term
</h3><p>The <strong>Fresnel Term</strong> describes the proportion of light reflected by a surface as a function of the <strong>incident angle</strong> and <strong>polarization</strong> of light.</p>
<h4 id="fresnel-equations-for-dielectrics">
<a class="header-anchor" href="#fresnel-equations-for-dielectrics"></a>
Fresnel Equations for Dielectrics
</h4><p>The Fresnel reflectance for <strong>perpendicular</strong> and <strong>parallel polarization components</strong> is given as:</p>
<p><strong>Perpendicular Polarization $R_s$:</strong></p>
$$R_s = \left| \frac{\eta_i\cos\theta_i - \eta_t\cos\theta_t}{\eta_i\cos\theta_i + \eta_t\cos\theta_t} \right|^2$$<p><strong>Parallel Polarization $R_p$:</strong></p>
$$R_p = \left| \frac{\eta_i\cos\theta_t - \eta_t\cos\theta_i}{\eta_i\cos\theta_t + \eta_t\cos\theta_i} \right|^2$$<p>The effective Fresnel term is:</p>
$$R_\text{eff} = \frac{1}{2}(R_s + R_p)$$<ul>
<li>$\eta_i$, $\eta_t$: Refractive indices of the incident and transmitted media.</li>
<li>For conductors (metals), $\eta_t$ becomes a <strong>complex number</strong> to account for the extinction coefficient.</li>
</ul>
<h4 id="schlicks-approximation">
<a class="header-anchor" href="#schlicks-approximation"></a>
Schlick&rsquo;s Approximation
</h4><p>Schlick&rsquo;s approximation simplifies the Fresnel term:</p>
$$R_{\text{Schlick}}\left(\theta\right) = R_0 + (1 - R_0)(1 - \cos\theta)^5$$<ul>
<li>$R_0 = \left(\frac{\eta_i - \eta_t}{\eta_i + \eta_t}\right)^2$: Reflectance at normal incidence.</li>
<li><strong>Accuracy:</strong>
<ul>
<li>For dielectrics, average error $ < 1\%$, maximum error $ 3.6\%$ at high angles.</li>
<li>For metals, the extinction coefficient $k$ must be considered for better accuracy.</li>
</ul>
</li>
</ul>
<h3 id="fresnel-approximations-for-metals">
<a class="header-anchor" href="#fresnel-approximations-for-metals"></a>
Fresnel Approximations for Metals
</h3><ol>
<li>
<p><strong>Lazániy and Szirmay-Kalos Approximation:</strong><br>
Expands Schlick&rsquo;s model to include the extinction coefficient $k$:</p>
$$R_{\text{Lazániy}}(\theta) = \frac{(\eta - 1)^2 + k^2 + 4\eta(1 - \cos\theta)^5}{(\eta + 1)^2 + k^2}$$<ul>
<li>For metals with large $\eta$ and $k$, additional compensation terms can reduce errors.</li>
</ul>
</li>
<li>
<p><strong>Hoffman’s Approximation:</strong><br>
Refines Lazániy’s approach with a correction term to minimize errors at grazing angles:</p>
$$R_{\text{Hoffman}}(\theta) \approx r + (1 - r)(1 - \cos\theta)^5 - a\cos\theta(1 - \cos\theta)^\alpha$$<ul>
<li>$r = \frac{(\eta - 1)^2 + k^2}{(\eta + 1)^2 + k^2}$: Reflectance at normal incidence.</li>
<li>$\alpha = 6$, $a$: Derived parameters.</li>
</ul>
</li>
</ol>
<h4 id="key-observations">
<a class="header-anchor" href="#key-observations"></a>
Key Observations
</h4><ul>
<li><strong>Schlick&rsquo;s approximation</strong> is efficient but introduces significant errors for <strong>metals</strong> or at high grazing angles.</li>
<li><strong>Advanced methods</strong> (e.g., Lazániy, Hoffman) offer high accuracy, especially for metals like aluminum with large $k$.</li>
<li>These advanced methods reduce errors to less than <strong>0.65%</strong>, significantly improving realism in metallic surfaces.</li>
</ul>
<h3 id="normal-distribution-function-ndf">
<a class="header-anchor" href="#normal-distribution-function-ndf"></a>
Normal Distribution Function (NDF)
</h3><p>The <strong>Normal Distribution Function (NDF)</strong> determines the proportion of microfacet normals oriented toward a given direction $h$. It is the <strong>probability density function (PDF)</strong> defined over the hemisphere above a surface.</p>
<h4 id="key-properties-of-ndf">
<a class="header-anchor" href="#key-properties-of-ndf"></a>
Key Properties of NDF
</h4><ul>
<li>The distribution&rsquo;s <strong>mean</strong> is the half-vector $h$.</li>
<li><strong>Variance</strong> describes material roughness:
<ul>
<li><strong>Small variance</strong>: The distribution is concentrated, representing <strong>glossy</strong> materials.</li>
<li><strong>Large variance</strong>: The distribution is spread out, representing <strong>diffuse</strong> materials.</li>
</ul>
</li>
</ul>
<h4 id="common-ndf-models">
<a class="header-anchor" href="#common-ndf-models"></a>
Common NDF Models
</h4><ol>
<li>
<p><strong>Beckmann Distribution</strong><br>
Derived from a normal distribution in <strong>slope space</strong>:</p>
$$D_\text{Beckmann}(h) = \frac{1}{\pi \alpha^2 \cos^4\theta_h} e^{-\frac{\tan^2\theta_h}{\alpha^2}}$$<ul>
<li>$\alpha$: Surface roughness parameter.</li>
<li>$\theta_h$: Angle between the microfacet normal and the surface normal.</li>
</ul>
</li>
<li>
<p><strong>GGX Distribution (Trowbridge-Reitz)</strong><br>
Known for its <strong>long tail</strong>, meaning probabilities decrease more gradually for deviations from the mean:</p>
$$D_\text{GGX}(h) = \frac{\alpha^2}{\pi \cos^4\theta_h (\alpha^2 + \tan^2\theta_h)^2}$$<ul>
<li>Under the same level of roughness, <strong>GGX produces more natural results</strong> because its <strong>long tail property</strong> ensures a <strong>smooth transition</strong> from highlights to non-highlighted areas. In contrast, <strong>Beckmann’s sharp falloff</strong> causes highlights to abruptly terminate at grazing angles, which looks less realistic.</li>
</ul>
</li>
</ol>
<h4 id="generalized-trowbridge-reitz-gtr">
<a class="header-anchor" href="#generalized-trowbridge-reitz-gtr"></a>
Generalized-Trowbridge-Reitz (GTR)
</h4><p>Brent Burley introduced the <strong>GTR distribution</strong> in the Disney Principled BRDF:</p>
$$D_\text{GTR}(h) = \frac{c}{(\alpha^2\cos^2\theta_h + \sin^2\theta_h)^\gamma}$$<ul>
<li>$c$: Normalization constant.</li>
<li>When $\gamma = 2$, $c = \frac{\alpha^2}{\pi}$, and GTR reduces to <strong>GGX</strong>.</li>
<li><strong>Even longer tail</strong> than GGX for more complex microfacet distributions.</li>
</ul>
<h4 id="advanced-ndf-for-surface-effects">
<a class="header-anchor" href="#advanced-ndf-for-surface-effects"></a>
Advanced NDF for Surface Effects
</h4><ul>
<li>Materials with <strong>micro-scratches</strong> or <strong>glints</strong> (e.g., slightly worn stainless steel under sharp lighting) require specialized NDFs.</li>
<li>Simple bump mapping is insufficient to reproduce the intricate patterns caused by <strong>near-field point lights</strong>.</li>
<li><strong>Yan et al.</strong> proposed a more complex NDF for these scenarios.</li>
</ul>
<h4 id="practical-considerations">
<a class="header-anchor" href="#practical-considerations"></a>
Practical Considerations
</h4><ul>
<li>In real-time rendering, <strong>simpler NDFs</strong> like Beckmann or GGX are typically used for efficiency.</li>
<li>Special cases, such as snow-covered surfaces, may justify the use of more advanced models.</li>
</ul>
<h3 id="shadowing-masking-term">
<a class="header-anchor" href="#shadowing-masking-term"></a>
Shadowing-Masking Term   
</h3><p>The <strong>shadowing-masking term</strong>, also known as the <strong>geometry term</strong>, accounts for the attenuation of light due to self-shadowing and masking among microfacets on a surface.   </p>
<ul>
<li>
<p><strong>Near-Perpendicular Angles</strong>:   <br>
When the light or viewing direction is almost <strong>perpendicular to the surface</strong>, there is minimal self-shadowing or masking. The geometry term approaches <strong>1</strong>, meaning little to no light attenuation occurs.   </p>
</li>
<li>
<p><strong>Grazing Angles</strong>:
At <strong>grazing angles</strong>, where the light or view direction is nearly <strong>parallel to the surface</strong>, microfacets heavily shadow or mask each other. The geometry term approaches <strong>0</strong>, indicating significant light attenuation.</p>
</li>
</ul>
<p>The geometry term works in conjunction with the <strong>normal distribution function</strong> to simulate the energy lost due to occlusion between microfacets.   </p>
<h4 id="smith-shadowing-masking-approximation">
<a class="header-anchor" href="#smith-shadowing-masking-approximation"></a>
Smith Shadowing-Masking Approximation
</h4><p>Under the <strong>Smith assumption</strong>, the <strong>bidirectional shadowing-masking term</strong> can be factored into two independent terms for light and view directions:   </p>
$$G\left(\omega_i,\,\omega_o,\,h\right) \approx G_1\left(\omega_i,\,h\right) \,G_1\left(\omega_o,\,h\right)$$<ul>
<li><strong>$G_1(\omega, h)$</strong>: Represents the fraction of microfacets visible from the light or view direction independently.   </li>
<li>This factorization simplifies calculations and is widely used in real-time rendering.   </li>
</ul>
<h2 id="kulla-conty-approximation">
<a class="header-anchor" href="#kulla-conty-approximation"></a>
Kulla-Conty Approximation
</h2><p>While the shadowing and masking term $G(\omega_i, \omega_o, h)$ accounts for energy loss due to micro-surface occlusion, it does not consider energy that scatters multiple times between micro-surfaces before exiting.</p>
<ul>
<li>
<p><strong>Heitz et al.</strong> proposed an accurate compensation method in their paper &ldquo;Multiple-Scattering Microfacet BSDFs with the Smith Model&rdquo; (<a href="https://eheitzresearch.wordpress.com/240-2/">link</a>).</p>
<ul>
<li>However, this method is computationally expensive and impractical for real-time rendering.</li>
</ul>
</li>
<li>
<p><strong>Kulla and Conty</strong> introduced a simpler and more efficient method suited for real-time rendering (<a href="https://fpsunflower.github.io/ckulla/data/s2017_pbs_imageworks_slides_v2.pdf">slides</a>). The key ideas are:</p>
</li>
</ul>
<h3 id="integral-estimation">
<a class="header-anchor" href="#integral-estimation"></a>
Integral Estimation
</h3><p>For rough materials, the BRDF is smoother, allowing the following approximation for indirect lighting:</p>
$$\begin{align}  
L_{o,\,\text{indirect}}\left(p,\,\omega_o\right) &= \int_{\Omega^+} L_i\left(p,\,\omega_i\right) f_r\left(p,\,\omega_i \to \omega_o\right)\cos\left<\omega_i,\, n\right>\, \mathrm{d}\omega_i  \notag \\  
&= \int_{\Omega^+} L_i\left(p,\,\omega_i\right) \frac{F(\omega_i,\,h)\, G(\omega_i,\,\omega_o,\,h) \, D(h)}{4\,\left| \omega_i\cdot n\right| \,\left| \omega_o\cdot n\right|} \cos\left<\omega_i,\, n\right>\, \mathrm{d}\omega_i \notag \\  
&\approx \frac{\int_{\Omega^+} F(\omega_i,\,h)\,\mathrm{d}\omega_i}{\int_{\Omega^+}\mathrm{d}\omega_i} \cdot \int_{\Omega^+} L_i\left(p,\,\omega_i\right) \frac{G(\omega_i,\,\omega_o,\,h) \, D(h)}{4\,\left| \omega_i\cdot n\right| \,\left| \omega_o\cdot n\right|} \cos\left<\omega_i,\, n\right>\, \mathrm{d}\omega_i   \notag 
\end{align}$$<h3 id="albedo-approximation">
<a class="header-anchor" href="#albedo-approximation"></a>
Albedo Approximation
</h3><p>To compute the <strong>albedo</strong> $E(\mu_o)$ for a specific direction $\mu_o$:</p>
$$E(\mu_o) = \int_0^{2\pi}\int_0^1 f\left(\mu_o,\,\mu_i,\,\phi\right)\mu_i\,\mathrm{d}\mu_i\mathrm{d}\phi$$<h3 id="energy-loss-in-direction-mu_o">
<a class="header-anchor" href="#energy-loss-in-direction-mu_o"></a>
Energy Loss in Direction $\mu_o$
</h3><p>The energy $E(\mu_o)$ represents the total energy radiated out after one bounce under the assumption of uniform lighting and an isotropic BRDF. The integral expression is:</p>
$$E(\mu_o) = \int_0^{2\pi}\int_0^1 f(\mu_o, \mu_i, \phi)\mu_i \, \mathrm{d}\mu_i \mathrm{d}\phi$$<ul>
<li>
<p><strong>Assumptions</strong>:</p>
<ul>
<li><strong>Lighting</strong>: The incident radiance is uniform and normalized to 1. This simplifies the rendering equation by omitting the lighting term.</li>
<li><strong>Isotropic BRDF</strong>: The BRDF $f(\mu_o, \mu_i, \phi)$ is independent of azimuthal angle $\phi$, meaning it depends only on $\mu_o$ and $\mu_i$.</li>
<li><strong>No Fresnel Term</strong>: Assume $F(\omega_i, h) = 1$, meaning there is no angular-dependent reflectance.</li>
<li><strong>Uniform Lighting</strong>: The result reflects the total energy emitted after one bounce when all incoming radiance is uniformly distributed.</li>
</ul>
</li>
<li>
<p><strong>Variables</strong>:</p>
<ul>
<li>$\mu = \sin\theta$: the projection of the incoming direction onto the surface normal.</li>
<li>The proportion of energy lost in direction $\mu_o$ is <strong>$1 - E(\mu_o)$</strong>.</li>
</ul>
</li>
</ul>
<h3 id="average-albedo">
<a class="header-anchor" href="#average-albedo"></a>
Average Albedo
</h3><p>The <strong>average albedo</strong> across all directions $E_\text{avg}$:</p>
$$E_\text{avg} = \frac{\int_0^1 E(\mu)\mu\,\mathrm{d}\mu}{\int_0^1 \mu\,\mathrm{d}\mu} = 2\int_0^1 E(\mu)\mu\,\mathrm{d}\mu$$<ul>
<li>The total <strong>energy loss proportion</strong> is $1 - E_\text{avg}$, which includes:
<ul>
<li>Energy absorbed by the material.</li>
<li>Energy that exits after multiple scattering.</li>
</ul>
</li>
</ul>
<h3 id="fresnel-term-and-multiple-scattering-compensation">
<a class="header-anchor" href="#fresnel-term-and-multiple-scattering-compensation"></a>
Fresnel Term and Multiple Scattering Compensation
</h3><h4 id="average-fresnel-term">
<a class="header-anchor" href="#average-fresnel-term"></a>
Average Fresnel Term
</h4><p>The average Fresnel term $F_\text{avg}$ is calculated as:</p>
$$F_\text{avg} = \frac{\int_0^1 F(\mu) \mu \, \mathrm{d}\mu}{\int_0^1 \mu \, \mathrm{d}\mu} = 2 \int_0^1 F(\mu) \mu \, \mathrm{d}\mu$$<h4 id="energy-distribution-after-surface-incidence">
<a class="header-anchor" href="#energy-distribution-after-surface-incidence"></a>
Energy Distribution After Surface Incidence
</h4><p>When light interacts with a surface:</p>
<ul>
<li><strong>First Reflection</strong>: The proportion of energy directly visible is $F_\text{avg} E_\text{avg}$.</li>
<li><strong>One Bounce of Scattering</strong>: Visible energy proportion is $F_\text{avg}(1-E_\text{avg}) \cdot F_\text{avg} E_\text{avg}$.</li>
<li><strong>Two Bounces of Scattering</strong>: Visible energy proportion is $F_\text{avg}(1-E_\text{avg}) \cdot F_\text{avg}(1-E_\text{avg}) \cdot F_\text{avg} E_\text{avg}$.</li>
<li><strong>k Bounces of Scattering</strong>: Visible energy proportion is: $$F^k_\text{avg}(1-E_\text{avg})^k \cdot F_\text{avg} E_\text{avg}$$</li>
</ul>
<h4 id="total-multiple-scattering-contribution">
<a class="header-anchor" href="#total-multiple-scattering-contribution"></a>
Total Multiple Scattering Contribution
</h4><p>The total energy proportion from all bounces is:</p>
$$f_\text{add} = \frac{\sum_{k=1}^\infty F^k_\text{avg}(1-E_\text{avg})^k \cdot F_\text{avg} E_\text{avg}}{1-E_\text{avg}} = \frac{F^2_\text{avg}E_\text{avg}}{1-F_\text{avg}(1-E_\text{avg})}$$<h3 id="final-brdf-with-compensation">
<a class="header-anchor" href="#final-brdf-with-compensation"></a>
Final BRDF with Compensation
</h3><p>The final BRDF compensating for multiple scattering is:</p>
$$f_r = f_\text{micro} + f_\text{add} \cdot f_\text{ms}$$<ul>
<li><strong>$f_\text{micro}$</strong>: The microfacet model’s original BRDF.</li>
<li><strong>Pre-computed values</strong>:
<ul>
<li>Average albedo $E_\text{avg}$,</li>
<li>Compensation factor $f_\text{ms}$,</li>
<li>Average Fresnel term $F_\text{avg}$.</li>
</ul>
</li>
</ul>
<p>These values can be pre-calculated and directly used during rendering.</p>
<h3 id="compensation-factor-f_textms">
<a class="header-anchor" href="#compensation-factor-f_textms"></a>
Compensation Factor $f_\text{ms}$
</h3><p>Kulla and Conty proposed a compensation factor for lost multiple scattering energy:</p>
$$f_\text{ms}(\mu_o, \mu_i) = \frac{[1-E(\mu_o)][1-E(\mu_i)]}{\pi(1-E_\text{avg})}$$<h4 id="validation">
<a class="header-anchor" href="#validation"></a>
Validation
</h4><p>The compensation factor $f_\text{ms}$ ensures energy conservation:</p>
$$\begin{align}  
E_\text{ms}(\mu_o) &= \int_0^{2\pi} \int_0^1 f_\text{ms}(\mu_o, \mu_i, \phi) \mu_i \, \mathrm{d}\mu_i \mathrm{d}\phi \notag \\  
&= 2\pi \int_0^1 \frac{[1-E(\mu_o)][1-E(\mu_i)]}{\pi(1-E_\text{avg})} \mu_i \, \mathrm{d}\mu_i \notag\\  
&= \frac{1-E(\mu_o)}{1-E_\text{avg}} \cdot (1-E_\text{avg}) \notag\\  
&= 1-E(\mu_o) \text{ (equal to the lost energy proportion).}  \notag
\end{align}$$<h3 id="other-energy-compensation-methods">
<a class="header-anchor" href="#other-energy-compensation-methods"></a>
Other Energy Compensation Methods
</h3><p>A physically accurate BRDF consists of two terms:</p>
$$f(\omega_i, \omega_o) = f_\text{micro}(\omega_i, \omega_o) + f_\text{multi}(\omega_i, \omega_o)$$<ul>
<li>
<p>The approximation of $f_\text{multi}$ by Kulla and Conty is <strong>physically correct</strong>.</p>
</li>
<li>
<p>To simplify, some methods use diffuse lobes to approximate $f_\text{multi}$.</p>
<ul>
<li>However, directly adding diffuse lobes leads to <strong>non-conservation of energy</strong> and is <strong>physically incorrect</strong>.</li>
</ul>
</li>
<li>
<p>Some research focuses on constructing <strong>energy-conserving diffuse lobes</strong> to approximate $f_\text{multi}$.</p>
</li>
</ul>
<h1 id="non-photorealistic-rendering-npr">
<a class="header-anchor" href="#non-photorealistic-rendering-npr"></a>
Non-Photorealistic Rendering (NPR)
</h1><p>NPR focuses on creating stylized images that mimic artistic styles rather than photorealistic scenes. Its primary goal in real-time rendering is to produce <strong>fast and reliable stylized results</strong> using lightweight techniques in shaders.</p>
<ul>
<li><strong>Deep learning methods</strong> are generally unsuitable due to performance and reliability concerns.</li>
<li>NPR research has declined, with the NPAR conference inactive since 2017.</li>
</ul>
<p>The general approach starts with realistic rendering and abstracts it to emphasize key features, producing a stylized result. Common styles include:</p>
<ul>
<li><strong>Cartoon style</strong>: Uses outlines to emphasize object contours and employs discrete color blocks.</li>
<li><strong>Sketch style</strong>: Utilizes textures with varying line densities to create a sketch-like appearance.</li>
</ul>
<h2 id="outline-rendering">
<a class="header-anchor" href="#outline-rendering"></a>
Outline Rendering
</h2><p>To emphasize object contours, various techniques can be applied:</p>
<ul>
<li>
<p><strong>Shading-based approach</strong>:</p>
<ul>
<li>Darken surfaces with normals nearly perpendicular to the view direction.</li>
<li>This approach may produce uneven line thickness.</li>
</ul>
</li>
<li>
<p><strong>Geometry-based approach</strong>:</p>
<ul>
<li>Enlarge back-facing surfaces relative to front-facing ones before shading.</li>
<li>Assign black color to back-facing surfaces, creating a visible black outline around the object.</li>
</ul>
</li>
<li>
<p><strong>Post-processing approach</strong>:</p>
<ul>
<li>Use edge-detection algorithms like the <strong>Sobel operator</strong> to extract outlines in screen space.</li>
</ul>
</li>
</ul>
<h2 id="color-blocks">
<a class="header-anchor" href="#color-blocks"></a>
Color Blocks
</h2><p>Discrete color blocks can be achieved using <strong>thresholding</strong> to create sharp boundaries between regions of different colors.</p>
<h2 id="sketch-style">
<a class="header-anchor" href="#sketch-style"></a>
Sketch Style
</h2><p>In the paper <a href="https://hhoppe.com/hatching.pdf">Real-time Hatching</a> (2001) by Praun et al., a method for sketch-style rendering was introduced, which represents surface variations using line textures of different densities.</p>
<ul>
<li>
<p><strong>Tonal Art Maps (TAMs)</strong>:</p>
<ul>
<li>TAMs consist of textures with line patterns at various densities, each with its own <strong>MIPMAP levels</strong>.</li>
<li><strong>MIPMAP levels within a TAM share the same density</strong>, ensuring consistent texture quality.</li>
</ul>
</li>
<li>
<p><strong>Shading Process</strong>:</p>
<ul>
<li>Select appropriate textures based on the <strong>brightness and position</strong> of shading points.</li>
</ul>
</li>
</ul>
<h1 id="real-time-ray-tracing">
<a class="header-anchor" href="#real-time-ray-tracing"></a>
Real-Time Ray Tracing
</h1><ul>
<li><strong>Key Technique</strong>: Uses acceleration structures like BVH (Bounding Volume Hierarchy) or KD-Tree for efficiently finding ray-object intersections.</li>
<li><strong>Hardware Role</strong>: NVIDIA RTX GPUs introduced dedicated RT cores to accelerate ray-tracing tasks, such as BVH traversal and intersection calculations, allowing significantly more rays to be processed.</li>
</ul>
<h2 id="rtx-and-path-tracing-in-real-time-applications">
<a class="header-anchor" href="#rtx-and-path-tracing-in-real-time-applications"></a>
RTX and Path Tracing in Real-Time Applications
</h2><p><strong>RTX Overview:</strong></p>
<ul>
<li>RTX technology focuses on enabling real-time ray tracing by leveraging hardware acceleration, such as NVIDIA&rsquo;s RT cores.</li>
<li><strong>Performance Benchmarks:</strong> Achieving 1 sample per pixel (SPP) in real-time applications requires around <strong>10 Giga rays per second</strong> processing capability.</li>
</ul>
<h3 id="components-of-1-spp-path-tracing">
<a class="header-anchor" href="#components-of-1-spp-path-tracing"></a>
Components of 1 SPP Path Tracing
</h3><p>A single sample per pixel (SPP) in path tracing typically involves the following steps:</p>
<ol>
<li>
<p><strong>Rasterization (Primary):</strong></p>
<ul>
<li>Used to determine primary visibility and initialize the scene&rsquo;s geometry.</li>
<li>This step is often paired with ray tracing to speed up the initial pixel visibility.</li>
</ul>
</li>
<li>
<p><strong>Ray Tracing - Primary Visibility Ray:</strong></p>
<ul>
<li>A ray is traced from the camera to determine the first visible point of the geometry in the scene (hit point).</li>
</ul>
</li>
<li>
<p><strong>Ray Tracing - Secondary Bounce Ray:</strong></p>
<ul>
<li>A secondary ray is generated to simulate light interaction at the primary hit point, such as reflection, refraction, or indirect lighting.</li>
</ul>
</li>
<li>
<p><strong>Ray Tracing - Secondary Visibility Ray:</strong></p>
<ul>
<li>This ray is cast to determine if the secondary hit point is directly visible to a light source, contributing to shadows and final illumination.</li>
</ul>
</li>
</ol>
<p>This combination of rasterization and multiple rays simulates realistic lighting and materials, even with only one sample per pixel.</p>
<h3 id="challenges-of-rtrt">
<a class="header-anchor" href="#challenges-of-rtrt"></a>
Challenges of RTRT
</h3><ul>
<li><strong>Noise</strong>: Path tracing introduces noise due to Monte Carlo sampling. At 1 spp, noise is particularly high, necessitating advanced <strong>denoising techniques</strong>.</li>
<li><strong>Performance</strong>: Balancing quality and speed, as ray tracing competes for computational resources with gameplay logic, post-processing, and other tasks.</li>
</ul>
<h3 id="motion-vectors">
<a class="header-anchor" href="#motion-vectors"></a>
Motion Vectors
</h3><ul>
<li>Describe how objects move between frames, allowing tracking of pixels&rsquo; corresponding positions across time.</li>
<li>Essential for <strong>temporal reprojection</strong>, enabling the reuse of filtered results from previous frames.</li>
</ul>
<h3 id="temporal-accumulation-and-filtering">
<a class="header-anchor" href="#temporal-accumulation-and-filtering"></a>
Temporal Accumulation and Filtering
</h3><ul>
<li><strong>Temporal Assumption</strong>: Scene movement is continuous and predictable, making shading consistent across frames.</li>
<li><strong>Method</strong>:
<ol>
<li>Use motion vectors to map a pixel in the current frame to its corresponding pixel in the previous frame.</li>
<li>Blend the previous frame&rsquo;s denoised result with the current noisy frame to accumulate detail and improve quality over time.</li>
<li>This recursive accumulation simulates having multiple spp per pixel, exponentially increasing effective spp across frames.</li>
</ol>
</li>
</ul>
<h3 id="denoising-solutions">
<a class="header-anchor" href="#denoising-solutions"></a>
Denoising Solutions
</h3><ol>
<li>
<p><strong>Temporal Filtering</strong>:</p>
<ul>
<li><strong>Linear Blending</strong>: Combine the current noisy frame (e.g., 20%) with the previous denoised frame (e.g., 80%) for stability.</li>
<li><strong>Key Balance</strong>: A blending factor ($\alpha$) controls the trade-off between reusing past data and incorporating new information.</li>
</ul>
</li>
<li>
<p><strong>Back Projection</strong>:</p>
<ul>
<li>Reprojects the world coordinates of a pixel in the current frame to its corresponding pixel in the previous frame using motion matrices.</li>
<li>Ensures alignment of pixel data across frames for effective temporal filtering.</li>
</ul>
</li>
</ol>
<h3 id="challenges-and-failure-cases">
<a class="header-anchor" href="#challenges-and-failure-cases"></a>
Challenges and Failure Cases
</h3><h4 id="switching-scenes">
<a class="header-anchor" href="#switching-scenes"></a>
Switching Scenes
</h4><ul>
<li><strong>Burn-In Period</strong>: New scenes lack sufficient temporal data, leading to visible noise until multiple frames are accumulated.</li>
</ul>
<h4 id="screen-space-issues">
<a class="header-anchor" href="#screen-space-issues"></a>
Screen-Space Issues
</h4><ul>
<li><strong>Walking Backwards</strong>: Newly revealed areas outside the previous frame&rsquo;s coverage cannot reuse past data, causing inconsistencies.</li>
<li><strong>Suddenly Appearing Backgrounds (Disocclusion)</strong>: Areas previously occluded (e.g., behind an object) now visible, creating temporal artifacts.</li>
</ul>
<h4 id="shading-changes">
<a class="header-anchor" href="#shading-changes"></a>
Shading Changes
</h4><ul>
<li>
<p><strong>Example</strong>: Moving light sources cause shadow artifacts as static motion vectors fail to track dynamic shading changes, leading to lagged reflections or shadow dragging.</p>
<ul>
<li>
<p>Temporal failures in shading occur when lighting changes while geometry remains static, e.g., a fence scene with a moving light source.</p>
</li>
<li>
<p>Glossy reflections and similar effects experience lag due to static motion vectors for geometrically static surfaces.</p>
</li>
</ul>
</li>
</ul>
<h3 id="solutions-for-temporal-artifacts">
<a class="header-anchor" href="#solutions-for-temporal-artifacts"></a>
Solutions for Temporal Artifacts
</h3><p><strong>1. Clamping</strong></p>
<ul>
<li>Adjust previous frame results to be closer to current frame results before blending.</li>
</ul>
<p><strong>2. Detection</strong></p>
<ul>
<li>
<p>Identify when previous frame data is unreliable:</p>
<ul>
<li>Use object IDs to validate corresponding motion vectors.</li>
<li>Tune the blending factor $\alpha$ (binary or continuous) based on reliability.</li>
<li>Increase spatial filtering for noisy current frame results.</li>
</ul>
</li>
<li>
<p><strong>Challenges</strong></p>
<ul>
<li>Clamping reduces ghosting but risks introducing noise.</li>
<li>Detection methods may eliminate reuse of prior data, reintroducing noise. This noise can be mitigated with increased spatial filtering (e.g., blurring).</li>
</ul>
</li>
</ul>
<h3 id="implementation-of-filtering">
<a class="header-anchor" href="#implementation-of-filtering"></a>
Implementation of Filtering
</h3><p>Assume a Gaussian filter centered at a pixel (2D):</p>
<ul>
<li>Each pixel in the neighborhood contributes to the result.</li>
<li>Contribution is determined by the <strong>distance between the current pixel</strong> $i$ and the neighboring pixel $j$.</li>
</ul>
<p>Filtering Algorithm</p>
<ol>
<li>
<p><strong>Initialization:</strong></p>
<ul>
<li>Set <code>sum_of_weights = 0.0</code>.</li>
<li>Set <code>sum_of_weighted_values = 0.0</code>.</li>
</ul>
</li>
<li>
<p><strong>For each pixel $i$:</strong></p>
<ul>
<li>Iterate over all pixels $j$ in the neighborhood of $i$.</li>
</ul>
</li>
<li>
<p><strong>Calculate the weight $w_{ij}$:</strong></p>
<ul>
<li>Use the Gaussian formula $G(|i - j|, \sigma)$ to determine the weight.</li>
<li>The weight depends on the distance between $i$ and $j$, controlled by $\sigma$.</li>
</ul>
</li>
<li>
<p><strong>Update sums:</strong></p>
<ul>
<li>Add $w_{ij} \times C^{\text{input}}[j]$ to <code>sum_of_weighted_values</code>.</li>
<li>Add $w_{ij}$ to <code>sum_of_weights</code>.</li>
</ul>
</li>
<li>
<p><strong>Output the filtered value:</strong></p>
<ul>
<li>Compute $C^{\text{output}}[i]=$ <code> sum_of_weighted_values / sum_of_weights</code>.</li>
</ul>
</li>
</ol>
<h4 id="important-notes">
<a class="header-anchor" href="#important-notes"></a>
Important Notes:
</h4><ul>
<li>
<p><strong>Normalization:</strong></p>
<ul>
<li>Keep track of <code>sum_of_weights</code> to ensure proper normalization of pixel values.</li>
</ul>
</li>
<li>
<p><strong>Handle edge cases:</strong></p>
<ul>
<li>Check if <code>sum_of_weights</code> is zero, especially when using non-Gaussian kernels.</li>
</ul>
</li>
<li>
<p><strong>Color channels:</strong></p>
<ul>
<li>The filter supports multi-channel color data (e.g., RGB), and each channel can be processed independently.</li>
</ul>
</li>
</ul>
<h3 id="bilateral-filtering">
<a class="header-anchor" href="#bilateral-filtering"></a>
Bilateral Filtering
</h3><ul>
<li>A Gaussian filter creates a blurred image, which smooths both the low-frequency and high-frequency regions, including boundaries.</li>
<li>To <strong>preserve sharp edges</strong> while smoothing, we use <strong>Bilateral Filtering</strong>, a technique that incorporates both spatial and intensity differences to reduce contributions from pixels across boundaries.</li>
</ul>
<h4 id="key-idea">
<a class="header-anchor" href="#key-idea"></a>
Key Idea:
</h4><ul>
<li><strong>Observation:</strong> Regions with abrupt color changes often correspond to edges or boundaries.</li>
<li><strong>How to preserve boundaries?</strong>
<ul>
<li>For neighboring pixels $j$ around $i$:
<ul>
<li>If their intensity difference is small, process them with a Gaussian weight.</li>
<li>If the intensity difference is large, reduce $j$&rsquo;s contribution to $i$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="filtering-kernel">
<a class="header-anchor" href="#filtering-kernel"></a>
Filtering Kernel:
</h4><p>The <strong>bilateral filtering kernel</strong> combines spatial and intensity information to selectively smooth an image while preserving edges.</p>
<h3 id="kernel-definition">
<a class="header-anchor" href="#kernel-definition"></a>
Kernel Definition:
</h3>$$ w(i, j, k, l) = \exp\left(-\frac{(i - k)^2 + (j - l)^2}{2\sigma_d^2}- \frac{\|I(i, j) - I(k, l)\|^2}{2\sigma_r^2} \right) $$<h4 id="components">
<a class="header-anchor" href="#components"></a>
Components:
</h4><ol>
<li>
<p><strong>First Term</strong> $\left(-\frac{(i - k)^2 + (j - l)^2}{2\sigma_d^2}\right)$:</p>
<ul>
<li>Represents a Gaussian function applied to the <strong>spatial distance</strong> between pixel $(i, j)$ and pixel $(k, l)$.</li>
<li>Ensures that contributions decrease with increasing spatial distance, favoring nearby pixels.</li>
</ul>
</li>
<li>
<p><strong>Second Term</strong> $\left(-\frac{\|I(i, j) - I(k, l)\|^2}{2\sigma_r^2}\right)$:</p>
<ul>
<li>Represents a Gaussian function applied to the <strong>intensity difference</strong> between the two pixels.</li>
<li>Ensures that contributions decrease with increasing intensity differences, favoring similar colors or intensities.</li>
</ul>
</li>
</ol>
<h4 id="key-behavior">
<a class="header-anchor" href="#key-behavior"></a>
Key Behavior:
</h4><ul>
<li>
<p><strong>Combined Effect:</strong></p>
<ul>
<li>The two terms together penalize contributions both from spatially distant pixels and from pixels with significantly different intensity values.</li>
<li>This dual consideration helps preserve edges by reducing the influence of pixels across sharp intensity transitions.</li>
</ul>
</li>
<li>
<p><strong>Edge Preservation:</strong></p>
<ul>
<li>Pixels with similar intensities across short spatial distances are weighted higher, helping smooth within regions while retaining boundary details.</li>
</ul>
</li>
</ul>
<h4 id="effects">
<a class="header-anchor" href="#effects"></a>
Effects:
</h4><ul>
<li>Smoothens low-frequency regions while <strong>preserving edge details</strong>.</li>
<li>Boundary pixels contribute less to neighboring pixels across the edge, avoiding boundary blurring.</li>
</ul>
<h3 id="joint-bilateral-filtering">
<a class="header-anchor" href="#joint-bilateral-filtering"></a>
Joint Bilateral Filtering
</h3><h4 id="overview">
<a class="header-anchor" href="#overview"></a>
Overview:
</h4><ul>
<li><strong>Gaussian Filtering:</strong> Considers only spatial distance.</li>
<li><strong>Bilateral Filtering:</strong> Considers <strong>two metrics</strong>—spatial distance and color similarity.</li>
<li><strong>Joint Bilateral Filtering:</strong> Extends bilateral filtering by incorporating <strong>additional features</strong> like depth or normals to improve results.</li>
</ul>
<h4 id="motivation">
<a class="header-anchor" href="#motivation"></a>
Motivation:
</h4><ul>
<li>Bilateral filtering may fail when:
<ul>
<li>Noise and edges are indistinguishable (e.g., Monte Carlo noise in path tracing).</li>
<li>Features beyond position and intensity, such as depth or normals, are needed.</li>
</ul>
</li>
</ul>
<h4 id="g-buffer-features">
<a class="header-anchor" href="#g-buffer-features"></a>
G-buffer Features:
</h4><ul>
<li><strong>G-buffer</strong> contains auxiliary data from rendering, such as:
<ul>
<li>Depth</li>
<li>Normals</li>
<li>World coordinates</li>
</ul>
</li>
<li><strong>Key Property:</strong> G-buffer data is noise-free, as it is derived from rasterization.</li>
</ul>
<h4 id="examples-of-feature-guidance">
<a class="header-anchor" href="#examples-of-feature-guidance"></a>
Examples of Feature Guidance:
</h4><ol>
<li>
<p><strong>Depth:</strong></p>
<ul>
<li>Pixels $A$ and $B$: If their depth values differ significantly, reduce the contribution.</li>
<li>Use a depth-based Gaussian to penalize contributions from pixels at different depths.</li>
</ul>
</li>
<li>
<p><strong>Normals:</strong></p>
<ul>
<li>Pixels $B$ and $C$: If their normal vectors differ (e.g., large angular difference), reduce the contribution.</li>
<li>Use a normal-based kernel, often derived from cosine similarity.</li>
</ul>
</li>
<li>
<p><strong>Color:</strong></p>
<ul>
<li>Pixels $D$ and $E$: If their color difference is large, reduce the contribution.</li>
</ul>
</li>
</ol>
<h4 id="implementation">
<a class="header-anchor" href="#implementation"></a>
Implementation:
</h4><ul>
<li>For joint bilateral filtering, <strong>multiple kernels</strong> are computed for different features (e.g., spatial distance, depth, normals, etc.).</li>
<li>The final kernel weight is the <strong>product of these feature-based contributions</strong>.</li>
</ul>
<h4 id="notes">
<a class="header-anchor" href="#notes"></a>
Notes:
</h4><ul>
<li><strong>Normalization:</strong> The kernel does not need to be pre-normalized; the filtering process includes normalization in the final calculation.</li>
<li><strong>Flexible Kernels:</strong> Any function that decays with distance can be used, not just Gaussians. Examples include exponential or triangular functions.</li>
<li><strong>Practical Applications:</strong> Joint bilateral filtering is effective in denoising Monte Carlo renders by leveraging G-buffer features.</li>
</ul>
<h3 id="implementing-large-filters">
<a class="header-anchor" href="#implementing-large-filters"></a>
Implementing Large Filters
</h3><p>For large filters (e.g., $128 \times 128$), directly applying the filter to compute the weighted contributions of $N \times N$ pixels for each pixel is computationally expensive. Here are two industrially popular methods to speed up the process:</p>
<h4 id="solution-1-separate-passes">
<a class="header-anchor" href="#solution-1-separate-passes"></a>
Solution 1: Separate Passes
</h4><ul>
<li>
<p><strong>Concept:</strong><br>
Instead of applying a $N \times N$ filter in one pass, break it into two passes:</p>
<ul>
<li>A <strong>horizontal filter</strong> ($1 \times N$)</li>
<li>Followed by a <strong>vertical filter</strong> ($N \times 1$)</li>
</ul>
</li>
<li>
<p><strong>Why it works:</strong></p>
<ul>
<li>A 2D Gaussian filter is <strong>separable</strong>, meaning: $$G_{2D}(x, y) = G_{1D}(x) \cdot G_{1D}(y)$$</li>
<li>Filtering with $N \times N$ involves $N^2$ texture accesses per pixel, but splitting it into two passes reduces this to <strong>$2N$ accesses</strong>.</li>
</ul>
</li>
<li>
<p><strong>Mathematical Intuition:</strong></p>
<ul>
<li>Filtering is convolution.</li>
<li>The convolution integral of $F(x, y)$ with $G_{2D}$ splits into two steps: $$\int \int F(x_0, y_0) G_{2D}(x_0 - x, y_0 - y) dx_0 dy_0  
        = \int \left(\int F(x_0, y_0) G_{1D}(x_0 - x) dx_0\right) G_{1D}(y_0 - y) dy_0$$</li>
</ul>
</li>
<li>
<p><strong>Practical Notes:</strong></p>
<ul>
<li>This approach is <strong>theoretically limited to Gaussian filters</strong>.</li>
<li>For <strong>bilateral filters</strong> or <strong>joint bilateral filters</strong>, it is challenging to split the kernel into $x$- and $y$-dependent terms. However, <strong>approximate separability</strong> is sometimes acceptable for smaller ranges (e.g., $32 \times 32$).</li>
</ul>
</li>
</ul>
<h4 id="solution-2-progressively-growing-sizes">
<a class="header-anchor" href="#solution-2-progressively-growing-sizes"></a>
Solution 2: Progressively Growing Sizes
</h4><ul>
<li>
<p><strong>Concept:</strong><br>
Apply multiple smaller filters (e.g., $5 \times 5$) in progressive passes, where the <strong>sample interval grows</strong> after each pass.</p>
</li>
<li>
<p><strong>Steps:</strong></p>
<ol>
<li>Start with a small filter ($5 \times 5$).</li>
<li>In each pass, increase the sample interval by $2^i$ (e.g., 1, 2, 4, 8, &hellip;).</li>
<li>After several passes, simulate the effect of a large $N \times N$ filter (e.g., $64 \times 64$).</li>
</ol>
</li>
<li>
<p><strong>Example:</strong><br>
To approximate a $64 \times 64$ filter:</p>
<ul>
<li>Use $5 \times 5$ filters in 5 passes, with sample intervals of $2^i$ where $i = 0, 1, 2, 3, 4$.</li>
<li>Instead of $4096$ texture accesses ($64 \times 64$), perform <strong>125 accesses</strong> ($5 \times 5 \times 5$).</li>
</ul>
</li>
<li>
<p><strong>Advantages:</strong></p>
<ul>
<li>Efficiently removes <strong>low-frequency information</strong> progressively.</li>
<li>Avoids aliasing by carefully selecting sample intervals.</li>
</ul>
</li>
<li>
<p><strong>Why it works:</strong></p>
<ul>
<li>Sampling theory:
<ul>
<li>Sampling creates <strong>repeated frequency spectra</strong>.</li>
<li>As sample intervals grow, aliasing is avoided by removing high frequencies in earlier passes.</li>
</ul>
</li>
<li>Progressive filters mitigate spectral overlap by ensuring earlier passes remove interfering high-frequency components.</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Challenges with Progressive Filters:</strong><br>
While progressive filters are efficient, <strong>artifacts</strong> can emerge due to:
<ul>
<li>
<p><strong>Incomplete high-frequency removal:</strong></p>
<ul>
<li>Each pass may leave residual high-frequency components, especially with <strong>non-Gaussian</strong> or <strong>joint bilateral filters</strong>.</li>
</ul>
</li>
<li>
<p><strong>Artifact accumulation:</strong></p>
<ul>
<li>Insufficient filtering in early passes can lead to visible distortions, particularly when high frequencies are intentionally preserved for detail retention.</li>
</ul>
</li>
<li>
<p><strong>Mitigation:</strong></p>
<ul>
<li>Apply an <strong>additional small-pass filter</strong> after the final pass to clean up lingering high-frequency artifacts.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="outlier-removal">
<a class="header-anchor" href="#outlier-removal"></a>
Outlier Removal
</h3><p>When using Monte Carlo methods for rendering, the results often include overly bright or dark points (outliers). Applying filters directly on such outliers can cause artifacts: for instance, a single bright point may brighten a larger region when processed by a 7×7 filter. To address this, outliers should be handled <strong>before</strong> filtering.</p>
<ul>
<li>
<p><strong>Outlier Detection</strong></p>
<ul>
<li>Define the dominant range of colors for each pixel using the surrounding region (e.g., a 7×7 or 5×5 window).</li>
<li>Compute the mean ($\mu$) and variance ($\sigma$) of the colors in this region.</li>
<li>Identify outliers as values outside the range $[\mu - k\sigma, \mu + k\sigma]$, where $k$ determines the tolerance.</li>
</ul>
</li>
<li>
<p><strong>Outlier Clamping</strong></p>
<ul>
<li>Clamp outlier values to the nearest boundary within the valid range.
<ul>
<li>For example, if the valid range is (-0.1, 0.1):
<ul>
<li>A value of <strong>10</strong> is clamped to <strong>0.1</strong>.</li>
<li>A value of <strong>-0.3</strong> is clamped to <strong>-0.1</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Note:</strong> Outlier removal does not discard values; it adjusts them, ensuring better filter results downstream.</li>
</ul>
</li>
</ul>
<h3 id="temporal-clamping">
<a class="header-anchor" href="#temporal-clamping"></a>
Temporal Clamping
</h3><p>Temporal clamping mitigates <strong>ghosting</strong> caused by motion vectors or excessive discrepancies between consecutive frames. It clamps previous frame values to match the valid range of the current frame before blending.</p>
<ul>
<li>
<p><strong>Process:</strong></p>
<ol>
<li>
<p><strong>Identify Valid Range</strong></p>
<ul>
<li>After spatial filtering of the current frame, find the mean ($\mu$) and variance ($\sigma$) within a small region around the corresponding pixel.</li>
<li>Define the valid range as $[\mu - k\sigma, \mu + k\sigma]$.</li>
</ul>
</li>
<li>
<p><strong>Clamp Previous Frame Values</strong></p>
<ul>
<li>If a value from the previous frame lies outside this range, clamp it to the nearest boundary.</li>
</ul>
</li>
<li>
<p><strong>Blend Frames</strong></p>
<ul>
<li>Perform <strong>linear blending</strong> between the clamped value and the current frame to produce a noisy-free result.</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>Notes:</strong></p>
<ul>
<li>It is a tradeoff mechanism rather than a complete solution.</li>
<li>This approach clamps unreliable previous frame data to the filtered range of the current frame for better temporal consistency.</li>
</ul>
</li>
<li>
<p>In real-time ray tracing, balancing <strong>noise</strong>, <strong>ghosting (lagging)</strong>, and <strong>over-blurring</strong> is crucial.</p>
</li>
</ul>
<h3 id="specific-filtering-approaches-for-rtrt">
<a class="header-anchor" href="#specific-filtering-approaches-for-rtrt"></a>
Specific Filtering Approaches for RTRT
</h3><h4 id="svgf">
<a class="header-anchor" href="#svgf"></a>
SVGF
</h4><ul>
<li><strong>SVGF (Spatiotemporal Variance-Guided Filtering)</strong> closely resembles general spatiotemporal denoising methods.</li>
<li>It incorporates <strong>variance analysis</strong> and specific <strong>tricks</strong> to enhance results, achieving outputs similar to ground truth.</li>
</ul>
<p><strong>Three Key Factors for Filtering:</strong></p>
<ol>
<li>
<p><strong>Depth:</strong></p>
<ul>
<li>Contribution weight depends on the <strong>depth difference</strong> between two points.<br>
$$w_z = \exp\left(-\frac{|z(p) - z(q)|}{\sigma_z |\nabla z(p) \cdot (p-q)| + \epsilon}\right)$$</li>
<li><strong>Key Insights:</strong>
<ul>
<li>Larger depth differences result in smaller contributions due to the exponential decay.</li>
<li><strong>$\sigma_z$</strong> controls the rate of decay, balancing how depth influences contributions.</li>
<li>The term <strong>$|\nabla z(p) \cdot (p-q)|$</strong> accounts for depth changes in the plane’s normal direction, ensuring reasonable contributions for co-planar points even if their depths differ significantly.</li>
<li><strong>$\epsilon$</strong> prevents division by zero and avoids numerical instability when two points are very close.</li>
</ul>
</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>If points <strong>A</strong> and <strong>B</strong> lie on the same slanted surface:
<ul>
<li>Their raw depth difference may be large, but their projected depth in the plane&rsquo;s normal direction is minimal, leading to a higher contribution weight.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Normal:</strong></p>
<ul>
<li><strong>Normal similarity</strong> is computed using dot products:<br>
$$w_n = \max(0, n(p) \cdot n(q))^{\sigma_n}$$</li>
<li><strong>Key Insights:</strong>
<ul>
<li>Higher <strong>$\sigma_n$</strong> results in stricter normal similarity criteria.</li>
<li>Normal maps for effects like bump mapping are not used; only original surface normals are considered.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Luminance:</strong></p>
<ul>
<li>Contribution based on grayscale color (luminance):<br>
$$w_l = \exp\left(-\frac{|l_i(p) - l_i(q)|}{\sigma_l \sqrt{g_{3\times3}(Var(l_i(p)))} + \epsilon}\right)$$</li>
<li><strong>Variance (V) Role:</strong>
<ul>
<li>Variance helps adjust weights when noisy pixels have misleading brightness.</li>
<li>A multi-step process ensures accurate variance:
<ul>
<li><strong>Spatial filtering</strong> to compute variance in a 7×7 region.</li>
<li><strong>Temporal accumulation</strong> using motion vectors to average variance over frames.</li>
<li><strong>Final spatial filtering</strong> with a smaller 3×3 region.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Results of SVGF</strong></p>
<ul>
<li>
<p><strong>Strengths:</strong></p>
<ul>
<li>SVGF effectively reduces noise and closely matches ground truth in most scenarios.</li>
<li>It leverages <strong>variance analysis</strong> to handle statistical noise and enhance filtering accuracy.</li>
</ul>
</li>
<li>
<p><strong>Limitations:</strong></p>
<ul>
<li><strong>Ghosting artifacts:</strong>
<ul>
<li>Occur when the light source moves without any geometric changes, resulting in zero motion vectors.</li>
<li>Temporal reuse of shadow data from the previous frame causes visible &ldquo;ghosting.&rdquo;</li>
</ul>
</li>
<li><strong>Boiling artifacts:</strong>
<ul>
<li>Persistent low-frequency noise leads to frame-to-frame flickering.</li>
<li>This effect resembles &ldquo;boiling water,&rdquo; despite using a large filter size.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="rae">
<a class="header-anchor" href="#rae"></a>
RAE
</h4><ul>
<li>RAE (Recurrent AutoEncoder) is a <strong>post-processing denoising method</strong> for real-time ray-traced images.</li>
<li>It reconstructs noisy Monte Carlo results into clean images.</li>
</ul>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Inputs:</strong>
<ul>
<li>Noisy image and G-buffer information.</li>
</ul>
</li>
<li><strong>AutoEncoder Architecture:</strong>
<ul>
<li>U-shaped neural network with skip connections to enhance training efficiency.</li>
<li>Performs image transformations effectively.</li>
</ul>
</li>
</ul>
<p><strong>Recurrent Connections:</strong></p>
<ul>
<li>Layers contain recurrent connections, enabling the model to:
<ul>
<li>Accumulate and adapt previous frame information without motion vectors.</li>
<li>Gradually &ldquo;forget&rdquo; old information as new frames are processed.</li>
</ul>
</li>
</ul>
<h1 id="real-time-rendering-techniques">
<a class="header-anchor" href="#real-time-rendering-techniques"></a>
Real-Time Rendering Techniques
</h1><h2 id="temporal-anti-aliasing-taa">
<a class="header-anchor" href="#temporal-anti-aliasing-taa"></a>
Temporal Anti-Aliasing (TAA)
</h2><ul>
<li>
<p><strong>Aliasing Origins:</strong><br>
Aliasing occurs due to insufficient samples per pixel during rasterization. The ultimate solution to aliasing is to use more samples, as seen in MSAA (Multisample Anti-Aliasing).</p>
</li>
<li>
<p><strong>TAA Approach:</strong><br>
TAA aims to approximate the effects of super-sampling without the performance cost by reusing samples from previous frames. Each frame effectively uses a single sample per pixel (1SPP), but temporal reuse increases the effective samples per pixel (SPP) over time.</p>
<ul>
<li><strong>Concept:</strong> Distribute sample points across N past frames. In each frame, retrieve samples from these frames and filter them to simulate N× super-sampling.</li>
<li><strong>Jittered Sampling:</strong> For static scenes, jittered sampling offsets sample points in a fixed pattern across consecutive frames. This allows averaging the contributions from different sample locations over time, achieving a similar result to upsampling (e.g., 2×2).</li>
</ul>
</li>
<li>
<p><strong>Moving Scenes:</strong></p>
<ul>
<li>Temporal samples are reprojected using motion vectors to map current geometry to its position in previous frames.</li>
<li><strong>Clamping:</strong> If temporal information is unreliable, clamping techniques adjust previous frame data to align with the current frame&rsquo;s results, avoiding visual inconsistencies.</li>
</ul>
</li>
</ul>
<h2 id="msaa-vs-ssaa">
<a class="header-anchor" href="#msaa-vs-ssaa"></a>
MSAA vs. SSAA
</h2><ul>
<li>
<p><strong>SSAA (Super-Sampling Anti-Aliasing):</strong><br>
SSAA renders a scene at a higher resolution and downsamples it to the target resolution, averaging pixel results. This method is accurate but computationally expensive.</p>
</li>
<li>
<p><strong>MSAA (Multisample Anti-Aliasing):</strong><br>
MSAA optimizes SSAA by reusing shading computations:</p>
<ul>
<li>
<p>Each pixel has multiple samples, but shading is performed per primitive rather than per sample. For example, a 4× MSAA only shades unique primitives once, significantly reducing computation compared to SSAA.</p>
</li>
<li>
<p><strong>Spatial Sample Reuse:</strong> Shared sample points at pixel boundaries can contribute to multiple pixels, reducing the required number of samples and improving efficiency.</p>
</li>
</ul>
</li>
</ul>
<h2 id="image-based-anti-aliasing">
<a class="header-anchor" href="#image-based-anti-aliasing"></a>
Image-Based Anti-Aliasing
</h2><ul>
<li><strong>Image-Based Solutions:</strong> These methods apply anti-aliasing as a post-process on the rendered image, identifying and correcting jagged edges directly from the image data.
<ul>
<li><strong>Popular Techniques:</strong>
<ul>
<li><strong>FXAA (Fast Approximate AA):</strong> Focuses on performance with moderate quality.</li>
<li><strong>MLAA (Morphological AA):</strong> Matches and smoothens jagged patterns in the image.</li>
<li><strong>SMAA (Enhanced Subpixel Morphological AA):</strong> Combines MLAA with additional steps for better edge detection and subpixel quality.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="no-anti-aliasing-on-g-buffers">
<a class="header-anchor" href="#no-anti-aliasing-on-g-buffers"></a>
No Anti-Aliasing on G-Buffers
</h2><ul>
<li><strong>G-buffers should never be anti-aliased.</strong> Applying anti-aliasing to G-buffers can corrupt their precise data representation, causing issues in subsequent rendering stages. This is a strict rule in deferred rendering pipelines.</li>
</ul>
<h2 id="dlss-deep-learning-super-sampling">
<a class="header-anchor" href="#dlss-deep-learning-super-sampling"></a>
DLSS (Deep Learning Super Sampling)
</h2><ul>
<li><strong>Super Resolution</strong> or <strong>Super Sampling</strong> aims to convert low-resolution images into high-resolution ones, essentially increasing the image&rsquo;s resolution.</li>
</ul>
<h3 id="dlss-10">
<a class="header-anchor" href="#dlss-10"></a>
DLSS 1.0
</h3><ul>
<li><strong>Approach:</strong> Relied purely on data-driven methods and made guesses to fill in missing information.
<ul>
<li>Operated on <strong>current frames only</strong>, without leveraging temporal information from previous frames.</li>
<li>Used game-specific or scene-specific neural network training to identify common patterns like edges.</li>
<li>Goal: Replace blurred edges with sharper ones in upscaled low-resolution images.</li>
</ul>
</li>
<li><strong>Limitation:</strong> Results were not robust and lacked temporal stability, leading to suboptimal performance.</li>
</ul>
<h3 id="dlss-20">
<a class="header-anchor" href="#dlss-20"></a>
DLSS 2.0
</h3><ul>
<li><strong>Core Idea:</strong> Shifted focus to <strong>temporal information</strong> rather than deep learning alone.
<ul>
<li>Leveraged <strong>temporal reuse</strong> to integrate information from previous frames for more accurate upscaling.</li>
<li>Core mechanism rooted in <strong>TAA (Temporal Anti-Aliasing)</strong> principles, ensuring improved resolution with temporal stability.</li>
</ul>
</li>
</ul>
<h4 id="challenges-in-dlss-20">
<a class="header-anchor" href="#challenges-in-dlss-20"></a>
Challenges in DLSS 2.0:
</h4><ol>
<li>
<p><strong>Handling Temporal Failures:</strong></p>
<ul>
<li>Simple clamping of unreliable temporal data is insufficient, especially for higher resolutions.</li>
<li>Blind clamping can result in over-smoothing, producing blurry high-resolution images.</li>
</ul>
</li>
<li>
<p><strong>Generating Accurate Pixel Values for New Pixels:</strong></p>
<ul>
<li>Increasing resolution introduces smaller, new pixels requiring precise values.</li>
<li>Poorly estimated values (e.g., resembling neighboring pixel colors) lead to blurry results.</li>
</ul>
</li>
<li>
<p><strong>Better Temporal Data Integration:</strong></p>
<ul>
<li>Requires advanced techniques to merge temporal information without clamping.</li>
<li><strong>Neural network role:</strong> Does not output final blended colors. Instead, it guides how to integrate information from the current and previous frames effectively.</li>
</ul>
</li>
</ol>
<h3 id="practical-considerations-for-dlss-20">
<a class="header-anchor" href="#practical-considerations-for-dlss-20"></a>
Practical Considerations for DLSS 2.0:
</h3><ul>
<li><strong>Performance:</strong> Inference time must be optimized for real-time usage.
<ul>
<li>Hardware-specific optimizations improve performance significantly.</li>
</ul>
</li>
<li><strong>Industry Variants:</strong>
<ul>
<li>AMD&rsquo;s <strong>FidelityFX Super Resolution</strong> serves as a competitive alternative, applying similar concepts.</li>
</ul>
</li>
</ul>
<h2 id="deferred-shading">
<a class="header-anchor" href="#deferred-shading"></a>
Deferred Shading
</h2><p><strong>Deferred Shading</strong> improves shading efficiency by only shading visible fragments.</p>
<h3 id="traditional-rasterization-process">
<a class="header-anchor" href="#traditional-rasterization-process"></a>
Traditional Rasterization Process
</h3><ul>
<li>Workflow:<br>
Triangles → Fragments → Depth Test → Shade → Pixel.</li>
<li><strong>Problem:</strong><br>
Every fragment is shaded, even those later discarded due to occlusion (e.g., fragments from distant objects are shaded unnecessarily).
<ul>
<li>Complexity: <strong>O(#fragments × #lights)</strong> because every fragment interacts with all lights.</li>
</ul>
</li>
</ul>
<h3 id="key-idea-1">
<a class="header-anchor" href="#key-idea-1"></a>
Key Idea
</h3><ul>
<li>Many fragments pass the depth test but are later <strong>overwritten</strong> by closer fragments.</li>
<li><strong>Solution:</strong> Shade only <strong>visible fragments</strong>.</li>
</ul>
<h3 id="solution-two-pass-rasterization">
<a class="header-anchor" href="#solution-two-pass-rasterization"></a>
Solution: Two-Pass Rasterization
</h3><ol>
<li><strong>Pass 1:</strong>
<ul>
<li>Rasterize the scene and update the <strong>depth buffer</strong> without performing shading.</li>
</ul>
</li>
<li><strong>Pass 2:</strong>
<ul>
<li>Use the depth buffer to ensure only the <strong>nearest visible fragments</strong> are shaded.</li>
</ul>
</li>
</ol>
<h3 id="benefits">
<a class="header-anchor" href="#benefits"></a>
Benefits
</h3><ul>
<li>Complexity reduced to <strong>O(#visible fragments × #lights)</strong>.</li>
<li>This approach assumes that <strong>rasterization is faster than shading unseen fragments</strong>, which is usually true.</li>
</ul>
<h3 id="issue">
<a class="header-anchor" href="#issue"></a>
Issue
</h3><ul>
<li>Anti-aliasing cannot be performed during Pass 1 and Pass 2 because of dependency on the <strong>depth buffer (G-buffer)</strong>.</li>
<li><strong>Solution:</strong> Use post-processing techniques like <strong>TAA</strong> (Temporal Anti-Aliasing) or image-based AA.</li>
</ul>
<h2 id="tiled-shading">
<a class="header-anchor" href="#tiled-shading"></a>
Tiled Shading
</h2><p><strong>Tiled Shading</strong> builds on Deferred Shading by dividing the screen into smaller tiles for more efficient shading.</p>
<h3 id="process-1">
<a class="header-anchor" href="#process-1"></a>
Process
</h3><ul>
<li>Divide the screen into tiles (e.g., 32 × 32 pixels).</li>
<li>Perform shading for each tile independently.</li>
</ul>
<h3 id="key-observations-1">
<a class="header-anchor" href="#key-observations-1"></a>
Key Observations
</h3><ul>
<li>Light intensity decreases with distance due to <strong>inverse-square law</strong>, limiting a light&rsquo;s effective range.</li>
<li><strong>Optimization:</strong> Each tile only considers lights within its <strong>affected region</strong> (approximated as spherical volumes).</li>
</ul>
<h3 id="complexity-reduction">
<a class="header-anchor" href="#complexity-reduction"></a>
Complexity Reduction
</h3><ul>
<li>From <strong>O(#visible fragments × #lights)</strong> to <strong>O(#visible fragments × avg #lights per tile)</strong>.</li>
</ul>
<h2 id="clustered-shading">
<a class="header-anchor" href="#clustered-shading"></a>
Clustered Shading
</h2><p><strong>Clustered Shading</strong> further improves efficiency by dividing the 3D space into clusters, adding depth-based subdivisions to the tiles.</p>
<h3 id="process-2">
<a class="header-anchor" href="#process-2"></a>
Process
</h3><ul>
<li>Extend Tiled Shading by slicing the scene along the <strong>depth axis</strong>, creating 3D clusters.</li>
</ul>
<h3 id="key-observations-2">
<a class="header-anchor" href="#key-observations-2"></a>
Key Observations
</h3><ul>
<li>In large depth ranges within a tile, lights may influence only specific depth slices.</li>
<li>By subdividing along the depth axis, <strong>fewer lights are associated with each cluster</strong>.</li>
</ul>
<h3 id="complexity-reduction-1">
<a class="header-anchor" href="#complexity-reduction-1"></a>
Complexity Reduction
</h3><ul>
<li>From <strong>O(#visible fragments × avg #lights per tile)</strong> to <strong>O(#visible fragments × avg #lights per cluster)</strong>.</li>
</ul>
<h3 id="final-result">
<a class="header-anchor" href="#final-result"></a>
Final Result
</h3><ul>
<li><strong>Efficiency Gains:</strong> Avoid unnecessary shading by optimizing the number of lights affecting each fragment.</li>
</ul>
<h2 id="level-of-detail-solutions">
<a class="header-anchor" href="#level-of-detail-solutions"></a>
Level of Detail Solutions
</h2><p><strong>Level of Detail (LoD)</strong> is a crucial technique for balancing visual fidelity and performance by adapting detail levels based on context.</p>
<h3 id="core-concepts">
<a class="header-anchor" href="#core-concepts"></a>
Core Concepts
</h3><ul>
<li><strong>LoD Purpose:</strong><br>
Select the appropriate detail level during computations to reduce resource usage without sacrificing quality.</li>
<li><strong>Texture Mip-maps:</strong><br>
A common example of LoD where lower-resolution textures are used for distant objects.</li>
<li><strong>Cascaded Approach:</strong><br>
Industrial LoD methods often involve selecting detail levels dynamically, such as in <strong>Cascaded Shadow Maps</strong>.</li>
</ul>
<h3 id="examples-of-lod-techniques">
<a class="header-anchor" href="#examples-of-lod-techniques"></a>
Examples of LoD Techniques
</h3><h4 id="cascaded-shadow-maps-csm">
<a class="header-anchor" href="#cascaded-shadow-maps-csm"></a>
Cascaded Shadow Maps (CSM)
</h4><ul>
<li><strong>Observation:</strong><br>
Shadow map texels near the camera cover smaller regions, requiring higher resolution. Texels farther away can use lower resolutions.</li>
<li><strong>Implementation:</strong>
<ul>
<li>Multiple shadow maps with varying resolutions (e.g., high-resolution near the camera and coarse maps farther away).</li>
<li><strong>Blending Transition:</strong><br>
Overlap between layers may cause abrupt changes (artifacts). Smooth transitions are achieved by blending maps based on distance.</li>
</ul>
</li>
<li><strong>Benefit:</strong><br>
Efficient use of resolution while minimizing artifacts.</li>
</ul>
<h4 id="cascaded-lpv-light-propagation-volumes">
<a class="header-anchor" href="#cascaded-lpv-light-propagation-volumes"></a>
Cascaded LPV (Light Propagation Volumes)
</h4><ul>
<li>Similar concept applied to global illumination.</li>
<li>Use finer grids near the camera and coarser grids farther away to save computation.</li>
</ul>
<h3 id="geometric-lod">
<a class="header-anchor" href="#geometric-lod"></a>
Geometric LoD
</h3><ul>
<li><strong>High Poly vs. Low Poly:</strong><br>
Models are simplified into lower-resolution versions (low-poly models). A hierarchy of models is generated.</li>
<li><strong>Dynamic Selection:</strong><br>
Based on distance from the camera:
<ul>
<li>Close objects → High poly.</li>
<li>Distant objects → Low poly.</li>
<li><strong>Per-Part LoD:</strong> Different parts of the same model can use different LoD levels.</li>
</ul>
</li>
<li><strong>Transition Challenge:</strong>
<ul>
<li>Popping artifacts occur when switching between levels suddenly.</li>
<li><strong>Solution:</strong> Temporal Anti-Aliasing (TAA) can mitigate this effect.</li>
</ul>
</li>
<li><strong>Advanced Example:</strong><br>
Nanite in Unreal Engine 5 dynamically handles geometric LoD with advanced optimizations.</li>
</ul>
<h3 id="technical-challenges">
<a class="header-anchor" href="#technical-challenges"></a>
Technical Challenges
</h3><ul>
<li><strong>Smooth Transitions:</strong><br>
Avoiding visual cracks or artifacts between levels.</li>
<li><strong>Efficient Loading and Scheduling:</strong><br>
Dynamically load levels to optimize cache and bandwidth usage.</li>
<li><strong>Geometry Representation:</strong><br>
Using triangles or alternate methods like geometry textures.</li>
<li><strong>Clipping and Culling:</strong><br>
Improve performance by excluding non-visible parts.</li>
</ul>
<h2 id="global-illumination-solutions">
<a class="header-anchor" href="#global-illumination-solutions"></a>
Global Illumination Solutions
</h2><h3 id="overview-1">
<a class="header-anchor" href="#overview-1"></a>
Overview
</h3><ul>
<li>
<p><strong>Screen Space Ray Tracing (SSR):</strong><br>
SSR is commonly used for global illumination but has inherent limitations:</p>
<ul>
<li>Fails when reflections go <strong>off-screen</strong>.</li>
<li>Reflections of objects behind the <strong>camera</strong> are not captured.</li>
<li>Uses a screen-space depth buffer, which only represents a single layer. Geometry behind this layer cannot be traced.</li>
<li>Other scenarios where depth information is insufficient.</li>
</ul>
</li>
<li>
<p><strong>Real-Time Ray Tracing (RTRT):</strong><br>
Theoretically solves all global illumination (GI) scenarios since it accurately simulates light paths.<br>
However, <strong>RTRT is too costly</strong> for full-scene implementation in real-time applications, leading to significant performance issues.</p>
</li>
<li>
<p><strong>Hybrid Solutions:</strong><br>
The industry often combines multiple GI methods to balance quality and performance.</p>
</li>
</ul>
<h3 id="a-typical-hybrid-gi-solution">
<a class="header-anchor" href="#a-typical-hybrid-gi-solution"></a>
A Typical Hybrid GI Solution
</h3><h4 id="step-1-use-ssr-for-an-approximation">
<a class="header-anchor" href="#step-1-use-ssr-for-an-approximation"></a>
<strong>Step 1: Use SSR for an Approximation</strong>
</h4><ul>
<li>SSR provides an initial, approximate GI solution.</li>
<li><strong>Limitations:</strong> Areas SSR cannot handle are addressed using other methods.</li>
</ul>
<h4 id="step-2-address-ssr-limitations-with-ray-tracing">
<a class="header-anchor" href="#step-2-address-ssr-limitations-with-ray-tracing"></a>
<strong>Step 2: Address SSR Limitations with Ray Tracing</strong>
</h4><ul>
<li>
<p><strong>Near-field Objects:</strong></p>
<ul>
<li>Use high-quality Signed Distance Fields (SDFs) around a shading point.</li>
<li>SDFs allow for efficient ray tracing in shaders.</li>
</ul>
</li>
<li>
<p><strong>Far-field Objects:</strong></p>
<ul>
<li>Use lower-quality SDFs to represent the entire scene.</li>
<li>This ensures global illumination is captured efficiently, both near and far.</li>
</ul>
</li>
<li>
<p><strong>Directional or Point Light Sources:</strong></p>
<ul>
<li>Handle using <strong>Reflective Shadow Maps (RSM)</strong> for localized illumination, such as flashlights.</li>
</ul>
</li>
<li>
<p><strong>Diffuse Scenes:</strong></p>
<ul>
<li>Use <strong>probes</strong> to store irradiance in a 3D grid for dynamic diffuse GI (e.g., <strong>DDGI</strong>).</li>
</ul>
</li>
</ul>
<h3 id="hardware-ray-tracing-techniques">
<a class="header-anchor" href="#hardware-ray-tracing-techniques"></a>
Hardware Ray Tracing Techniques
</h3><ul>
<li>
<p><strong>Simplified Geometry:</strong></p>
<ul>
<li>Replace original geometry with low-poly proxies.</li>
<li>This optimization makes RTRT faster while maintaining acceptable accuracy for indirect illumination.</li>
</ul>
</li>
<li>
<p><strong>Probes and RTXGI:</strong></p>
<ul>
<li>Combine hardware ray tracing with probe-based methods (e.g., <strong>RTXGI</strong>) for efficient dynamic lighting.</li>
</ul>
</li>
</ul>
<h3 id="example-lumen-in-unreal-engine-5">
<a class="header-anchor" href="#example-lumen-in-unreal-engine-5"></a>
Example: Lumen in Unreal Engine 5
</h3><ul>
<li>A hybrid approach combining <strong>SSR</strong>, <strong>SDF-based tracing</strong>, <strong>RSM</strong>, and <strong>low-poly RTRT</strong> forms the foundation of <strong>UE5&rsquo;s Lumen system</strong>, achieving efficient and high-quality GI.</li>
</ul>
<h2 id="challenges-in-rendering-engines">
<a class="header-anchor" href="#challenges-in-rendering-engines"></a>
Challenges in Rendering Engines
</h2><p>From an engine&rsquo;s perspective, the real challenge lies in managing numerous technical issues:</p>
<ul>
<li>Making solutions adaptable across various scenes.</li>
<li>Ensuring solutions are efficient and fast.</li>
</ul>
<h1 id="uncovered-topics">
<a class="header-anchor" href="#uncovered-topics"></a>
Uncovered Topics
</h1><ul>
<li>
<p><strong>Texturing an SDF:</strong></p>
<ul>
<li>Applying textures to Signed Distance Fields (SDFs) efficiently remains a technical hurdle.</li>
</ul>
</li>
<li>
<p><strong>Transparent Materials &amp; Order-Independent Transparency:</strong></p>
<ul>
<li>Accurately rendering transparent materials while avoiding sorting artifacts.</li>
<li>Solutions like Order-Independent Transparency (OIT) can help but are computationally expensive.</li>
</ul>
</li>
<li>
<p><strong>Particle Rendering:</strong></p>
<ul>
<li>Efficiently simulating and rendering millions of particles, especially with complex effects like lighting and transparency.</li>
</ul>
</li>
<li>
<p><strong>Post-Processing:</strong></p>
<ul>
<li>Effects like depth of field, motion blur, bloom, and tone mapping.</li>
<li>These are computationally expensive but essential for photorealism.</li>
</ul>
</li>
<li>
<p><strong>Random Seed &amp; Blue Noise:</strong></p>
<ul>
<li>Using <strong>blue noise</strong> for stochastic sampling to reduce visible noise in ray tracing and other algorithms.</li>
</ul>
</li>
<li>
<p><strong>Foveated Rendering:</strong></p>
<ul>
<li>Rendering at higher quality in regions the viewer focuses on, leveraging eye-tracking.</li>
<li>Crucial for VR/AR applications.</li>
</ul>
</li>
<li>
<p><strong>Probe-Based Global Illumination:</strong></p>
<ul>
<li>Efficient use of probes to approximate lighting and irradiance in real-time scenarios.</li>
</ul>
</li>
<li>
<p><strong>Advanced Lighting Techniques:</strong></p>
<ul>
<li><strong>ReSTIR (Reservoir-based Spatiotemporal Importance Resampling):</strong><br>
Optimizes light sampling for real-time ray tracing.</li>
<li><strong>Neural Radiance Caching:</strong><br>
Uses machine learning to approximate radiance fields for global illumination.</li>
<li><strong>Many-Light Theory &amp; Light Cuts:</strong><br>
Efficiently handling thousands of lights in a scene by clustering and pruning lights.</li>
</ul>
</li>
<li>
<p><strong>Participating Media:</strong></p>
<ul>
<li>Simulating light interactions with fog, smoke, and volumetric effects.</li>
<li>Subsurface scattering (SSSSS) for materials like skin and wax.</li>
</ul>
</li>
<li>
<p><strong>Hair Rendering:</strong></p>
<ul>
<li>Realistically rendering hair with accurate light scattering and anisotropic reflections.</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      

      

      

      

      

      

      
      <ul class="article-tag-list" itemprop="keywords">
  
</ul>

    </footer>
  </div>
  
    
  <nav
    id="article-nav"
    data-aos="fade-up"
  >
    
      <div class="article-nav-link-wrap article-nav-link-left">
        
          
          
            <img
              data-src="https://raw.githubusercontent.com/NothingToSay0031/Images/main/202412220910158.jpg"
              data-sizes="auto"
              alt="Shadertoy Projects"
              class="lazyload"
            />
          
        
        <a href="https://nothingtosay0031.github.io/post/shadertoy/"></a>
        <div class="article-nav-caption">Newer</div>
        <h3 class="article-nav-title">
          
            Shadertoy Projects
          
        </h3>
      </div>
    

    
      <div class="article-nav-link-wrap article-nav-link-right">
        
          
          
            <img
              data-src="https://raw.githubusercontent.com/NothingToSay0031/Images/main/202412220910158.jpg"
              data-sizes="auto"
              alt="ReSTIR - Path Reuse in Real-time"
              class="lazyload"
            />
          
        
        <a href="https://nothingtosay0031.github.io/post/restir/"></a>
        <div class="article-nav-caption">Older</div>
        <h3 class="article-nav-title">
          
            ReSTIR - Path Reuse in Real-time
          
        </h3>
      </div>
    
  </nav>


  
</article>










</section>
          
            <aside id="sidebar">
  <div class="sidebar-wrapper wrap-sticky">
    <div
      class="sidebar-wrap"
      data-aos="fade-up"
    >
      
        <div class="sidebar-toc-sidebar">
          <div class="sidebar-toc">
  <h3 class="toc-title">Contents</h3>
  <div class="sidebar-toc-wrapper toc-div-class">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#real-time-shadows">Real-Time Shadows</a>
      <ul>
        <li><a href="#shadow-mapping">Shadow Mapping</a>
          <ul>
            <li><a href="#algorithm-steps">Algorithm Steps</a></li>
            <li><a href="#self-occlusion-shadow-acne">Self-Occlusion (Shadow Acne)</a>
              <ul>
                <li><a href="#mitigating-self-occlusion">Mitigating Self-Occlusion</a></li>
              </ul>
            </li>
            <li><a href="#aliasing">Aliasing</a></li>
          </ul>
        </li>
        <li><a href="#percentage-closer-filtering">Percentage Closer Filtering</a>
          <ul>
            <li><a href="#process">Process</a></li>
            <li><a href="#example">Example</a></li>
            <li><a href="#effect-of-kernel-size">Effect of Kernel Size</a></li>
          </ul>
        </li>
        <li><a href="#soft-shadows">Soft Shadows</a></li>
        <li><a href="#pcss-percentage-closer-soft-shadows">PCSS (Percentage Closer Soft Shadows)</a>
          <ul>
            <li><a href="#pcss-algorithm-steps">PCSS Algorithm Steps</a></li>
          </ul>
        </li>
        <li><a href="#vssm-variance-soft-shadow-mapping">VSSM (Variance Soft Shadow Mapping)</a>
          <ul>
            <li><a href="#calculating-mean-and-variance-of-depth-distribution">Calculating Mean and Variance of Depth Distribution</a></li>
            <li><a href="#estimating-unoccluded-depth-or-visibility-with-inequalities">Estimating Unoccluded Depth or Visibility with Inequalities</a></li>
          </ul>
        </li>
        <li><a href="#moment-shadow-mapping">Moment Shadow Mapping</a></li>
        <li><a href="#distance-field-soft-shadows">Distance Field Soft Shadows</a>
          <ul>
            <li><a href="#safe-angle-calculation">Safe Angle Calculation</a></li>
            <li><a href="#considerations">Considerations</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#real-time-environment-mapping">Real-Time Environment Mapping</a>
      <ul>
        <li><a href="#ambient-lighting-and-shading">Ambient Lighting and Shading</a>
          <ul>
            <li><a href="#image-based-lighting-ibl">Image-Based Lighting (IBL)</a>
              <ul>
                <li><a href="#approximation-with-monte-carlo-sampling">Approximation with Monte Carlo Sampling</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#split-sum-approximation">Split Sum Approximation</a>
          <ul>
            <li><a href="#approximation-in-rtr">Approximation in RTR</a>
              <ul>
                <li><a href="#key-points">Key Points</a></li>
              </ul>
            </li>
            <li><a href="#applying-split-sum-to-rendering-equation">Applying Split Sum to Rendering Equation</a></li>
            <li><a href="#sum-based-approximation">Sum-Based Approximation</a></li>
            <li><a href="#pre-filtering-of-the-environment-map">Pre-filtering of the Environment Map</a></li>
            <li><a href="#evaluating-the-brdf-integral">Evaluating the BRDF Integral</a></li>
          </ul>
        </li>
        <li><a href="#environment-lighting-shadows">Environment Lighting Shadows</a>
          <ul>
            <li><a href="#key-challenges">Key Challenges</a></li>
            <li><a href="#industry-solutions">Industry Solutions</a></li>
            <li><a href="#research-directions">Research Directions</a></li>
          </ul>
        </li>
        <li><a href="#precomputed-radiance-transfer">Precomputed Radiance Transfer</a>
          <ul>
            <li><a href="#precomputing-lighting-and-light-transport">Precomputing Lighting and Light Transport</a>
              <ul>
                <li><a href="#key-points-1">Key Points</a></li>
              </ul>
            </li>
            <li><a href="#spherical-harmonics-sh-in-prt">Spherical Harmonics (SH) in PRT</a></li>
            <li><a href="#prt-diffuse-case">PRT Diffuse Case</a></li>
            <li><a href="#prt-glossy-case">PRT Glossy Case</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#real-time-global-illumination">Real-Time Global Illumination</a>
      <ul>
        <li><a href="#image-space-algorithms">Image-Space Algorithms</a></li>
        <li><a href="#reflective-shadow-maps-rsm">Reflective Shadow Maps (RSM)</a>
          <ul>
            <li><a href="#key-concepts-in-rsm">Key Concepts in RSM</a></li>
            <li><a href="#steps-in-the-rsm-algorithm">Steps in the RSM Algorithm</a></li>
            <li><a href="#acceleration">Acceleration</a></li>
            <li><a href="#pros-and-cons">Pros and Cons</a></li>
          </ul>
        </li>
        <li><a href="#world-space-algorithms">World Space Algorithms</a></li>
        <li><a href="#light-propagation-volumes-lpv">Light Propagation Volumes (LPV)</a>
          <ul>
            <li><a href="#key-principles-of-lpv">Key Principles of LPV</a></li>
            <li><a href="#indirect-lighting-computation-with-lpv">Indirect Lighting Computation with LPV</a></li>
            <li><a href="#lpv-process-steps">LPV Process Steps</a></li>
            <li><a href="#limitations-and-artifacts">Limitations and Artifacts</a></li>
          </ul>
        </li>
        <li><a href="#voxel-global-illumination-vxgi">Voxel Global Illumination (VXGI)</a>
          <ul>
            <li><a href="#core-concept-of-vxgi">Core Concept of VXGI</a></li>
            <li><a href="#vxgi-process-steps">VXGI Process Steps</a></li>
            <li><a href="#handling-different-surface-materials">Handling Different Surface Materials</a></li>
            <li><a href="#cone-tracing-and-sparse-octree-acceleration">Cone Tracing and Sparse Octree Acceleration</a></li>
            <li><a href="#vxgi-vs-lpv-and-rsm">VXGI vs. LPV and RSM</a></li>
            <li><a href="#advantages-and-trade-offs">Advantages and Trade-Offs</a></li>
            <li><a href="#application-example">Application Example</a></li>
          </ul>
        </li>
        <li><a href="#screen-space-algorithms">Screen Space Algorithms</a>
          <ul>
            <li><a href="#ssao-screen-space-ambient-occlusion">SSAO (Screen Space Ambient Occlusion)</a></li>
            <li><a href="#derivation-of-ssao">Derivation of SSAO</a>
              <ul>
                <li><a href="#separating-the-visibility-term">Separating the Visibility Term</a></li>
                <li><a href="#why-include-cos-theta_i-in-visibility-term">Why Include  in Visibility Term?</a></li>
              </ul>
            </li>
            <li><a href="#real-time-computation-of-k_ap-in-ssao">Real-Time Computation of  in SSAO</a>
              <ul>
                <li><a href="#key-points-2">Key Points</a></li>
                <li><a href="#sampling-in-ssao">Sampling in SSAO</a></li>
              </ul>
            </li>
            <li><a href="#horizon-based-ambient-occlusion-hbao">Horizon-Based Ambient Occlusion (HBAO)</a>
              <ul>
                <li><a href="#key-characteristics">Key Characteristics</a></li>
                <li><a href="#why-hbao-is-more-accurate">Why HBAO is More Accurate</a></li>
              </ul>
            </li>
            <li><a href="#ssdo-screen-space-directional-occlusion">SSDO (Screen Space Directional Occlusion)</a>
              <ul>
                <li><a href="#core-idea">Core Idea</a></li>
                <li><a href="#advantages-of-ssdo">Advantages of SSDO</a></li>
                <li><a href="#steps-of-the-ssdo-algorithm">Steps of the SSDO Algorithm</a></li>
                <li><a href="#example-workflow"><strong>Example Workflow</strong></a></li>
              </ul>
            </li>
            <li><a href="#issues-with-ssdo">Issues with SSDO</a></li>
          </ul>
        </li>
        <li><a href="#screen-space-reflection-ssr">Screen Space Reflection (SSR)</a></li>
      </ul>
    </li>
    <li><a href="#physically-based-rendering">Physically-Based Rendering</a>
      <ul>
        <li><a href="#microfacet-brdf">Microfacet BRDF</a>
          <ul>
            <li><a href="#fresnel-term">Fresnel Term</a>
              <ul>
                <li><a href="#fresnel-equations-for-dielectrics">Fresnel Equations for Dielectrics</a></li>
                <li><a href="#schlicks-approximation">Schlick&rsquo;s Approximation</a></li>
              </ul>
            </li>
            <li><a href="#fresnel-approximations-for-metals">Fresnel Approximations for Metals</a>
              <ul>
                <li><a href="#key-observations">Key Observations</a></li>
              </ul>
            </li>
            <li><a href="#normal-distribution-function-ndf">Normal Distribution Function (NDF)</a>
              <ul>
                <li><a href="#key-properties-of-ndf">Key Properties of NDF</a></li>
                <li><a href="#common-ndf-models">Common NDF Models</a></li>
                <li><a href="#generalized-trowbridge-reitz-gtr">Generalized-Trowbridge-Reitz (GTR)</a></li>
                <li><a href="#advanced-ndf-for-surface-effects">Advanced NDF for Surface Effects</a></li>
                <li><a href="#practical-considerations">Practical Considerations</a></li>
              </ul>
            </li>
            <li><a href="#shadowing-masking-term">Shadowing-Masking Term   </a>
              <ul>
                <li><a href="#smith-shadowing-masking-approximation">Smith Shadowing-Masking Approximation</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#kulla-conty-approximation">Kulla-Conty Approximation</a>
          <ul>
            <li><a href="#integral-estimation">Integral Estimation</a></li>
            <li><a href="#albedo-approximation">Albedo Approximation</a></li>
            <li><a href="#energy-loss-in-direction-mu_o">Energy Loss in Direction </a></li>
            <li><a href="#average-albedo">Average Albedo</a></li>
            <li><a href="#fresnel-term-and-multiple-scattering-compensation">Fresnel Term and Multiple Scattering Compensation</a>
              <ul>
                <li><a href="#average-fresnel-term">Average Fresnel Term</a></li>
                <li><a href="#energy-distribution-after-surface-incidence">Energy Distribution After Surface Incidence</a></li>
                <li><a href="#total-multiple-scattering-contribution">Total Multiple Scattering Contribution</a></li>
              </ul>
            </li>
            <li><a href="#final-brdf-with-compensation">Final BRDF with Compensation</a></li>
            <li><a href="#compensation-factor-f_textms">Compensation Factor </a>
              <ul>
                <li><a href="#validation">Validation</a></li>
              </ul>
            </li>
            <li><a href="#other-energy-compensation-methods">Other Energy Compensation Methods</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#non-photorealistic-rendering-npr">Non-Photorealistic Rendering (NPR)</a>
      <ul>
        <li><a href="#outline-rendering">Outline Rendering</a></li>
        <li><a href="#color-blocks">Color Blocks</a></li>
        <li><a href="#sketch-style">Sketch Style</a></li>
      </ul>
    </li>
    <li><a href="#real-time-ray-tracing">Real-Time Ray Tracing</a>
      <ul>
        <li><a href="#rtx-and-path-tracing-in-real-time-applications">RTX and Path Tracing in Real-Time Applications</a>
          <ul>
            <li><a href="#components-of-1-spp-path-tracing">Components of 1 SPP Path Tracing</a></li>
            <li><a href="#challenges-of-rtrt">Challenges of RTRT</a></li>
            <li><a href="#motion-vectors">Motion Vectors</a></li>
            <li><a href="#temporal-accumulation-and-filtering">Temporal Accumulation and Filtering</a></li>
            <li><a href="#denoising-solutions">Denoising Solutions</a></li>
            <li><a href="#challenges-and-failure-cases">Challenges and Failure Cases</a>
              <ul>
                <li><a href="#switching-scenes">Switching Scenes</a></li>
                <li><a href="#screen-space-issues">Screen-Space Issues</a></li>
                <li><a href="#shading-changes">Shading Changes</a></li>
              </ul>
            </li>
            <li><a href="#solutions-for-temporal-artifacts">Solutions for Temporal Artifacts</a></li>
            <li><a href="#implementation-of-filtering">Implementation of Filtering</a>
              <ul>
                <li><a href="#important-notes">Important Notes:</a></li>
              </ul>
            </li>
            <li><a href="#bilateral-filtering">Bilateral Filtering</a>
              <ul>
                <li><a href="#key-idea">Key Idea:</a></li>
                <li><a href="#filtering-kernel">Filtering Kernel:</a></li>
              </ul>
            </li>
            <li><a href="#kernel-definition">Kernel Definition:</a>
              <ul>
                <li><a href="#components">Components:</a></li>
                <li><a href="#key-behavior">Key Behavior:</a></li>
                <li><a href="#effects">Effects:</a></li>
              </ul>
            </li>
            <li><a href="#joint-bilateral-filtering">Joint Bilateral Filtering</a>
              <ul>
                <li><a href="#overview">Overview:</a></li>
                <li><a href="#motivation">Motivation:</a></li>
                <li><a href="#g-buffer-features">G-buffer Features:</a></li>
                <li><a href="#examples-of-feature-guidance">Examples of Feature Guidance:</a></li>
                <li><a href="#implementation">Implementation:</a></li>
                <li><a href="#notes">Notes:</a></li>
              </ul>
            </li>
            <li><a href="#implementing-large-filters">Implementing Large Filters</a>
              <ul>
                <li><a href="#solution-1-separate-passes">Solution 1: Separate Passes</a></li>
                <li><a href="#solution-2-progressively-growing-sizes">Solution 2: Progressively Growing Sizes</a></li>
              </ul>
            </li>
            <li><a href="#outlier-removal">Outlier Removal</a></li>
            <li><a href="#temporal-clamping">Temporal Clamping</a></li>
            <li><a href="#specific-filtering-approaches-for-rtrt">Specific Filtering Approaches for RTRT</a>
              <ul>
                <li><a href="#svgf">SVGF</a></li>
                <li><a href="#rae">RAE</a></li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#real-time-rendering-techniques">Real-Time Rendering Techniques</a>
      <ul>
        <li><a href="#temporal-anti-aliasing-taa">Temporal Anti-Aliasing (TAA)</a></li>
        <li><a href="#msaa-vs-ssaa">MSAA vs. SSAA</a></li>
        <li><a href="#image-based-anti-aliasing">Image-Based Anti-Aliasing</a></li>
        <li><a href="#no-anti-aliasing-on-g-buffers">No Anti-Aliasing on G-Buffers</a></li>
        <li><a href="#dlss-deep-learning-super-sampling">DLSS (Deep Learning Super Sampling)</a>
          <ul>
            <li><a href="#dlss-10">DLSS 1.0</a></li>
            <li><a href="#dlss-20">DLSS 2.0</a>
              <ul>
                <li><a href="#challenges-in-dlss-20">Challenges in DLSS 2.0:</a></li>
              </ul>
            </li>
            <li><a href="#practical-considerations-for-dlss-20">Practical Considerations for DLSS 2.0:</a></li>
          </ul>
        </li>
        <li><a href="#deferred-shading">Deferred Shading</a>
          <ul>
            <li><a href="#traditional-rasterization-process">Traditional Rasterization Process</a></li>
            <li><a href="#key-idea-1">Key Idea</a></li>
            <li><a href="#solution-two-pass-rasterization">Solution: Two-Pass Rasterization</a></li>
            <li><a href="#benefits">Benefits</a></li>
            <li><a href="#issue">Issue</a></li>
          </ul>
        </li>
        <li><a href="#tiled-shading">Tiled Shading</a>
          <ul>
            <li><a href="#process-1">Process</a></li>
            <li><a href="#key-observations-1">Key Observations</a></li>
            <li><a href="#complexity-reduction">Complexity Reduction</a></li>
          </ul>
        </li>
        <li><a href="#clustered-shading">Clustered Shading</a>
          <ul>
            <li><a href="#process-2">Process</a></li>
            <li><a href="#key-observations-2">Key Observations</a></li>
            <li><a href="#complexity-reduction-1">Complexity Reduction</a></li>
            <li><a href="#final-result">Final Result</a></li>
          </ul>
        </li>
        <li><a href="#level-of-detail-solutions">Level of Detail Solutions</a>
          <ul>
            <li><a href="#core-concepts">Core Concepts</a></li>
            <li><a href="#examples-of-lod-techniques">Examples of LoD Techniques</a>
              <ul>
                <li><a href="#cascaded-shadow-maps-csm">Cascaded Shadow Maps (CSM)</a></li>
                <li><a href="#cascaded-lpv-light-propagation-volumes">Cascaded LPV (Light Propagation Volumes)</a></li>
              </ul>
            </li>
            <li><a href="#geometric-lod">Geometric LoD</a></li>
            <li><a href="#technical-challenges">Technical Challenges</a></li>
          </ul>
        </li>
        <li><a href="#global-illumination-solutions">Global Illumination Solutions</a>
          <ul>
            <li><a href="#overview-1">Overview</a></li>
            <li><a href="#a-typical-hybrid-gi-solution">A Typical Hybrid GI Solution</a>
              <ul>
                <li><a href="#step-1-use-ssr-for-an-approximation"><strong>Step 1: Use SSR for an Approximation</strong></a></li>
                <li><a href="#step-2-address-ssr-limitations-with-ray-tracing"><strong>Step 2: Address SSR Limitations with Ray Tracing</strong></a></li>
              </ul>
            </li>
            <li><a href="#hardware-ray-tracing-techniques">Hardware Ray Tracing Techniques</a></li>
            <li><a href="#example-lumen-in-unreal-engine-5">Example: Lumen in Unreal Engine 5</a></li>
          </ul>
        </li>
        <li><a href="#challenges-in-rendering-engines">Challenges in Rendering Engines</a></li>
      </ul>
    </li>
    <li><a href="#uncovered-topics">Uncovered Topics</a></li>
  </ul>
</nav>
  </div>
</div>
        </div>
        <div class="sidebar-common-sidebar hidden">
          
<div class="sidebar-author">
  <img
    data-src="https://nothingtosay0031.github.io/avatar/../avatar.webp"
    data-sizes="auto"
    alt="NothingToSay0031"
    class="lazyload"
  />
  <div class="sidebar-author-name">NothingToSay0031</div>
  <div class="sidebar-description">又是一个做水果蛋糕的好天气啊！</div>
</div>
<div class="sidebar-state">
  <div class="sidebar-state-article">
    <div>Posts</div>
    
    <div class="sidebar-state-number">16</div>
  </div>
  <div class="sidebar-state-category">
    <div>Categories</div>
    <div class="sidebar-state-number">
      0
    </div>
  </div>
  <div class="sidebar-state-tag">
    <div>Tags</div>
    <div class="sidebar-state-number">0</div>
  </div>
</div>
<div class="sidebar-social">
  
    <div class="icon-email sidebar-social-icon">
      <a
        href="mailto:jhwzju@gmail.com"
        itemprop="url"
        target="_blank"
        aria-label="email"
        rel="noopener external nofollow noreferrer"
      ></a>
    </div>
  
    <div class="icon-github sidebar-social-icon">
      <a
        href="https://github.com/NothingToSay0031"
        itemprop="url"
        target="_blank"
        aria-label="github"
        rel="noopener external nofollow noreferrer"
      ></a>
    </div>
  
    <div class="icon-linkedin sidebar-social-icon">
      <a
        href="https://www.linkedin.com/in/hongweiji"
        itemprop="url"
        target="_blank"
        aria-label="linkedin"
        rel="noopener external nofollow noreferrer"
      ></a>
    </div>
  
</div>
<div class="sidebar-menu">
  
    <div class="sidebar-menu-link-wrap">
      <a
        class="sidebar-menu-link-dummy"
        href="https://nothingtosay0031.github.io/"
        aria-label="Home"
      ></a>
      <div class='sidebar-menu-icon icon rotate'>
        
          
            &#xe62b;
          
        
      </div>
      <div class="sidebar-menu-link">Home</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a
        class="sidebar-menu-link-dummy"
        href="https://nothingtosay0031.github.io/archives"
        aria-label="Archives"
      ></a>
      <div class='sidebar-menu-icon icon rotate'>
        
          
            &#xe62b;
          
        
      </div>
      <div class="sidebar-menu-link">Archives</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a
        class="sidebar-menu-link-dummy"
        href="https://nothingtosay0031.github.io/about"
        aria-label="About"
      ></a>
      <div class='sidebar-menu-icon icon rotate'>
        
          
            &#xe62b;
          
        
      </div>
      <div class="sidebar-menu-link">About</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a
        class="sidebar-menu-link-dummy"
        href="https://nothingtosay0031.github.io/friend"
        aria-label="Friend"
      ></a>
      <div class='sidebar-menu-icon icon rotate'>
        
          
            &#xe62b;
          
        
      </div>
      <div class="sidebar-menu-link">Friend</div>
    </div>
  
</div>

        </div>
      

      
        <div class="sidebar-btn-wrapper" style="position:static">
          <div class="sidebar-toc-btn current"></div>
          <div class="sidebar-common-btn"></div>
        </div>
      
    </div>
  </div>

  
</aside>

          
        </main>
        



  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  



<footer id="footer">
  <div style="width: 100%; overflow: hidden">
    <div class="footer-line"></div>
  </div>
  <div id="footer-info">
    <div>
      <span class="icon-copyright"></span>
      2021 -
      2025
      <span class="footer-info-sep rotate"></span>
      NothingToSay0031
    </div>
    
      <div>
        Powered by&nbsp;<a
          href="https://gohugo.io/"
          target="_blank"
          >Hugo</a
        >&nbsp; Theme.<a
          href="https://github.com/D-Sketon/hugo-theme-reimu"
          target="_blank"
          >Reimu</a
        >
      </div>
    
    
      <div>
        <span class="icon-brush"
          >&nbsp;
            40.0k
          </span
        >
        &nbsp;|&nbsp;
        <span class="icon-coffee">&nbsp;
          
          

          03:18
        </span>
      </div>
    
    
    
    
      <div>
        <span class="icon-eye"></span>
        <span id="busuanzi_container_site_pv"
          >Number of visits&nbsp;<span
            id="busuanzi_value_site_pv"
          ></span
        ></span>
        &nbsp;|&nbsp;
        <span class="icon-user"></span>
        <span id="busuanzi_container_site_uv"
          >Number of visitors&nbsp;<span
            id="busuanzi_value_site_uv"
          ></span
        ></span>
      </div>
    
  </div>
</footer>

        
          <div class="sidebar-top">
            <div class="sidebar-top-taichi rotate"></div>
            <div class="arrow-up"></div>
          </div>
        
        <div id="mask" class="hide"></div>
      </div>
      <nav id="mobile-nav">
  <div class="sidebar-wrap">
    
      <div class="sidebar-toc-sidebar">
        <div class="sidebar-toc">
  <h3 class="toc-title">Contents</h3>
  <div class="sidebar-toc-wrapper toc-div-class">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#real-time-shadows">Real-Time Shadows</a>
      <ul>
        <li><a href="#shadow-mapping">Shadow Mapping</a>
          <ul>
            <li><a href="#algorithm-steps">Algorithm Steps</a></li>
            <li><a href="#self-occlusion-shadow-acne">Self-Occlusion (Shadow Acne)</a>
              <ul>
                <li><a href="#mitigating-self-occlusion">Mitigating Self-Occlusion</a></li>
              </ul>
            </li>
            <li><a href="#aliasing">Aliasing</a></li>
          </ul>
        </li>
        <li><a href="#percentage-closer-filtering">Percentage Closer Filtering</a>
          <ul>
            <li><a href="#process">Process</a></li>
            <li><a href="#example">Example</a></li>
            <li><a href="#effect-of-kernel-size">Effect of Kernel Size</a></li>
          </ul>
        </li>
        <li><a href="#soft-shadows">Soft Shadows</a></li>
        <li><a href="#pcss-percentage-closer-soft-shadows">PCSS (Percentage Closer Soft Shadows)</a>
          <ul>
            <li><a href="#pcss-algorithm-steps">PCSS Algorithm Steps</a></li>
          </ul>
        </li>
        <li><a href="#vssm-variance-soft-shadow-mapping">VSSM (Variance Soft Shadow Mapping)</a>
          <ul>
            <li><a href="#calculating-mean-and-variance-of-depth-distribution">Calculating Mean and Variance of Depth Distribution</a></li>
            <li><a href="#estimating-unoccluded-depth-or-visibility-with-inequalities">Estimating Unoccluded Depth or Visibility with Inequalities</a></li>
          </ul>
        </li>
        <li><a href="#moment-shadow-mapping">Moment Shadow Mapping</a></li>
        <li><a href="#distance-field-soft-shadows">Distance Field Soft Shadows</a>
          <ul>
            <li><a href="#safe-angle-calculation">Safe Angle Calculation</a></li>
            <li><a href="#considerations">Considerations</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#real-time-environment-mapping">Real-Time Environment Mapping</a>
      <ul>
        <li><a href="#ambient-lighting-and-shading">Ambient Lighting and Shading</a>
          <ul>
            <li><a href="#image-based-lighting-ibl">Image-Based Lighting (IBL)</a>
              <ul>
                <li><a href="#approximation-with-monte-carlo-sampling">Approximation with Monte Carlo Sampling</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#split-sum-approximation">Split Sum Approximation</a>
          <ul>
            <li><a href="#approximation-in-rtr">Approximation in RTR</a>
              <ul>
                <li><a href="#key-points">Key Points</a></li>
              </ul>
            </li>
            <li><a href="#applying-split-sum-to-rendering-equation">Applying Split Sum to Rendering Equation</a></li>
            <li><a href="#sum-based-approximation">Sum-Based Approximation</a></li>
            <li><a href="#pre-filtering-of-the-environment-map">Pre-filtering of the Environment Map</a></li>
            <li><a href="#evaluating-the-brdf-integral">Evaluating the BRDF Integral</a></li>
          </ul>
        </li>
        <li><a href="#environment-lighting-shadows">Environment Lighting Shadows</a>
          <ul>
            <li><a href="#key-challenges">Key Challenges</a></li>
            <li><a href="#industry-solutions">Industry Solutions</a></li>
            <li><a href="#research-directions">Research Directions</a></li>
          </ul>
        </li>
        <li><a href="#precomputed-radiance-transfer">Precomputed Radiance Transfer</a>
          <ul>
            <li><a href="#precomputing-lighting-and-light-transport">Precomputing Lighting and Light Transport</a>
              <ul>
                <li><a href="#key-points-1">Key Points</a></li>
              </ul>
            </li>
            <li><a href="#spherical-harmonics-sh-in-prt">Spherical Harmonics (SH) in PRT</a></li>
            <li><a href="#prt-diffuse-case">PRT Diffuse Case</a></li>
            <li><a href="#prt-glossy-case">PRT Glossy Case</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#real-time-global-illumination">Real-Time Global Illumination</a>
      <ul>
        <li><a href="#image-space-algorithms">Image-Space Algorithms</a></li>
        <li><a href="#reflective-shadow-maps-rsm">Reflective Shadow Maps (RSM)</a>
          <ul>
            <li><a href="#key-concepts-in-rsm">Key Concepts in RSM</a></li>
            <li><a href="#steps-in-the-rsm-algorithm">Steps in the RSM Algorithm</a></li>
            <li><a href="#acceleration">Acceleration</a></li>
            <li><a href="#pros-and-cons">Pros and Cons</a></li>
          </ul>
        </li>
        <li><a href="#world-space-algorithms">World Space Algorithms</a></li>
        <li><a href="#light-propagation-volumes-lpv">Light Propagation Volumes (LPV)</a>
          <ul>
            <li><a href="#key-principles-of-lpv">Key Principles of LPV</a></li>
            <li><a href="#indirect-lighting-computation-with-lpv">Indirect Lighting Computation with LPV</a></li>
            <li><a href="#lpv-process-steps">LPV Process Steps</a></li>
            <li><a href="#limitations-and-artifacts">Limitations and Artifacts</a></li>
          </ul>
        </li>
        <li><a href="#voxel-global-illumination-vxgi">Voxel Global Illumination (VXGI)</a>
          <ul>
            <li><a href="#core-concept-of-vxgi">Core Concept of VXGI</a></li>
            <li><a href="#vxgi-process-steps">VXGI Process Steps</a></li>
            <li><a href="#handling-different-surface-materials">Handling Different Surface Materials</a></li>
            <li><a href="#cone-tracing-and-sparse-octree-acceleration">Cone Tracing and Sparse Octree Acceleration</a></li>
            <li><a href="#vxgi-vs-lpv-and-rsm">VXGI vs. LPV and RSM</a></li>
            <li><a href="#advantages-and-trade-offs">Advantages and Trade-Offs</a></li>
            <li><a href="#application-example">Application Example</a></li>
          </ul>
        </li>
        <li><a href="#screen-space-algorithms">Screen Space Algorithms</a>
          <ul>
            <li><a href="#ssao-screen-space-ambient-occlusion">SSAO (Screen Space Ambient Occlusion)</a></li>
            <li><a href="#derivation-of-ssao">Derivation of SSAO</a>
              <ul>
                <li><a href="#separating-the-visibility-term">Separating the Visibility Term</a></li>
                <li><a href="#why-include-cos-theta_i-in-visibility-term">Why Include  in Visibility Term?</a></li>
              </ul>
            </li>
            <li><a href="#real-time-computation-of-k_ap-in-ssao">Real-Time Computation of  in SSAO</a>
              <ul>
                <li><a href="#key-points-2">Key Points</a></li>
                <li><a href="#sampling-in-ssao">Sampling in SSAO</a></li>
              </ul>
            </li>
            <li><a href="#horizon-based-ambient-occlusion-hbao">Horizon-Based Ambient Occlusion (HBAO)</a>
              <ul>
                <li><a href="#key-characteristics">Key Characteristics</a></li>
                <li><a href="#why-hbao-is-more-accurate">Why HBAO is More Accurate</a></li>
              </ul>
            </li>
            <li><a href="#ssdo-screen-space-directional-occlusion">SSDO (Screen Space Directional Occlusion)</a>
              <ul>
                <li><a href="#core-idea">Core Idea</a></li>
                <li><a href="#advantages-of-ssdo">Advantages of SSDO</a></li>
                <li><a href="#steps-of-the-ssdo-algorithm">Steps of the SSDO Algorithm</a></li>
                <li><a href="#example-workflow"><strong>Example Workflow</strong></a></li>
              </ul>
            </li>
            <li><a href="#issues-with-ssdo">Issues with SSDO</a></li>
          </ul>
        </li>
        <li><a href="#screen-space-reflection-ssr">Screen Space Reflection (SSR)</a></li>
      </ul>
    </li>
    <li><a href="#physically-based-rendering">Physically-Based Rendering</a>
      <ul>
        <li><a href="#microfacet-brdf">Microfacet BRDF</a>
          <ul>
            <li><a href="#fresnel-term">Fresnel Term</a>
              <ul>
                <li><a href="#fresnel-equations-for-dielectrics">Fresnel Equations for Dielectrics</a></li>
                <li><a href="#schlicks-approximation">Schlick&rsquo;s Approximation</a></li>
              </ul>
            </li>
            <li><a href="#fresnel-approximations-for-metals">Fresnel Approximations for Metals</a>
              <ul>
                <li><a href="#key-observations">Key Observations</a></li>
              </ul>
            </li>
            <li><a href="#normal-distribution-function-ndf">Normal Distribution Function (NDF)</a>
              <ul>
                <li><a href="#key-properties-of-ndf">Key Properties of NDF</a></li>
                <li><a href="#common-ndf-models">Common NDF Models</a></li>
                <li><a href="#generalized-trowbridge-reitz-gtr">Generalized-Trowbridge-Reitz (GTR)</a></li>
                <li><a href="#advanced-ndf-for-surface-effects">Advanced NDF for Surface Effects</a></li>
                <li><a href="#practical-considerations">Practical Considerations</a></li>
              </ul>
            </li>
            <li><a href="#shadowing-masking-term">Shadowing-Masking Term   </a>
              <ul>
                <li><a href="#smith-shadowing-masking-approximation">Smith Shadowing-Masking Approximation</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#kulla-conty-approximation">Kulla-Conty Approximation</a>
          <ul>
            <li><a href="#integral-estimation">Integral Estimation</a></li>
            <li><a href="#albedo-approximation">Albedo Approximation</a></li>
            <li><a href="#energy-loss-in-direction-mu_o">Energy Loss in Direction </a></li>
            <li><a href="#average-albedo">Average Albedo</a></li>
            <li><a href="#fresnel-term-and-multiple-scattering-compensation">Fresnel Term and Multiple Scattering Compensation</a>
              <ul>
                <li><a href="#average-fresnel-term">Average Fresnel Term</a></li>
                <li><a href="#energy-distribution-after-surface-incidence">Energy Distribution After Surface Incidence</a></li>
                <li><a href="#total-multiple-scattering-contribution">Total Multiple Scattering Contribution</a></li>
              </ul>
            </li>
            <li><a href="#final-brdf-with-compensation">Final BRDF with Compensation</a></li>
            <li><a href="#compensation-factor-f_textms">Compensation Factor </a>
              <ul>
                <li><a href="#validation">Validation</a></li>
              </ul>
            </li>
            <li><a href="#other-energy-compensation-methods">Other Energy Compensation Methods</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#non-photorealistic-rendering-npr">Non-Photorealistic Rendering (NPR)</a>
      <ul>
        <li><a href="#outline-rendering">Outline Rendering</a></li>
        <li><a href="#color-blocks">Color Blocks</a></li>
        <li><a href="#sketch-style">Sketch Style</a></li>
      </ul>
    </li>
    <li><a href="#real-time-ray-tracing">Real-Time Ray Tracing</a>
      <ul>
        <li><a href="#rtx-and-path-tracing-in-real-time-applications">RTX and Path Tracing in Real-Time Applications</a>
          <ul>
            <li><a href="#components-of-1-spp-path-tracing">Components of 1 SPP Path Tracing</a></li>
            <li><a href="#challenges-of-rtrt">Challenges of RTRT</a></li>
            <li><a href="#motion-vectors">Motion Vectors</a></li>
            <li><a href="#temporal-accumulation-and-filtering">Temporal Accumulation and Filtering</a></li>
            <li><a href="#denoising-solutions">Denoising Solutions</a></li>
            <li><a href="#challenges-and-failure-cases">Challenges and Failure Cases</a>
              <ul>
                <li><a href="#switching-scenes">Switching Scenes</a></li>
                <li><a href="#screen-space-issues">Screen-Space Issues</a></li>
                <li><a href="#shading-changes">Shading Changes</a></li>
              </ul>
            </li>
            <li><a href="#solutions-for-temporal-artifacts">Solutions for Temporal Artifacts</a></li>
            <li><a href="#implementation-of-filtering">Implementation of Filtering</a>
              <ul>
                <li><a href="#important-notes">Important Notes:</a></li>
              </ul>
            </li>
            <li><a href="#bilateral-filtering">Bilateral Filtering</a>
              <ul>
                <li><a href="#key-idea">Key Idea:</a></li>
                <li><a href="#filtering-kernel">Filtering Kernel:</a></li>
              </ul>
            </li>
            <li><a href="#kernel-definition">Kernel Definition:</a>
              <ul>
                <li><a href="#components">Components:</a></li>
                <li><a href="#key-behavior">Key Behavior:</a></li>
                <li><a href="#effects">Effects:</a></li>
              </ul>
            </li>
            <li><a href="#joint-bilateral-filtering">Joint Bilateral Filtering</a>
              <ul>
                <li><a href="#overview">Overview:</a></li>
                <li><a href="#motivation">Motivation:</a></li>
                <li><a href="#g-buffer-features">G-buffer Features:</a></li>
                <li><a href="#examples-of-feature-guidance">Examples of Feature Guidance:</a></li>
                <li><a href="#implementation">Implementation:</a></li>
                <li><a href="#notes">Notes:</a></li>
              </ul>
            </li>
            <li><a href="#implementing-large-filters">Implementing Large Filters</a>
              <ul>
                <li><a href="#solution-1-separate-passes">Solution 1: Separate Passes</a></li>
                <li><a href="#solution-2-progressively-growing-sizes">Solution 2: Progressively Growing Sizes</a></li>
              </ul>
            </li>
            <li><a href="#outlier-removal">Outlier Removal</a></li>
            <li><a href="#temporal-clamping">Temporal Clamping</a></li>
            <li><a href="#specific-filtering-approaches-for-rtrt">Specific Filtering Approaches for RTRT</a>
              <ul>
                <li><a href="#svgf">SVGF</a></li>
                <li><a href="#rae">RAE</a></li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#real-time-rendering-techniques">Real-Time Rendering Techniques</a>
      <ul>
        <li><a href="#temporal-anti-aliasing-taa">Temporal Anti-Aliasing (TAA)</a></li>
        <li><a href="#msaa-vs-ssaa">MSAA vs. SSAA</a></li>
        <li><a href="#image-based-anti-aliasing">Image-Based Anti-Aliasing</a></li>
        <li><a href="#no-anti-aliasing-on-g-buffers">No Anti-Aliasing on G-Buffers</a></li>
        <li><a href="#dlss-deep-learning-super-sampling">DLSS (Deep Learning Super Sampling)</a>
          <ul>
            <li><a href="#dlss-10">DLSS 1.0</a></li>
            <li><a href="#dlss-20">DLSS 2.0</a>
              <ul>
                <li><a href="#challenges-in-dlss-20">Challenges in DLSS 2.0:</a></li>
              </ul>
            </li>
            <li><a href="#practical-considerations-for-dlss-20">Practical Considerations for DLSS 2.0:</a></li>
          </ul>
        </li>
        <li><a href="#deferred-shading">Deferred Shading</a>
          <ul>
            <li><a href="#traditional-rasterization-process">Traditional Rasterization Process</a></li>
            <li><a href="#key-idea-1">Key Idea</a></li>
            <li><a href="#solution-two-pass-rasterization">Solution: Two-Pass Rasterization</a></li>
            <li><a href="#benefits">Benefits</a></li>
            <li><a href="#issue">Issue</a></li>
          </ul>
        </li>
        <li><a href="#tiled-shading">Tiled Shading</a>
          <ul>
            <li><a href="#process-1">Process</a></li>
            <li><a href="#key-observations-1">Key Observations</a></li>
            <li><a href="#complexity-reduction">Complexity Reduction</a></li>
          </ul>
        </li>
        <li><a href="#clustered-shading">Clustered Shading</a>
          <ul>
            <li><a href="#process-2">Process</a></li>
            <li><a href="#key-observations-2">Key Observations</a></li>
            <li><a href="#complexity-reduction-1">Complexity Reduction</a></li>
            <li><a href="#final-result">Final Result</a></li>
          </ul>
        </li>
        <li><a href="#level-of-detail-solutions">Level of Detail Solutions</a>
          <ul>
            <li><a href="#core-concepts">Core Concepts</a></li>
            <li><a href="#examples-of-lod-techniques">Examples of LoD Techniques</a>
              <ul>
                <li><a href="#cascaded-shadow-maps-csm">Cascaded Shadow Maps (CSM)</a></li>
                <li><a href="#cascaded-lpv-light-propagation-volumes">Cascaded LPV (Light Propagation Volumes)</a></li>
              </ul>
            </li>
            <li><a href="#geometric-lod">Geometric LoD</a></li>
            <li><a href="#technical-challenges">Technical Challenges</a></li>
          </ul>
        </li>
        <li><a href="#global-illumination-solutions">Global Illumination Solutions</a>
          <ul>
            <li><a href="#overview-1">Overview</a></li>
            <li><a href="#a-typical-hybrid-gi-solution">A Typical Hybrid GI Solution</a>
              <ul>
                <li><a href="#step-1-use-ssr-for-an-approximation"><strong>Step 1: Use SSR for an Approximation</strong></a></li>
                <li><a href="#step-2-address-ssr-limitations-with-ray-tracing"><strong>Step 2: Address SSR Limitations with Ray Tracing</strong></a></li>
              </ul>
            </li>
            <li><a href="#hardware-ray-tracing-techniques">Hardware Ray Tracing Techniques</a></li>
            <li><a href="#example-lumen-in-unreal-engine-5">Example: Lumen in Unreal Engine 5</a></li>
          </ul>
        </li>
        <li><a href="#challenges-in-rendering-engines">Challenges in Rendering Engines</a></li>
      </ul>
    </li>
    <li><a href="#uncovered-topics">Uncovered Topics</a></li>
  </ul>
</nav>
  </div>
</div>
      </div>
      <div class="sidebar-common-sidebar hidden">
        
<div class="sidebar-author">
  <img
    data-src="https://nothingtosay0031.github.io/avatar/../avatar.webp"
    data-sizes="auto"
    alt="NothingToSay0031"
    class="lazyload"
  />
  <div class="sidebar-author-name">NothingToSay0031</div>
  <div class="sidebar-description">又是一个做水果蛋糕的好天气啊！</div>
</div>
<div class="sidebar-state">
  <div class="sidebar-state-article">
    <div>Posts</div>
    
    <div class="sidebar-state-number">16</div>
  </div>
  <div class="sidebar-state-category">
    <div>Categories</div>
    <div class="sidebar-state-number">
      0
    </div>
  </div>
  <div class="sidebar-state-tag">
    <div>Tags</div>
    <div class="sidebar-state-number">0</div>
  </div>
</div>
<div class="sidebar-social">
  
    <div class="icon-email sidebar-social-icon">
      <a
        href="mailto:jhwzju@gmail.com"
        itemprop="url"
        target="_blank"
        aria-label="email"
        rel="noopener external nofollow noreferrer"
      ></a>
    </div>
  
    <div class="icon-github sidebar-social-icon">
      <a
        href="https://github.com/NothingToSay0031"
        itemprop="url"
        target="_blank"
        aria-label="github"
        rel="noopener external nofollow noreferrer"
      ></a>
    </div>
  
    <div class="icon-linkedin sidebar-social-icon">
      <a
        href="https://www.linkedin.com/in/hongweiji"
        itemprop="url"
        target="_blank"
        aria-label="linkedin"
        rel="noopener external nofollow noreferrer"
      ></a>
    </div>
  
</div>
<div class="sidebar-menu">
  
    <div class="sidebar-menu-link-wrap">
      <a
        class="sidebar-menu-link-dummy"
        href="https://nothingtosay0031.github.io/"
        aria-label="Home"
      ></a>
      <div class='sidebar-menu-icon icon rotate'>
        
          
            &#xe62b;
          
        
      </div>
      <div class="sidebar-menu-link">Home</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a
        class="sidebar-menu-link-dummy"
        href="https://nothingtosay0031.github.io/archives"
        aria-label="Archives"
      ></a>
      <div class='sidebar-menu-icon icon rotate'>
        
          
            &#xe62b;
          
        
      </div>
      <div class="sidebar-menu-link">Archives</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a
        class="sidebar-menu-link-dummy"
        href="https://nothingtosay0031.github.io/about"
        aria-label="About"
      ></a>
      <div class='sidebar-menu-icon icon rotate'>
        
          
            &#xe62b;
          
        
      </div>
      <div class="sidebar-menu-link">About</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a
        class="sidebar-menu-link-dummy"
        href="https://nothingtosay0031.github.io/friend"
        aria-label="Friend"
      ></a>
      <div class='sidebar-menu-icon icon rotate'>
        
          
            &#xe62b;
          
        
      </div>
      <div class="sidebar-menu-link">Friend</div>
    </div>
  
</div>

      </div>
    
  </div>
  
    <div class="sidebar-btn-wrapper">
      <div class="sidebar-toc-btn current"></div>
      <div class="sidebar-common-btn"></div>
    </div>
  
</nav>

    </div>
    
    






  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/lazysizes@5.3.2/lazysizes.min.js"
    
    
    
    
    integrity="sha384-3gT/vsepWkfz/ff7PpWNUeMzeWoH3cDhm/A8jM7ouoAK0/fP/9bcHHR5kHq2nf&#43;e" crossorigin="anonymous"
  ></script>




  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/clipboard@2.0.11/dist/clipboard.min.js"
    
    
    
    
    integrity="sha384-J08i8An/QeARD9ExYpvphB8BsyOj3Gh2TSh1aLINKO3L0cMSH2dN3E22zFoXEi0Q" crossorigin="anonymous"
  ></script>









  
      
      <script src="https://nothingtosay0031.github.io/js/main.js" integrity="" crossorigin="anonymous" ></script>
      



  





  
      
      <script src="https://nothingtosay0031.github.io/js/aos.js" integrity="" crossorigin="anonymous" ></script>
      

  <script>
    var aosInit = () => {
      AOS.init({
        duration: 1000,
        easing: "ease",
        once: true,
        offset: 50,
      });
    };
    if (document.readyState === "loading") {
      document.addEventListener("DOMContentLoaded", aosInit);
    } else {
      aosInit();
    }
  </script>








  
      
      <script src="https://nothingtosay0031.github.io/js/pjax_main.js" integrity="" crossorigin="anonymous" data-pjax></script>
      





  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/mouse-firework@0.0.6/dist/index.umd.js"
    
    
    
    
    integrity="sha384-vkGvf25gm1C1PbcoD5dNfc137HzNL/hr1RKA5HniJOaawtvUmH5lTVFgFAruE9Ge" crossorigin="anonymous"
  ></script>


<script>
  if (window.firework) {
    const options = JSON.parse("{\"excludeelements\":[\"a\",\"button\"],\"particles\":[{\"colors\":[\"#ff5252\",\"#ff7c7c\",\"#ffafaf\",\"#ffd0d0\"],\"duration\":[1200,1800],\"easing\":\"easeOutExpo\",\"move\":[\"emit\"],\"number\":20,\"shape\":\"circle\",\"shapeOptions\":{\"alpha\":[0.3,0.5],\"radius\":[16,32]}},{\"colors\":[\"#ff0000\"],\"duration\":[1200,1800],\"easing\":\"easeOutExpo\",\"move\":[\"diffuse\"],\"number\":1,\"shape\":\"circle\",\"shapeOptions\":{\"alpha\":[0.2,0.5],\"lineWidth\":6,\"radius\":20}}]}");
    options.excludeElements = options.excludeelements;
    delete options.excludeelements;
    window.firework(options);
  }
</script>








<div id="lazy-script">
  <div>
    
    
      





  
      
      <script src="https://nothingtosay0031.github.io/js/insert_highlight.js" integrity="" crossorigin="anonymous" data-pjax></script>
      

      
      
      
      
      <script type="module" data-pjax>
        const PhotoSwipeLightbox = (await safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe-lightbox.esm.min.js", "sha384-DiL6M\/gG\u002bwmTxmCRZyD1zee6lIhawn5TGvED0FOh7fXcN9B0aZ9dexSF\/N6lrZi\/")).default;

        const pswp = () => {
          if (_$$('.article-entry a.article-gallery-item').length > 0) {
            new PhotoSwipeLightbox({
              gallery: '.article-entry',
              children: 'a.article-gallery-item',
              pswpModule: () => safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js", "sha384-WkkO3GCmgkC3VQWpaV8DqhKJqpzpF9JoByxDmnV8\u002boTJ7m3DfYEWX1fu1scuS4\u002bs")
            }).init();
          }
          if(_$$('.article-gallery a.article-gallery-item').length > 0) {
            new PhotoSwipeLightbox({
              gallery: '.article-gallery',
              children: 'a.article-gallery-item',
              pswpModule: () => safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js", "sha384-WkkO3GCmgkC3VQWpaV8DqhKJqpzpF9JoByxDmnV8\u002boTJ7m3DfYEWX1fu1scuS4\u002bs")
            }).init();
          }
          window.lightboxStatus = 'done';
          window.removeEventListener('lightbox:ready', pswp);
        }
        if(window.lightboxStatus === 'ready') {
          pswp()
        } else {
          window.addEventListener('lightbox:ready', pswp);
        }
      </script>
      












    
    
      
        

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/katex@0.16.9/dist/katex.min.js"
    
    
    data-pjax
    
    integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8" crossorigin="anonymous"
  ></script>


        

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/katex@0.16.9/dist/contrib/auto-render.min.js"
    
    
    data-pjax
    
    integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05" crossorigin="anonymous"
  ></script>


        <script data-pjax>
          var renderMath = () => {
            if (!window.renderMathInElement) return;
            window.renderMathInElement(document.body, {
              delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "\\[", right: "\\]", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
              ],
            });
          };
          if (document.readyState === "loading") {
            document.addEventListener("DOMContentLoaded", renderMath, { once: true });
          } else {
            renderMath();
          }
        </script>
      
      
    
  </div>
</div>




  

  
  
  
  
  
  
  <script
    src="https://npm.webcache.cn/busuanzi@2.3.0/bsz.pure.mini.js"
    
    async
    
    
    integrity="sha384-0M75wtSkhjIInv4coYlaJU83&#43;OypaRCIq2SukQVQX04eGTCBXJDuWAbJet56id&#43;S" crossorigin="anonymous"
  ></script>





  <script>
    if ('serviceWorker' in navigator) {
      navigator.serviceWorker.getRegistrations().then((registrations) => {
        for (let registration of registrations) {
          registration.unregister();
        }
      });
    }
  </script>


<script>
  const reimuCopyright = String.raw`
   ______     ______     __     __    __     __  __    
  /\  == \   /\  ___\   /\ \   /\ "-./  \   /\ \/\ \   
  \ \  __<   \ \  __\   \ \ \  \ \ \-./\ \  \ \ \_\ \  
   \ \_\ \_\  \ \_____\  \ \_\  \ \_\ \ \_\  \ \_____\ 
    \/_/ /_/   \/_____/   \/_/   \/_/  \/_/   \/_____/ 
                                                    
  `;
  console.log(String.raw`%c ${reimuCopyright}`, "color: #ff5252;");
  console.log(
    "%c Theme.Reimu" + " %c https://github.com/D-Sketon/hugo-theme-reimu ",
    "color: white; background: #ff5252; padding:5px 0;",
    "padding:4px;border:1px solid #ff5252;",
  );
</script>

  </body>
</html>
